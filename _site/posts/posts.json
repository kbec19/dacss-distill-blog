[
  {
    "path": "posts/protest-network-diffusion/",
    "title": "Network Article Review",
    "description": "The Dynamics of Protest Recruitment Through an Online Network",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-04-11",
    "categories": [
      "networks",
      "presentation",
      "analysis"
    ],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-11T22:58:46-05:00",
    "input_file": "protest-network-diffusion.knit.md"
  },
  {
    "path": "posts/dacss603hw4/",
    "title": "DACSS 603 Homework 4",
    "description": "Transformations & Logistic Regression and Final Project Status",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-04-09",
    "categories": [
      "statistics",
      "homework"
    ],
    "contents": "\r\n\r\nContents\r\nPart 1\r\nQuestion 1\r\nA\r\nB\r\nC\r\nD\r\nE\r\n\r\nQuestion 2\r\nA\r\nB\r\n\r\nQuestion 3\r\n\r\nPart 2 (Course Final Project)\r\nQuestion 1\r\nQuestion 2\r\nQuestion 3\r\n\r\n\r\nPart 1\r\nQuestion 1\r\n(SMSS 14.3, 14.4, merged & modified, using data file “house.selling.price.2” from smss R package)\r\nFor the house.selling.price.2 data the tables show a correlation matrix and a model fit using four predictors of selling price. With these four predictors,\r\nA\r\nFor backward elimination, which variable would be deleted first? Why?\r\nAnswer\r\nFirst, I will re-create the correlation matrix and the model fit table from the question.\r\n\r\n\r\nShow code\r\n\r\n#load the data and create a data frame\r\ndata(\"house.selling.price.2\")\r\ndf1a <- as_tibble(house.selling.price.2)\r\n#assign column names to represent variables accurately\r\ncolnames(df1a) <- c(\"Price\", \"Size\", \"Beds\", \"Baths\", \"New\")\r\n#create a correlation matrix and round to 3 decimals\r\ndf1acorr <- cor(df1a)\r\nround(df1acorr, 3)\r\n\r\n\r\n      Price  Size  Beds Baths   New\r\nPrice 1.000 0.899 0.590 0.714 0.357\r\nSize  0.899 1.000 0.669 0.662 0.176\r\nBeds  0.590 0.669 1.000 0.334 0.267\r\nBaths 0.714 0.662 0.334 1.000 0.182\r\nNew   0.357 0.176 0.267 0.182 1.000\r\n\r\nShow code\r\n\r\n#run the linear model\r\nlm1a <- lm(Price ~ ., data = df1a)\r\ntidylm1a <- tidy(lm1a, conf.int = FALSE) \r\ntidylm1a %>%\r\n  mutate_if(is.numeric, round, 4)\r\n\r\n\r\n# A tibble: 5 x 5\r\n  term        estimate std.error statistic p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\r\n1 (Intercept)   -41.8      12.1     -3.45   0.0009\r\n2 Size           64.8       5.63    11.5    0     \r\n3 Beds           -2.77      3.96    -0.698  0.487 \r\n4 Baths          19.2       5.65     3.40   0.001 \r\n5 New            19.0       3.87     4.90   0     \r\n\r\nB\r\nFor forward selection, which variable would be added first? Why?\r\nC\r\nWhy do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\r\nD\r\nUsing software with these four predictors, find the model that would be selected using each criterion:\r\nR2\r\nAdjusted R2\r\nPRESS\r\nAIC\r\nBIC\r\nE\r\nExplain which model you prefer and why.\r\nQuestion 2\r\nFrom the documentation:\r\n“This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labelled Girth in the data. It is measured at 4 ft 6 in above the ground.”\r\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,\r\nA\r\nfit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables\r\nB\r\nRun regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\r\nQuestion 3\r\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\r\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\r\nPart 2 (Course Final Project)\r\nQuestion 1\r\nWhat is your research question for the final project?\r\nQuestion 2\r\nWhat is your hypothesis (i.e. an answer to the research question) that you want to test?\r\nQuestion 3\r\nPresent some exploratory analysis. In particular:\r\nNumerically summarize (e.g. with the summary() function) the variables of interest (the outcome, the explanatory variable, the control variables).\r\nPlot the relationships between key variables. You can do this any way you want, but one straightforward way of doing this would be with the pairs() function or other scatter plots / box plots. Interpret what you see.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-08T17:50:30-05:00",
    "input_file": "dacss603hw4.knit.md"
  },
  {
    "path": "posts/analyzing-headlines/",
    "title": "DACSS 697D Post 7",
    "description": "Assignment 7 for DACSS 697D Course 'Text as Data': \"Analyzing Print v. Web Headlines using Dictionary Methods\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-04-09",
    "categories": [
      "text as data",
      "homework"
    ],
    "contents": "\r\n\r\nContents\r\nNew Research Path: Comparing Different Headlines\r\nMaking Choices on Inclusion of Observations\r\nTokenization\r\nDictionary Analysis\r\n\r\nNew Research Path: Comparing Different Headlines\r\nAs I mentioned in my last post on this topic, last semester my research group hand coded PDF copies of articles resulting from a simple search on the websites of the New York Times and Wall Street Journal from Feburary 29, 2020 through September 30, 2021 using the term “Afghanistan withdrawal”. One thing I noticed was that when loading the PDF articles into NVivo for coding, it was difficult to match the New York Times articles to the citation information in Zotero for many of the articles because the article titles did not match. I realized that in the process of saving the articles in Zotero, they were saved with the title viewable on the web version of the article; however, once the article had been preserved by using the site’s “Print to PDF” function, the article title that it used as a default file name was different than the web version.\r\nAfter taking an initial look at the differences, I have cleaned the data further, and now want to look more closely at the differences in the fields for the “main headline” and “print headline” pulled from the API for similarity.\r\nMaking Choices on Inclusion of Observations\r\nIn my initial look at the data, it was clear that not all of the articles had different headlines; some are the same entries, and some have “N/A” in the “print” version only, indicating they were online-only stories. Although I initially felt inclined to leave the “N/A” observations in the analysis, I removed those observations as they would not be relevant to my new research questions comparing the framing for different audiences.\r\nI also removed whole sections where the API returned an observation as there was apparently use of the word “Afghanistan” somewhere in the article/entry, but the type of entry was clearly not being represented in the headline. For example, “Obituary” entries had headlines consisting primarily of just the name of the deceased and no further information. Similarly, “Corrections” entries have headlines consisting only of the term “Corrections” and the corresponding date. Similar choices were made on the “Books”, “Movies”, “Theater”, and “Food” sections when entries are primarily the names of the things being reviewed that may have a reference to Afghanistan somewhere in the text, but it is not relevant specifically to the withdrawal time period being analyzed.\r\nAt this point, I included the entirety of the “U.S.” and “World” news sections, even if the content related to Afghanistan is not readily observable.\r\nI’ll start by loading the smaller headline data in more detail.\r\n\r\n  doc.id\r\n1      1\r\n2      2\r\n3      3\r\n4      4\r\n5      5\r\n6      6\r\n                                                                         headline.main\r\n1                           Can Trump Make Foreign Policy a Democratic Campaign Issue?\r\n2                                                         Serving in a Twenty-Year War\r\n3                              Suleimani’s Gone, and the Iran Nuclear Deal May Be Next\r\n4 In Era of Perpetual Conflict, a Volatile President Grabs Expanded Powers to Make War\r\n5                        Muslims Organize Huge Protests Across India, Challenging Modi\r\n6                                                     A War Without Winners Winds Down\r\n       date section.name\r\n1  1/8/2020      Opinion\r\n2  9/8/2021         U.S.\r\n3  1/3/2020        World\r\n4  1/4/2020        World\r\n5  1/4/2020        World\r\n6 2/29/2020      Opinion\r\n  doc.id                                          headline.print\r\n1      1                  Foreign Policy Should Matter to Voters\r\n2      2                                    Mission ‘Unfinished’\r\n3      3 U.S. Strike May Have Dealt a Fatal Blow to Nuclear Deal\r\n4      4        More Powers, Few Limits And a Volatile President\r\n5      5  Huge Protests in India Keep Steadfast Pressure on Modi\r\n6      6                        A War Without Winners Winds Down\r\n       date section.name\r\n1  1/8/2020      Opinion\r\n2  9/8/2021         U.S.\r\n3  1/3/2020        World\r\n4  1/4/2020        World\r\n5  1/4/2020        World\r\n6 2/29/2020      Opinion\r\n\r\nI want to create a corpus of each of the headline data frames for next step analysis:\r\n\r\n  Text Types Tokens Sentences      date section.name\r\n1    1    10     10         1  1/8/2020      Opinion\r\n2    2     5      5         1  9/8/2021         U.S.\r\n3    3    11     11         1  1/3/2020        World\r\n4    4    15     15         1  1/4/2020        World\r\n5    5     9      9         1  1/4/2020        World\r\n6    6     6      6         1 2/29/2020      Opinion\r\n  Text Types Tokens Sentences      date section.name\r\n1    1     6      6         1  1/8/2020      Opinion\r\n2    2     3      4         1  9/8/2021         U.S.\r\n3    3    12     12         2  1/3/2020        World\r\n4    4     9      9         1  1/4/2020        World\r\n5    5     9      9         1  1/4/2020        World\r\n6    6     6      6         1 2/29/2020      Opinion\r\n\r\n\r\n\r\n\r\nTokenization\r\nI finally realized how to remove the “�” symbol that has plagued me since starting working with this API by using “remove_symbols=TRUE” in addition to removing the punctuation when tokenizing:\r\n\r\nTokens consisting of 1,371 documents and 2 docvars.\r\n1 :\r\n[1] \"Can\"        \"Trump\"      \"Make\"       \"Foreign\"    \"Policy\"    \r\n[6] \"a\"          \"Democratic\" \"Campaign\"   \"Issue\"     \r\n\r\n2 :\r\n[1] \"Serving\"     \"in\"          \"a\"           \"Twenty-Year\"\r\n[5] \"War\"        \r\n\r\n3 :\r\n [1] \"Suleimani's\" \"Gone\"        \"and\"         \"the\"        \r\n [5] \"Iran\"        \"Nuclear\"     \"Deal\"        \"May\"        \r\n [9] \"Be\"          \"Next\"       \r\n\r\n4 :\r\n [1] \"In\"        \"Era\"       \"of\"        \"Perpetual\" \"Conflict\" \r\n [6] \"a\"         \"Volatile\"  \"President\" \"Grabs\"     \"Expanded\" \r\n[11] \"Powers\"    \"to\"       \r\n[ ... and 2 more ]\r\n\r\n5 :\r\n[1] \"Muslims\"     \"Organize\"    \"Huge\"        \"Protests\"   \r\n[5] \"Across\"      \"India\"       \"Challenging\" \"Modi\"       \r\n\r\n6 :\r\n[1] \"A\"       \"War\"     \"Without\" \"Winners\" \"Winds\"   \"Down\"   \r\n\r\n[ reached max_ndoc ... 1,365 more documents ]\r\nTokens consisting of 1,371 documents and 2 docvars.\r\n1 :\r\n[1] \"Foreign\" \"Policy\"  \"Should\"  \"Matter\"  \"to\"      \"Voters\" \r\n\r\n2 :\r\n[1] \"Mission\"    \"Unfinished\"\r\n\r\n3 :\r\n [1] \"U.S\"     \"Strike\"  \"May\"     \"Have\"    \"Dealt\"   \"a\"      \r\n [7] \"Fatal\"   \"Blow\"    \"to\"      \"Nuclear\" \"Deal\"   \r\n\r\n4 :\r\n[1] \"More\"      \"Powers\"    \"Few\"       \"Limits\"    \"And\"      \r\n[6] \"a\"         \"Volatile\"  \"President\"\r\n\r\n5 :\r\n[1] \"Huge\"      \"Protests\"  \"in\"        \"India\"     \"Keep\"     \r\n[6] \"Steadfast\" \"Pressure\"  \"on\"        \"Modi\"     \r\n\r\n6 :\r\n[1] \"A\"       \"War\"     \"Without\" \"Winners\" \"Winds\"   \"Down\"   \r\n\r\n[ reached max_ndoc ... 1,365 more documents ]\r\n\r\nNow that I have been able to remove the symbols, I should be able to get a better dfm result. In addition to removing the punctuation, symbols, and stopwords, I also want to remove my new top result, “s”, because the tokenization removing contractoins left many stranded “s” characters.\r\nFirst, for the main headlines:\r\n\r\n[1] 1371\r\nTokens consisting of 1,371 documents and 2 docvars.\r\n1 :\r\n[1] \"Can\"        \"Trump\"      \"Make\"       \"Foreign\"    \"Policy\"    \r\n[6] \"Democratic\" \"Campaign\"   \"Issue\"     \r\n\r\n2 :\r\n[1] \"Serving\"     \"Twenty-Year\" \"War\"        \r\n\r\n3 :\r\n[1] \"Suleimani's\" \"Gone\"        \"Iran\"        \"Nuclear\"    \r\n[5] \"Deal\"        \"May\"         \"Next\"       \r\n\r\n4 :\r\n [1] \"Era\"       \"Perpetual\" \"Conflict\"  \"Volatile\"  \"President\"\r\n [6] \"Grabs\"     \"Expanded\"  \"Powers\"    \"Make\"      \"War\"      \r\n\r\n5 :\r\n[1] \"Muslims\"     \"Organize\"    \"Huge\"        \"Protests\"   \r\n[5] \"Across\"      \"India\"       \"Challenging\" \"Modi\"       \r\n\r\n6 :\r\n[1] \"War\"     \"Without\" \"Winners\" \"Winds\"  \r\n\r\n[ reached max_ndoc ... 1,365 more documents ]\r\n\r\nThen the print headlines:\r\n\r\n\r\nprint_tokens <- tokens(print_corpus, remove_punct = TRUE) %>%\r\n  tokens(print_corpus, remove_symbols = TRUE) %>%\r\n  tokens_remove(stopwords(\"english\")) %>%\r\n  tokens_remove(c(\"s\"))\r\n\r\n\r\nmain_dfm <- dfm(print_tokens)\r\n\r\nlength(print_tokens)\r\n\r\n\r\n[1] 1371\r\n\r\nprint(print_tokens)\r\n\r\n\r\nTokens consisting of 1,371 documents and 2 docvars.\r\n1 :\r\n[1] \"Foreign\" \"Policy\"  \"Matter\"  \"Voters\" \r\n\r\n2 :\r\n[1] \"Mission\"    \"Unfinished\"\r\n\r\n3 :\r\n[1] \"U.S\"     \"Strike\"  \"May\"     \"Dealt\"   \"Fatal\"   \"Blow\"   \r\n[7] \"Nuclear\" \"Deal\"   \r\n\r\n4 :\r\n[1] \"Powers\"    \"Limits\"    \"Volatile\"  \"President\"\r\n\r\n5 :\r\n[1] \"Huge\"      \"Protests\"  \"India\"     \"Keep\"      \"Steadfast\"\r\n[6] \"Pressure\"  \"Modi\"     \r\n\r\n6 :\r\n[1] \"War\"     \"Without\" \"Winners\" \"Winds\"  \r\n\r\n[ reached max_ndoc ... 1,365 more documents ]\r\n\r\nNow I can use quanteda to generate the document-feature matrices\r\n\r\n\r\nmain_dfm <- dfm(main_tokens)\r\nmain_dfm\r\n\r\n\r\nDocument-feature matrix of: 1,371 documents, 3,432 features (99.79% sparse) and 2 docvars.\r\n    features\r\ndocs can trump make foreign policy democratic campaign issue serving\r\n   1   1     1    1       1      1          1        1     1       0\r\n   2   0     0    0       0      0          0        0     0       1\r\n   3   0     0    0       0      0          0        0     0       0\r\n   4   0     0    1       0      0          0        0     0       0\r\n   5   0     0    0       0      0          0        0     0       0\r\n   6   0     0    0       0      0          0        0     0       0\r\n    features\r\ndocs twenty-year\r\n   1           0\r\n   2           1\r\n   3           0\r\n   4           0\r\n   5           0\r\n   6           0\r\n[ reached max_ndoc ... 1,365 more documents, reached max_nfeat ... 3,422 more features ]\r\n\r\nprint_dfm <- dfm(print_tokens)\r\nprint_dfm\r\n\r\n\r\nDocument-feature matrix of: 1,371 documents, 3,327 features (99.81% sparse) and 2 docvars.\r\n    features\r\ndocs foreign policy matter voters mission unfinished u.s strike may\r\n   1       1      1      1      1       0          0   0      0   0\r\n   2       0      0      0      0       1          1   0      0   0\r\n   3       0      0      0      0       0          0   1      1   1\r\n   4       0      0      0      0       0          0   0      0   0\r\n   5       0      0      0      0       0          0   0      0   0\r\n   6       0      0      0      0       0          0   0      0   0\r\n    features\r\ndocs dealt\r\n   1     0\r\n   2     0\r\n   3     1\r\n   4     0\r\n   5     0\r\n   6     0\r\n[ reached max_ndoc ... 1,365 more documents, reached max_nfeat ... 3,317 more features ]\r\n\r\n#create a word frequency variable and the rankings\r\nmain_counts <- as.data.frame(sort(colSums(main_dfm),dec=T))\r\ncolnames(main_counts) <- c(\"Frequency\")\r\nmain_counts$Rank <- c(1:ncol(main_dfm))\r\nhead(main_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               240    1\r\nafghanistan       202    2\r\nafghan            200    3\r\ntaliban           150    4\r\nbiden             120    5\r\nwar                96    6\r\n\r\nprint_counts <- as.data.frame(sort(colSums(print_dfm),dec=T))\r\ncolnames(print_counts) <- c(\"Frequency\")\r\nprint_counts$Rank <- c(1:ncol(print_dfm))\r\nhead(print_counts)\r\n\r\n\r\n            Frequency Rank\r\nu.s               236    1\r\nafghan            148    2\r\ntaliban           145    3\r\nafghanistan       134    4\r\nbiden              93    5\r\nwar                83    6\r\n\r\nNow I can take a look at this network of feature co-occurrences for the main headlines:\r\n\r\n[1] 3432 3432\r\n[1] 20 20\r\n\r\n\r\nand for the print headlines:\r\n\r\n[1] 3327 3327\r\n[1] 20 20\r\n\r\n\r\nThis brings me to where I had previously stopped in my comparison and analysis, and now that I’m able to produce a cleaner result, I’ll move on to further analysis using the quanteda dictionary.\r\nDictionary Analysis\r\n\r\n\r\n# convert tokens from each headline data set to DFM using the dictionary \"NRC\"\r\nmain_nrc <- dfm(main_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\nprint_nrc <- dfm(print_tokens) %>%\r\n  dfm_lookup(data_dictionary_NRC)\r\n\r\ndim(main_nrc)\r\n\r\n\r\n[1] 1371   10\r\n\r\nmain_nrc\r\n\r\n\r\nDocument-feature matrix of: 1,371 documents, 10 features (68.56% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        1        0       0\r\n   2     0            0       0    1   0        1        0       0\r\n   3     0            1       0    0   1        0        1       0\r\n   4     1            0       0    2   0        2        1       1\r\n   5     0            0       0    0   0        0        1       0\r\n   6     0            0       0    1   0        1        0       0\r\n    features\r\ndocs surprise trust\r\n   1        1     1\r\n   2        0     0\r\n   3        1     1\r\n   4        0     1\r\n   5        0     0\r\n   6        0     0\r\n[ reached max_ndoc ... 1,365 more documents ]\r\n\r\ndim(print_nrc)\r\n\r\n\r\n[1] 1371   10\r\n\r\nprint_nrc\r\n\r\n\r\nDocument-feature matrix of: 1,371 documents, 10 features (69.85% sparse) and 2 docvars.\r\n    features\r\ndocs anger anticipation disgust fear joy negative positive sadness\r\n   1     0            0       0    0   0        1        0       0\r\n   2     0            0       0    0   0        1        0       0\r\n   3     2            1       0    1   1        2        1       1\r\n   4     0            0       0    0   0        0        1       0\r\n   5     0            0       0    0   0        1        1       0\r\n   6     0            0       0    1   0        1        0       0\r\n    features\r\ndocs surprise trust\r\n   1        0     1\r\n   2        0     0\r\n   3        1     1\r\n   4        0     1\r\n   5        0     1\r\n   6        0     0\r\n[ reached max_ndoc ... 1,365 more documents ]\r\n\r\nAnd use the information in a data frame to plot the output:\r\n\r\n\r\n#for the main headlines\r\ndf_main_nrc <- convert(main_nrc, to = \"data.frame\")\r\ndf_main_nrc$polarity <- (df_main_nrc$positive - df_main_nrc$negative)/(df_main_nrc$positive + df_main_nrc$negative)\r\ndf_main_nrc$polarity[which((df_main_nrc$positive + df_main_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_main_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n#and the print headlines\r\ndf_print_nrc <- convert(print_nrc, to = \"data.frame\")\r\ndf_print_nrc$polarity <- (df_print_nrc$positive - df_print_nrc$negative)/(df_print_nrc$positive + df_print_nrc$negative)\r\ndf_print_nrc$polarity[which((df_print_nrc$positive + df_print_nrc$negative) == 0)] <- 0\r\n\r\nggplot(df_print_nrc) + \r\n  geom_histogram(aes(x=polarity)) + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nLooking at the headlines that are indicated as “1”, or positive in sentiment, it’s clear that this dictionary is not capturing the sentiment accurately.\r\n\r\n\r\nhead(main_corpus[which(df_main_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n3 :\r\n\"Suleimani’s Gone, and the Iran Nuclear Deal May Be Next\"\r\n\r\n5 :\r\n\"Muslims Organize Huge Protests Across India, Challenging Mod...\"\r\n\r\n14 :\r\n\"Trump Isn’t Telling Us Everything About His Deal With the Ta...\"\r\n\r\n28 :\r\n\"Mohammed bin Zayed’s Dark Vision of the Middle East’s Future\"\r\n\r\n29 :\r\n\"He Fled Iran as a Child. Now He’s Commanding a U.S. Aircraft...\"\r\n\r\n30 :\r\n\"Pentagon Rules Out Striking Iranian Cultural Sites, Contradi...\"\r\n\r\nhead(print_corpus[which(df_print_nrc$polarity == 1)])\r\n\r\n\r\nCorpus consisting of 6 documents and 2 docvars.\r\n4 :\r\n\"More Powers, Few Limits And a Volatile President\"\r\n\r\n7 :\r\n\"As Tehran and Washington Stare Each Other Down, ISIS Stands ...\"\r\n\r\n9 :\r\n\"References In Address To 2013 Deal Had Holes\"\r\n\r\n10 :\r\n\"President Exaggerates His Own Record On Funding Pentagon and...\"\r\n\r\n20 :\r\n\"Majority Leader Says a Briefing for the Senate Is Scheduled ...\"\r\n\r\n22 :\r\n\"Victims Included Citizens From at Least 7 Countries\"\r\n\r\nI am going to want to look at multiple dictionaries to see if one can best apply to this data. First, the LSD 2015 dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the LSD2015 dictionary\r\nmain_lsd2015 <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create main polarity measure for LSD2015\r\nmain_lsd2015 <- convert(main_lsd2015, to = \"data.frame\")\r\nmain_lsd2015$polarity <- (main_lsd2015$positive - main_lsd2015$negative)/(main_lsd2015$positive + main_lsd2015$negative)\r\nmain_lsd2015$polarity[which((main_lsd2015$positive + main_lsd2015$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the LSD2015 dictionary\r\nprint_lsd2015 <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                              tolower = TRUE) %>%\r\n                          dfm_lookup(data_dictionary_LSD2015)\r\n# create print polarity measure for LSD2015\r\nprint_lsd2015 <- convert(print_lsd2015, to = \"data.frame\")\r\nprint_lsd2015$polarity <- (print_lsd2015$positive - print_lsd2015$negative)/(print_lsd2015$positive + print_lsd2015$negative)\r\nprint_lsd2015$polarity[which((print_lsd2015$positive + print_lsd2015$negative) == 0)] <- 0\r\n\r\n\r\n\r\nand the General Inquirer dictionary:\r\n\r\n\r\n# convert main corpus to DFM using the General Inquirer dictionary\r\nmain_geninq <- dfm(tokens(main_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create main polarity measure for GenInq\r\nmain_geninq <- convert(main_geninq, to = \"data.frame\")\r\nmain_geninq$polarity <- (main_geninq$positive - main_geninq$negative)/(main_geninq$positive + main_geninq$negative)\r\nmain_geninq$polarity[which((main_geninq$positive + main_geninq$negative) == 0)] <- 0\r\n# convert print corpus to DFM using the General Inquirer dictionary\r\nprint_geninq <- dfm(tokens(print_corpus, remove_punct = TRUE),\r\n                             tolower = TRUE) %>%\r\n                    dfm_lookup(data_dictionary_geninqposneg)\r\n# create print polarity measure for GenInq\r\nprint_geninq <- convert(print_geninq, to = \"data.frame\")\r\nprint_geninq$polarity <- (print_geninq$positive - print_geninq$negative)/(print_geninq$positive + print_geninq $negative)\r\nprint_geninq$polarity[which((print_geninq$positive + print_geninq$negative) == 0)] <- 0\r\n\r\n\r\n\r\nNow I’m going to be able to compare the different dictionary scores in one data frame for each type of headline.\r\n\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                0           0        0       0\r\n2         10         0                0           0        1       0\r\n3        100         1                3           1        1       2\r\n4       1000         0                0           0        0       0\r\n5       1001         0                0           0        0       0\r\n6       1002         0                1           0        0       1\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            1            0           0            1         1\r\n2            1            1           0            1         0\r\n3            1            2           0            1         1\r\n4            1            0           1            0         0\r\n5            0            2           0            0         1\r\n6            1            1           0            0         2\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1   -1.0000000                0                0                    0\r\n2    0.0000000                1                0                    0\r\n3    0.3333333                2                2                    0\r\n4   -1.0000000                0                0                    0\r\n5    1.0000000                0                0                    0\r\n6    0.0000000                1                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0                0               1\r\n2                    0               -1               0\r\n3                    0                0               0\r\n4                    0                0               0\r\n5                    0                0               1\r\n6                    0               -1               0\r\n  geninq_negative geninq_polarity\r\n1               2      -0.3333333\r\n2               1      -1.0000000\r\n3               2      -1.0000000\r\n4               1      -1.0000000\r\n5               0       1.0000000\r\n6               2      -1.0000000\r\n  nrc_doc_id nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_joy\r\n1          1         0                0           0        0       0\r\n2         10         0                0           0        0       0\r\n3        100         1                1           0        0       2\r\n4       1000         0                0           0        0       0\r\n5       1001         1                0           0        1       0\r\n6       1002         0                0           0        0       0\r\n  nrc_negative nrc_positive nrc_sadness nrc_surprise nrc_trust\r\n1            1            0           0            0         1\r\n2            0            1           0            0         1\r\n3            1            2           1            1         1\r\n4            1            0           1            0         0\r\n5            2            0           1            0         1\r\n6            1            1           0            0         1\r\n  nrc_polarity lsd2015_negative lsd2015_positive lsd2015_neg_positive\r\n1   -1.0000000                0                0                    0\r\n2    1.0000000                2                0                    0\r\n3    0.3333333                0                1                    0\r\n4   -1.0000000                0                0                    0\r\n5   -1.0000000                1                1                    0\r\n6    0.0000000                2                0                    0\r\n  lsd2015_neg_negative lsd2015_polarity geninq_positive\r\n1                    0                0               1\r\n2                    0               -1               0\r\n3                    0                1               0\r\n4                    0                0               0\r\n5                    0                0               1\r\n6                    0               -1               0\r\n  geninq_negative geninq_polarity\r\n1               2      -0.3333333\r\n2               0       0.0000000\r\n3               0       0.0000000\r\n4               1      -1.0000000\r\n5               2      -0.3333333\r\n6               0       0.0000000\r\n\r\nNow that we have them all in a single data frame, it’s straightforward to figure out a bit about how well our different measures of polarity agree across the different approaches by looking at their correlation using the “cor()” function.\r\n\r\n\r\ncor(main_sent$nrc_polarity, main_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4787681\r\n\r\ncor(main_sent$nrc_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.4746162\r\n\r\ncor(main_sent$lsd2015_polarity, main_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5205449\r\n\r\n\r\n\r\ncor(print_sent$nrc_polarity, print_sent$lsd2015_polarity)\r\n\r\n\r\n[1] 0.4984596\r\n\r\ncor(print_sent$nrc_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5085233\r\n\r\ncor(print_sent$lsd2015_polarity, print_sent$geninq_polarity)\r\n\r\n\r\n[1] 0.5235135\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-09T11:52:03-05:00",
    "input_file": "analyzing-headlines.knit.md"
  },
  {
    "path": "posts/networks-community/",
    "title": "Networks: Community",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-04-09",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nCreating a function to extract the giant component. Two of the functions will not evaluate unconnected graphs, so I did this step first. The one isolate node, “Bruce Hornsby”, is the only songwriter who wrote only a song without collaborating with anyone, and in the fast & greedy, walktrap, and spinglass models the isolate was treated as a separate community.\r\nFast and Greedy Community\r\nPlot the Network with Community Colors\r\nWalktrap Community Detection and Plot\r\nCompare Community Partitions - Fast and Greedy and Walktrap\r\nLeading Label Propagation Community Detection\r\nEdge Betweenness Community Detection\r\nEigenvector Community Detection\r\nSpinglass Community Detection\r\nAdding Community Membership to Node Info\r\n\r\nI am continuing to use the Grateful Dead song writing data set that I used in previous assignments to examine co-writing links and centrality. The data set consists of the links between co-writers of songs played by the Grateful Dead over their 30-year touring career that I compiled.\r\nThere are 26 songwriters that contributed to the songs played over the course of the Grateful Dead history, resulting in 26 nodes in the dataset.\r\nThere are a total of 183 (updated and still under review!) unique songs played, and the varies combinations of co-writing combinations are now represented in a binary affiliation matrix.\r\n\r\n\r\n\r\nThis week I will calculate community clusters using various algorithms. First, I will get my data into an igraph network object and inspect it.\r\n\r\n\r\nShow code\r\n\r\n#import data\r\ngd_vertices <- read.csv(\"gd_nodes.csv\", header=T, stringsAsFactors=F)\r\ngd_affiliation <- read.csv(\"gd_affiliation_matrix.csv\", row.names = 1, header = TRUE, check.names = FALSE)\r\ngd_matrix <- as.matrix(gd_affiliation)\r\ngd_projection <- gd_matrix%*%t(gd_matrix)\r\n#Create igraph object\r\ngd_network_ig <- graph.adjacency(gd_projection,mode=\"undirected\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nCreating a function to extract the giant component. Two of the functions will not evaluate unconnected graphs, so I did this step first. The one isolate node, “Bruce Hornsby”, is the only songwriter who wrote only a song without collaborating with anyone, and in the fast & greedy, walktrap, and spinglass models the isolate was treated as a separate community.\r\n\r\n\r\nShow code\r\n\r\ngiant.component <- function(graph) {\r\n  cl <- clusters(graph)\r\n  induced.subgraph(graph, which(cl$membership == which.max(cl$csize)))\r\n}\r\n\r\n\r\n\r\n\r\n\r\n#extract giant component\r\ngd_giant<-giant.component(gd_network_ig)\r\n\r\n\r\n\r\nFast and Greedy Community\r\n\r\n\r\nShow code\r\n\r\n#load fast and greedy list\r\nfg_gd <- readRDS(\"fg_gd.RData\")\r\n#inspect\r\nnames(fg_gd)\r\n\r\n\r\n[1] \"merges\"     \"modularity\" \"membership\" \"names\"      \"algorithm\" \r\n[6] \"vcount\"    \r\n\r\n\r\n\r\nShow code\r\n\r\nigraph::groups(fg_gd)\r\n\r\n\r\n$`1`\r\n[1] \"Frank Guida\" \"Dave Parker\" \"Pigpen\"      \"Joe Royster\"\r\n\r\n$`2`\r\n[1] \"Eric Andersen\" \"John Barlow\"   \"Bob Bralove\"   \"Willie Dixon\" \r\n[5] \"Gerrit Graham\" \"Robert Hunter\" \"Rob Wasserman\" \"Bob Weir\"     \r\n[9] \"Vince Welnick\"\r\n\r\n$`3`\r\n[1] \"John Dawson\"     \"Jerry Garcia\"    \"Donna Godchaux\" \r\n[4] \"Keith Godchaux\"  \"Mickey Hart\"     \"Bill Kreutzmann\"\r\n\r\n$`4`\r\n[1] \"Andrew Charles\"  \"Ned Lagin\"       \"Phil Lesh\"      \r\n[4] \"Peter Monk\"      \"Brent Mydland\"   \"Robert Petersen\"\r\n\r\n\r\n\r\nShow code\r\n\r\n#Inspect community membership vector\r\nfg_gd$membership\r\n\r\n\r\n [1] 2 2 2 4 3 2 3 3 3 2 1 3 2 3 4 4 4 4 1 4 1 1 2 2 2\r\n\r\n\r\n\r\nShow code\r\n\r\n#Compare to vertex names vector\r\nfg_gd$names\r\n\r\n\r\n [1] \"Eric Andersen\"   \"John Barlow\"     \"Bob Bralove\"    \r\n [4] \"Andrew Charles\"  \"John Dawson\"     \"Willie Dixon\"   \r\n [7] \"Jerry Garcia\"    \"Donna Godchaux\"  \"Keith Godchaux\" \r\n[10] \"Gerrit Graham\"   \"Frank Guida\"     \"Mickey Hart\"    \r\n[13] \"Robert Hunter\"   \"Bill Kreutzmann\" \"Ned Lagin\"      \r\n[16] \"Phil Lesh\"       \"Peter Monk\"      \"Brent Mydland\"  \r\n[19] \"Dave Parker\"     \"Robert Petersen\" \"Pigpen\"         \r\n[22] \"Joe Royster\"     \"Rob Wasserman\"   \"Bob Weir\"       \r\n[25] \"Vince Welnick\"  \r\n\r\nConfirming which of the 4 membership groups each songwriter is part of:\r\n\r\n\r\nShow code\r\n\r\n#Membership function\r\nmembership(fg_gd)\r\n\r\n\r\n  Eric Andersen     John Barlow     Bob Bralove  Andrew Charles \r\n              2               2               2               4 \r\n    John Dawson    Willie Dixon    Jerry Garcia  Donna Godchaux \r\n              3               2               3               3 \r\n Keith Godchaux   Gerrit Graham     Frank Guida     Mickey Hart \r\n              3               2               1               3 \r\n  Robert Hunter Bill Kreutzmann       Ned Lagin       Phil Lesh \r\n              2               3               4               4 \r\n     Peter Monk   Brent Mydland     Dave Parker Robert Petersen \r\n              4               4               1               4 \r\n         Pigpen     Joe Royster   Rob Wasserman        Bob Weir \r\n              1               1               2               2 \r\n  Vince Welnick \r\n              2 \r\n\r\nPlot the Network with Community Colors\r\n\r\n\r\nShow code\r\n\r\n#plot network with community coloring\r\nplot(fg_gd,gd_giant)\r\n\r\n\r\n\r\n\r\nWalktrap Community Detection and Plot\r\nThe walktrap community detection originally made two communities; one community was the lone isolate, and the rest of the songwriters were in the other community. With just the giant component, there is just the one community.\r\n\r\n\r\nShow code\r\n\r\n#load walktrap list\r\nwt_gd <- readRDS(\"wt_gd.RData\")\r\n#inspect\r\nnames(wt_gd)\r\n\r\n\r\n[1] \"merges\"     \"modularity\" \"membership\" \"names\"      \"vcount\"    \r\n[6] \"algorithm\" \r\n\r\nShow code\r\n\r\n#plot network with community coloring\r\nplot(wt_gd,gd_giant)\r\n\r\n\r\n\r\n\r\nCompare Community Partitions - Fast and Greedy and Walktrap\r\n\r\n\r\nShow code\r\n\r\n#compare community partition modularity scores\r\nmodularity(fg_gd)\r\n\r\n\r\n[1] 0.2792899\r\n\r\n\r\n\r\nShow code\r\n\r\n#compare community partition modularity scores\r\nmodularity(wt_gd)\r\n\r\n\r\n[1] 0.2483386\r\n\r\nLeading Label Propagation Community Detection\r\nIn this evaluation, each of the nodes was indicated to be in its’ own community.\r\n\r\n\r\nShow code\r\n\r\n#load leading label list\r\nlab_gd <- readRDS(\"lab_gd.RData\")\r\n#inspect\r\nnames(lab_gd)\r\n\r\n\r\n[1] \"membership\" \"modularity\" \"names\"      \"vcount\"     \"algorithm\" \r\n\r\nShow code\r\n\r\n#plot network with community coloring\r\nplot(lab_gd,gd_giant)\r\n\r\n\r\n\r\n\r\nEdge Betweenness Community Detection\r\nAgain, each of the nodes was indicated to be in its’ own community.\r\n\r\n\r\nShow code\r\n\r\n#load edge betweenness list\r\nedge_gd <- readRDS(\"edge_gd.RData\")\r\n#inspect\r\nnames(edge_gd)\r\n\r\n\r\n[1] \"membership\" \"modularity\" \"names\"      \"vcount\"     \"algorithm\" \r\n\r\nShow code\r\n\r\n#plot network with community coloring\r\nplot(edge_gd,gd_giant)\r\n\r\n\r\n\r\n\r\nEigenvector Community Detection\r\nAnd again, each of the nodes was indicated to be in its’ own community.\r\n\r\n\r\nShow code\r\n\r\n#load eigenvector list\r\neigen_gd <- readRDS(\"eigen_gd.RData\")\r\n#inspect\r\nnames(eigen_gd)\r\n\r\n\r\n[1] \"membership\" \"modularity\" \"names\"      \"vcount\"     \"algorithm\" \r\n\r\nShow code\r\n\r\n#plot network with community coloring\r\nplot(eigen_gd,gd_giant)\r\n\r\n\r\n\r\n\r\nSpinglass Community Detection\r\nThis is the other result that actually gave the songwriters communities outside of one giant community or each in their individual community.\r\n\r\n\r\nShow code\r\n\r\n#load walktrap list\r\nspin_gd <- readRDS(\"spin_gd.RData\")\r\n#inspect\r\nnames(spin_gd)\r\n\r\n\r\n[1] \"membership\"  \"csize\"       \"modularity\"  \"temperature\"\r\n[5] \"algorithm\"   \"vcount\"      \"names\"      \r\n\r\nShow code\r\n\r\n#plot network with community coloring\r\nplot(spin_gd,gd_giant)\r\n\r\n\r\n\r\n\r\nAdding Community Membership to Node Info\r\nI have already added each of the community membership evaluations to a dataframe so I will load that data so I can continue to evaluate it.\r\n\r\n\r\nShow code\r\n\r\n#import data\r\ngd_nodes <- read_csv(\"giant_df.csv\")\r\ngd_nodes\r\n\r\n\r\n# A tibble: 25 x 9\r\n      X1 `\\xef..node.id` name      fg_gd wt_gd lab_gd edge_gd eigen_gd\r\n   <dbl>           <dbl> <chr>     <dbl> <dbl>  <dbl>   <dbl>    <dbl>\r\n 1     1               1 Eric And~     2     1      1       1        1\r\n 2     2               2 John Bar~     2     1      2       2        2\r\n 3     3               3 Bob Bral~     2     1      3       3        3\r\n 4     4               4 Andrew C~     4     1      4       4        4\r\n 5     5               5 John Daw~     3     1      5       5        5\r\n 6     6               6 Willie D~     2     1      6       6        6\r\n 7     7               7 Jerry Ga~     3     1      7       7        7\r\n 8     8               8 Donna Go~     3     1      8       8        8\r\n 9     9               9 Keith Go~     3     1      9       9        9\r\n10    10              10 Gerrit G~     2     1     10      10       10\r\n# ... with 15 more rows, and 1 more variable: spin_gd <dbl>\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/networks-community/distill-preview.png",
    "last_modified": "2022-04-10T01:59:06-05:00",
    "input_file": "networks-community.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/comparing-headlines/",
    "title": "DACSS 697D Post 6",
    "description": "Assignment 6 for DACSS 697D Course 'Text as Data': \"Comparing New York Times Print v. Web Headlines\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-04-07",
    "categories": [
      "text as data",
      "homework"
    ],
    "contents": "\r\nGetting Started\r\nIn my previous posts, I have used the data pulled by the New York Times “article search” API to analyze the 3,442 results of the search query “Afghanistan”. The search parameters have been of the years 2020 and 2021, and my overall goal has been to analyze the articles for differences in framing and use of sources compared to the sentiment of the relevant article. However, I have been limited in that the article search API for the New York Times does not pull the entire article; rather, I have been able to pull the abstract/summary, lead paragraph, and snippet for each article as well as the keywords, authors, sections, and url. In addition, I can get the article titles for both the print and online versions of the article. That last bit is important to my next phase of research, and touches on the research conducted on the same topic in last semester’s Research Design course.\r\nNew Research Path: Comparing Different Headlines\r\nIn that course, our research group hand coded PDF copies of articles resulting from a simple search on the websites of the New York Times and Wall Street Journal from Feburary 29, 2020 through September 30, 2021 using the term “Afghanistan withdrawal”. One thing I noticed was that when loading the PDF articles into NVivo for coding, it was difficult to match the New York Times articles to the citation information in Zotero for many of the articles because the article titles did not match. I realized that in the process of saving the articles in Zotero, they were saved with a title viewable on the web version of the article; however, once the article had been preserved by using the site’s “Print to PDF” function, the article title that it used as a default file name was different than the web version.\r\nSince I am somewhat limited in my research on the articles pulled by the API due to the fact that I only have the lead paragraph and not the entire article, I want to look at the differences in the fields for the “main headline” and “print headline” pulled from the API for similarity.\r\nI’ll start by loading the whole of the data from my collection phase and looking at the headlines in more detail.\r\n\r\n\r\n#load data\r\nafghanistan_articles <- read.csv(\"afghanistan.articles.headlines.csv\")\r\nafghanistan_articles <- as.data.frame(afghanistan_articles)\r\n#create subset to analyze\r\nsmall_df <- afghanistan_articles %>%\r\n  select(date, section.name, news.desk, headline.main, headline.print, material)\r\nhead(small_df)\r\n\r\n\r\n        date section.name news.desk\r\n1 12/10/2020        World  Magazine\r\n2  11/5/2020     Magazine  Magazine\r\n3 12/17/2020      Opinion      OpEd\r\n4 12/27/2020        World   Foreign\r\n5 12/10/2020        World   Foreign\r\n6 12/16/2020        World   Foreign\r\n                                                    headline.main\r\n1                       Afghan War Casualty Report: December 2020\r\n2                       Afghan War Casualty Report: November 2020\r\n3                      The Afghan War Is Over. Did Anyone Notice?\r\n4 In a Village of Widows, the Opium Trade Has Taken a Deadly Toll\r\n5   Afghan Journalist Is Killed in Latest Attack on Media Figures\r\n6           ‘Sticky Bombs’ Sow Terror and Chaos in a City on Edge\r\n                                                       headline.print\r\n1                                                                <NA>\r\n2                                                                <NA>\r\n3                                      Afghanistan: American ‘Iliad’?\r\n4        Village of Widows Scrapes By in Shadow of Afghan Opium Trade\r\n5               Afghan Journalist Is Killed in Latest Attack on Media\r\n6 Taliban Use Lethal ‘Sticky Bombs’ Nearly Daily to Terrorize Afghans\r\n  material\r\n1     News\r\n2     News\r\n3    Op-Ed\r\n4     News\r\n5     News\r\n6     News\r\n\r\nI can see that there is not always a pair of headlines/titles to review; to find our how many I’ll use the “complete.cases()” function. It tells me that of the 3,442 observations (articles), 1,547 of them have only one of the two headlines indicated. Here, I need to make a decision about inclusion/exclusion as part of pre-processing. I think that in making a text analysis, I’ll want to leave the data in.\r\n\r\n\r\nsmall_df[!complete.cases(small_df),]\r\n\r\n\r\n\r\nNow I need to look at the article headlines, independently, and create a corpus.\r\n\r\n\r\n#load individual data\r\nmain_headlines <- read_csv(\"main_headlines.csv\")\r\nprint_headlines <- read_csv(\"print_headlines.csv\")\r\n\r\nhead(main_headlines)\r\n\r\n\r\n# A tibble: 6 x 6\r\n  doc.id text                  date   section.name  news.desk material\r\n   <dbl> <chr>                 <chr>  <chr>         <chr>     <chr>   \r\n1      1 Quotation of the Day~ 1/1/2~ \"Today\\x92s ~ Summary   Quote   \r\n2      2 Afghan War Casualty ~ 1/2/2~ \"Magazine\"    Magazine  News    \r\n3      3 The 50 TV Shows You ~ 1/2/2~ \"Arts\"        Weekend   News    \r\n4      4 A History of War in ~ 1/3/2~ \"Magazine\"    Magazine  News    \r\n5      5 Airstrike Pushes Nat~ 1/3/2~ \"U.S.\"        Politics  News    \r\n6      6 The Case for a One-T~ 1/3/2~ \"Opinion\"     OpEd      Op-Ed   \r\n\r\nhead(print_headlines)\r\n\r\n\r\n# A tibble: 6 x 6\r\n  doc.id text                  date   section.name  news.desk material\r\n   <dbl> <chr>                 <chr>  <chr>         <chr>     <chr>   \r\n1      1 Quote of the Day      1/1/2~ \"Today\\x92s ~ Summary   Quote   \r\n2      2 <NA>                  1/2/2~ \"Magazine\"    Magazine  News    \r\n3      3 50 TV Shows To Watch~ 1/2/2~ \"Arts\"        Weekend   News    \r\n4      4 <NA>                  1/3/2~ \"Magazine\"    Magazine  News    \r\n5      5 Airstrike Pushes For~ 1/3/2~ \"U.S.\"        Politics  News    \r\n6      6 The Case for a One-T~ 1/3/2~ \"Opinion\"     OpEd      Op-Ed   \r\n\r\n\r\n\r\nmain_corpus <- corpus(main_headlines)\r\nprint_corpus <- corpus(print_headlines)\r\nmain_summary <- summary(main_corpus)\r\nprint_summary <- summary(print_corpus)\r\nhead(main_summary)\r\n\r\n\r\n   Text Types Tokens Sentences doc.id     date         section.name\r\n1 text1    11     11         1      1 1/1/2020 Today<U+0092>s Paper\r\n2 text2     7      7         1      2 1/2/2020             Magazine\r\n3 text3    10     10         1      3 1/2/2020                 Arts\r\n4 text4     7      7         1      4 1/3/2020             Magazine\r\n5 text5     9      9         1      5 1/3/2020                 U.S.\r\n6 text6     6      6         1      6 1/3/2020              Opinion\r\n  news.desk material\r\n1   Summary    Quote\r\n2  Magazine     News\r\n3   Weekend     News\r\n4  Magazine     News\r\n5  Politics     News\r\n6      OpEd    Op-Ed\r\n\r\nAt this point, I would like to add an indicator for the deadline type for later usage, if necessary. However, I cannot execute this function as the “summary” versions only return 100 observations. I’ll leave this for a follow up discussion.\r\n\r\n\r\nmain_summary$type <- \"Main Headline\"\r\nprint_summary$type <- \"Print Headline\"\r\n#docvars(main_corpus, field = \"type\") <- main_summary$type\r\n#docvars(print_corpus, field = \"type\") <- print_summary$type\r\n\r\n\r\n\r\nTokenization\r\nFor now, I’ll move on to tokenization\r\n\r\n\r\n# the default breaks on white space\r\nmain_tokens <- tokens(main_corpus)\r\nprint(main_tokens)\r\n\r\n\r\nTokens consisting of 3,442 documents and 5 docvars.\r\ntext1 :\r\n [1] \"Quotation\" \"of\"        \"the\"       \"Day\"       \":\"        \r\n [6] \"Ex-SEAL\"   \"Now\"       \"Pitching\"  \"Products\"  \"and\"      \r\n[11] \"President\"\r\n\r\ntext2 :\r\n[1] \"Afghan\"   \"War\"      \"Casualty\" \"Report\"   \":\"        \"January\" \r\n[7] \"2020\"    \r\n\r\ntext3 :\r\n [1] \"The\"    \"50\"     \"TV\"     \"Shows\"  \"You\"    \"Need\"   \"to\"    \r\n [8] \"Watch\"  \"This\"   \"Winter\"\r\n\r\ntext4 :\r\n[1] \"A\"       \"History\" \"of\"      \"War\"     \"in\"      \"Six\"    \r\n[7] \"Drugs\"  \r\n\r\ntext5 :\r\n[1] \"Airstrike\" \"Pushes\"    \"National\"  \"Security\"  \"to\"       \r\n[6] \"Forefront\" \"of\"        \"2020\"      \"Race\"     \r\n\r\ntext6 :\r\n[1] \"The\"      \"Case\"     \"for\"      \"a\"        \"One-Term\" \"Joe\"     \r\n\r\n[ reached max_ndoc ... 3,436 more documents ]\r\n\r\n\r\n\r\n# the default breaks on white space\r\nprint_tokens <- tokens(print_corpus)\r\nprint(print_tokens)\r\n\r\n\r\nTokens consisting of 3,442 documents and 5 docvars.\r\ntext1 :\r\n[1] \"Quote\" \"of\"    \"the\"   \"Day\"  \r\n\r\ntext2 :\r\ncharacter(0)\r\n\r\ntext3 :\r\n[1] \"50\"     \"TV\"     \"Shows\"  \"To\"     \"Watch\"  \"This\"   \"Winter\"\r\n\r\ntext4 :\r\ncharacter(0)\r\n\r\ntext5 :\r\n[1] \"Airstrike\"    \"Pushes\"       \"Foreign\"      \"Policy\"      \r\n[5] \"To\"           \"Front\"        \"of\"           \"Presidential\"\r\n[9] \"Race\"        \r\n\r\ntext6 :\r\n[1] \"The\"       \"Case\"      \"for\"       \"a\"         \"One-Term\" \r\n[6] \"President\" \"Biden\"    \r\n\r\n[ reached max_ndoc ... 3,436 more documents ]\r\n\r\nThis is a bit difficult to navigate given that it is clear that not all of the returned articles had a primary focus on Afghanistan, rather, many of them simply will have included the term “Afghanistan” somewhere in the article, even if it is ancillary to the topic. In addition, many of the results are news briefs with quick run downs of facts and very little in the way of context or framing.\r\nI will again do some pre-processing, removing punctuation. I do not want to remove numbers, as they may represent data on the situation in Afghanstan such as deaths, attacks, etc.\r\n\r\n\r\nmain_tokens <- tokens(main_corpus,\r\n                   remove_punct = TRUE)\r\nprint_tokens <- tokens(print_corpus,\r\n                   remove_punct = TRUE)\r\nmain_tokens\r\n\r\n\r\nTokens consisting of 3,442 documents and 5 docvars.\r\ntext1 :\r\n [1] \"Quotation\" \"of\"        \"the\"       \"Day\"       \"Ex-SEAL\"  \r\n [6] \"Now\"       \"Pitching\"  \"Products\"  \"and\"       \"President\"\r\n\r\ntext2 :\r\n[1] \"Afghan\"   \"War\"      \"Casualty\" \"Report\"   \"January\"  \"2020\"    \r\n\r\ntext3 :\r\n [1] \"The\"    \"50\"     \"TV\"     \"Shows\"  \"You\"    \"Need\"   \"to\"    \r\n [8] \"Watch\"  \"This\"   \"Winter\"\r\n\r\ntext4 :\r\n[1] \"A\"       \"History\" \"of\"      \"War\"     \"in\"      \"Six\"    \r\n[7] \"Drugs\"  \r\n\r\ntext5 :\r\n[1] \"Airstrike\" \"Pushes\"    \"National\"  \"Security\"  \"to\"       \r\n[6] \"Forefront\" \"of\"        \"2020\"      \"Race\"     \r\n\r\ntext6 :\r\n[1] \"The\"      \"Case\"     \"for\"      \"a\"        \"One-Term\" \"Joe\"     \r\n\r\n[ reached max_ndoc ... 3,436 more documents ]\r\n\r\nprint_tokens\r\n\r\n\r\nTokens consisting of 3,442 documents and 5 docvars.\r\ntext1 :\r\n[1] \"Quote\" \"of\"    \"the\"   \"Day\"  \r\n\r\ntext2 :\r\ncharacter(0)\r\n\r\ntext3 :\r\n[1] \"50\"     \"TV\"     \"Shows\"  \"To\"     \"Watch\"  \"This\"   \"Winter\"\r\n\r\ntext4 :\r\ncharacter(0)\r\n\r\ntext5 :\r\n[1] \"Airstrike\"    \"Pushes\"       \"Foreign\"      \"Policy\"      \r\n[5] \"To\"           \"Front\"        \"of\"           \"Presidential\"\r\n[9] \"Race\"        \r\n\r\ntext6 :\r\n[1] \"The\"       \"Case\"      \"for\"       \"a\"         \"One-Term\" \r\n[6] \"President\" \"Biden\"    \r\n\r\n[ reached max_ndoc ... 3,436 more documents ]\r\n\r\nWith the data frame subset version, I can look at the most frequent terms (features), starting with the top 20 frequent terms. it’s clear that the frequency of the “briefings” will skew my analysis. It’s also clear that the “remove_punct()” command did not remove the top result, a symbol.\r\n\r\n\r\nmain_df <- corpus_subset(main_corpus) %>%\r\n    tokens(remove_punct = TRUE) %>%\r\n    dfm()\r\ntopfeatures(main_df, n=20)\r\n\r\n\r\n   <U+FFFD>         the          in          to           a \r\n       1307        1137         734         683         653 \r\n         of        your    briefing           s         and \r\n        599         546         531         499         417 \r\nafghanistan         u.s         for      afghan       biden \r\n        391         380         318         291         271 \r\n         on     taliban          is       trump          as \r\n        265         242         227         208         206 \r\n\r\nprint_df <- corpus_subset(print_corpus) %>%\r\n    tokens(remove_punct = TRUE) %>%\r\n    dfm()\r\ntopfeatures(print_df, n=20)\r\n\r\n\r\n   <U+FFFD>         the          to          in          of \r\n        626         461         445         443         414 \r\n          a           s         u.s         and          on \r\n        406         251         249         224         195 \r\n        for      afghan afghanistan     taliban          is \r\n        191         151         149         149         140 \r\n      biden          as          at       trump         war \r\n        131         130         101         100          99 \r\n\r\nTaking a look at the distribution of word frequencies, I create a data frame. Just from looking at the data frame of the main headlines, it is clear I need to also remove stop words from my analysis.\r\n\r\n\r\nword_counts <- as.data.frame(sort(colSums(main_df),dec=T))\r\ncolnames(word_counts) <- c(\"Frequency\")\r\nword_counts$Rank <- c(1:ncol(main_df))\r\nhead(word_counts)\r\n\r\n\r\n    Frequency Rank\r\n<U+FFFD>        1307    1\r\nthe      1137    2\r\nin        734    3\r\nto        683    4\r\na         653    5\r\nof        599    6\r\n\r\nUntil I do so, analyzing the results will not be very meaningful.\r\n\r\n\r\nmain_tokens <- tokens(main_corpus)\r\nmain_tokens <- tokens_tolower(main_tokens)\r\nmain_tokens <- tokens_select(main_tokens, \r\n                             pattern = stopwords(\"en\"),\r\n                             selection = \"remove\")\r\n\r\nmain_dfm <- dfm(main_tokens)\r\n\r\nlength(main_tokens)\r\n\r\n\r\n[1] 3442\r\n\r\nprint(main_tokens)\r\n\r\n\r\nTokens consisting of 3,442 documents and 5 docvars.\r\ntext1 :\r\n[1] \"quotation\" \"day\"       \":\"         \"ex-seal\"   \"now\"      \r\n[6] \"pitching\"  \"products\"  \"president\"\r\n\r\ntext2 :\r\n[1] \"afghan\"   \"war\"      \"casualty\" \"report\"   \":\"        \"january\" \r\n[7] \"2020\"    \r\n\r\ntext3 :\r\n[1] \"50\"     \"tv\"     \"shows\"  \"need\"   \"watch\"  \"winter\"\r\n\r\ntext4 :\r\n[1] \"history\" \"war\"     \"six\"     \"drugs\"  \r\n\r\ntext5 :\r\n[1] \"airstrike\" \"pushes\"    \"national\"  \"security\"  \"forefront\"\r\n[6] \"2020\"      \"race\"     \r\n\r\ntext6 :\r\n[1] \"case\"     \"one-term\" \"joe\"     \r\n\r\n[ reached max_ndoc ... 3,436 more documents ]\r\n\r\nprint_tokens <- tokens(print_corpus)\r\nprint_tokens <- tokens_tolower(print_tokens)\r\nprint_tokens <- tokens_select(print_tokens, \r\n                             pattern = stopwords(\"en\"),\r\n                             selection = \"remove\")\r\n\r\n\r\nlength(print_tokens)\r\n\r\n\r\n[1] 3442\r\n\r\nprint(print_tokens)\r\n\r\n\r\nTokens consisting of 3,442 documents and 5 docvars.\r\ntext1 :\r\n[1] \"quote\" \"day\"  \r\n\r\ntext2 :\r\ncharacter(0)\r\n\r\ntext3 :\r\n[1] \"50\"     \"tv\"     \"shows\"  \"watch\"  \"winter\"\r\n\r\ntext4 :\r\ncharacter(0)\r\n\r\ntext5 :\r\n[1] \"airstrike\"    \"pushes\"       \"foreign\"      \"policy\"      \r\n[5] \"front\"        \"presidential\" \"race\"        \r\n\r\ntext6 :\r\n[1] \"case\"      \"one-term\"  \"president\" \"biden\"    \r\n\r\n[ reached max_ndoc ... 3,436 more documents ]\r\n\r\nNow I can use quanteda to generate the document-feature matrices\r\n\r\n\r\nmain_dfm <- dfm(main_tokens)\r\nmain_dfm\r\n\r\n\r\nDocument-feature matrix of: 3,442 documents, 5,701 features (99.87% sparse) and 5 docvars.\r\n       features\r\ndocs    quotation day : ex-seal now pitching products president\r\n  text1         1   1 1       1   1        1        1         1\r\n  text2         0   0 1       0   0        0        0         0\r\n  text3         0   0 0       0   0        0        0         0\r\n  text4         0   0 0       0   0        0        0         0\r\n  text5         0   0 0       0   0        0        0         0\r\n  text6         0   0 0       0   0        0        0         0\r\n       features\r\ndocs    afghan war\r\n  text1      0   0\r\n  text2      1   1\r\n  text3      0   0\r\n  text4      0   1\r\n  text5      0   0\r\n  text6      0   0\r\n[ reached max_ndoc ... 3,436 more documents, reached max_nfeat ... 5,691 more features ]\r\n\r\nprint_dfm <- dfm(print_tokens)\r\nprint_dfm\r\n\r\n\r\nDocument-feature matrix of: 3,442 documents, 4,121 features (99.91% sparse) and 5 docvars.\r\n       features\r\ndocs    quote day 50 tv shows watch winter airstrike pushes foreign\r\n  text1     1   1  0  0     0     0      0         0      0       0\r\n  text2     0   0  0  0     0     0      0         0      0       0\r\n  text3     0   0  1  1     1     1      1         0      0       0\r\n  text4     0   0  0  0     0     0      0         0      0       0\r\n  text5     0   0  0  0     0     0      0         1      1       1\r\n  text6     0   0  0  0     0     0      0         0      0       0\r\n[ reached max_ndoc ... 3,436 more documents, reached max_nfeat ... 4,111 more features ]\r\n\r\n# trim based on the overall frequency (i.e., the word counts) with a max at the top \"non-gibberish\" term.\r\nsmaller_main_dfm <- dfm_trim(main_dfm, max_termfreq = 1137)\r\nsmaller_print_dfm <- dfm_trim(print_dfm, max_termfreq = 461)\r\n# trim based on the proportion of documents that the feature appears in; here, \r\n# the feature needs to appear in more than 5% of documents (articles)\r\n#smaller_main_dfm <- dfm_trim(smaller_main_dfm, min_docfreq = 0.10, docfreq_type = \"prop\")\r\n#smaller_main_dfm\r\n#smaller_print_dfm <- dfm_trim(smaller_print_dfm, min_docfreq = 0.10, docfreq_type = \"prop\")\r\n#smaller_print_dfm\r\n\r\n\r\n\r\nNow I can take a look at the updated word frequency metrics:\r\n\r\n\r\n# first, we need to create a word frequency variable and the rankings\r\nword_counts <- as.data.frame(sort(colSums(smaller_main_dfm),dec=T))\r\ncolnames(word_counts) <- c(\"Frequency\")\r\nword_counts$Rank <- c(1:ncol(smaller_main_dfm))\r\nhead(word_counts)\r\n\r\n\r\n            Frequency Rank\r\n.                1090    1\r\n:                 597    2\r\nbriefing          531    3\r\ns                 499    4\r\nafghanistan       391    5\r\nu.s               380    6\r\n\r\nOn initial review, I have successfully reduced the sparsity from over 99% to >90%. But that’s still quite a sparsity percentage. I could drop terms found in the updated word count list that appear to still be punctuation and letters, if I’m sure they are not relevant to the context of my research. Also, rather than dropping the term “briefing”, I will run this analysis again, but selecting only certain types of news desks, and excluding “briefing” types of entries.\r\nFeature Co-Occurrence Matrix\r\nI’m going to again try to exclude the jibberish, and increase the number of articles being evaluated to represent those in more than 1% of the articles rather than 5%.\r\nNow I can take a look at this network of feature co-occurrences for the main headlines:\r\n\r\n\r\n# create fcm from dfm\r\nsmaller_main_fcm <- fcm(smaller_main_dfm)\r\n# check the dimensions (i.e., the number of rows and the number of columnns)\r\n# of the matrix we created\r\ndim(smaller_main_fcm)\r\n\r\n\r\n[1] 5699 5699\r\n\r\n# pull the top features\r\nmyFeatures <- names(topfeatures(smaller_main_fcm, 20))\r\n# retain only those top features as part of our matrix\r\neven_smaller_main_fcm <- fcm_select(smaller_main_fcm, pattern = myFeatures, selection = \"keep\")\r\n# check dimensions\r\ndim(even_smaller_main_fcm)\r\n\r\n\r\n[1] 20 20\r\n\r\n# compute size weight for vertices in network\r\nsize <- log(colSums(even_smaller_main_fcm))\r\n# create plot\r\ntextplot_network(even_smaller_main_fcm, vertex_size = size / max(size) * 3)\r\n\r\n\r\n\r\n\r\nand for the print headlines:\r\n\r\n\r\n# create fcm from dfm\r\nsmaller_print_fcm <- fcm(smaller_print_dfm)\r\n# check the dimensions (i.e., the number of rows and the number of columnns)\r\n# of the matrix we created\r\ndim(smaller_print_fcm)\r\n\r\n\r\n[1] 4119 4119\r\n\r\n# pull the top features\r\nmyFeatures <- names(topfeatures(smaller_print_fcm, 20))\r\n# retain only those top features as part of our matrix\r\neven_smaller_print_fcm <- fcm_select(smaller_print_fcm, pattern = myFeatures, selection = \"keep\")\r\n# check dimensions\r\ndim(even_smaller_print_fcm)\r\n\r\n\r\n[1] 20 20\r\n\r\n# compute size weight for vertices in network\r\nsize <- log(colSums(even_smaller_print_fcm))\r\n# create plot\r\ntextplot_network(even_smaller_print_fcm, vertex_size = size / max(size) * 3)\r\n\r\n\r\n\r\n\r\nFinally, I’m trying something new I found for pre-processing and word cloud modeling before going back to the start and using what I’ve learned here on the data tomorrow!\r\n\r\n\r\npreprocessing = function (doc){\r\n  doc = gsub(\"[^[:alnum:]]\",\" \",doc)\r\n  #create corpus\r\n  corpus = Corpus(VectorSource(doc))\r\n  #Removal of punctuation\r\n  corpus = tm_map(corpus, removePunctuation)\r\n  #customize my stopwords\r\n  mystopword = \"briefing\"\r\n  #Removal of stopwords\r\n  corpus = tm_map(corpus, removeWords, c(stopwords(\"english\"),mystopword))\r\n  #retun result\r\n  return(corpus)\r\n}\r\n\r\nmain_clean = preprocessing(main_corpus)\r\nprint_clean = preprocessing(print_corpus)\r\n\r\nset.seed(1234)\r\n# draw the wordcloud\r\nlibrary(wordcloud)\r\n\r\npar(mfrow=c(1,2)) # 1x2 panel plot\r\npar(mar=c(1, 3, 1, 3)) # Set the plot margin\r\npar(bg=\"black\") # set background color as black\r\npar(col.main=\"white\") # set title color as white\r\nwordcloud(main_clean, scale=c(4,.5),min.freq=3, max.words=Inf, random.order=F, \r\n          colors = brewer.pal(8, \"Set3\"))   \r\ntitle(\"Main Website Headlines\")\r\nwordcloud(print_clean, scale=c(4,.5),min.freq=3, max.words=Inf, random.order=F, \r\n          colors = brewer.pal(8, \"Set3\"))   \r\ntitle(\"Print Headlines\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/comparing-headlines/distill-preview.png",
    "last_modified": "2022-04-08T19:37:12-05:00",
    "input_file": "comparing-headlines.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-05-dacss-697d-post-5/",
    "title": "DACSS 697D Post 5",
    "description": "Assignment 5 for DACSS 697D Course 'Text as Data': \"Representing Texts\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-04-05",
    "categories": [
      "text as data",
      "homework"
    ],
    "contents": "\r\nGetting Started\r\nI have begun pulling the data I will be analyzing for my final project in the course. So far, I have pulled a collection of articles by month beginning January 2020 through December 2021 from the New York Times using their “article search” API for the search query “Afghanistan”. I have not limited my search by any filter at this time. However, I have been limited in that the article search API for the New York Times does not pull the entire article; rather, I have been able to pull the abstract/summary, lead paragraph, and snippet for each article as well as the keywords, authors, sections, and url. In addition, I can get the article titles for both the print and online versions of the article.\r\nLoading the data from my collection phase:\r\n\r\n\r\n#load data\r\nload(\"afghanistan.articles.table.RData\")\r\nafghanistan_lead <- read.csv(\"lead.paragraph.table.csv\")\r\nafghanistan_articles <- as.data.frame(afghanistan_lead)\r\n\r\n\r\n\r\nCreating a corpus from the data:\r\n\r\n\r\nafghanistan_corpus <- corpus(afghanistan_articles)\r\nafghanistan_summary <- summary(afghanistan_corpus)\r\nhead(afghanistan_corpus)\r\n\r\n\r\nCorpus consisting of 6 documents and 11 docvars.\r\ntext1 :\r\n\"“He’s a Rambo version of the same story Trump has been telli...\"\r\n\r\ntext2 :\r\n\"Sign up for our Watching newsletter to get recommendations o...\"\r\n\r\ntext3 :\r\n\"At least 87 pro-government forces and 12 civilians were kill...\"\r\n\r\ntext4 :\r\n\"TOKYO — Carlos Ghosn was aided in his escape from Japan by a...\"\r\n\r\ntext5 :\r\n\"You’re reading this week’s At War newsletter. Sign up here t...\"\r\n\r\ntext6 :\r\n\"He changed the shape of the Syrian civil war and tightened I...\"\r\n\r\nThis time I’m going to add an indicator of the search term used for this corpus, in case I want to add more search terms in the future.\r\n\r\n\r\n#of the search term used\r\nafghanistan_articles$term <- \"Afghanistan\"\r\n# add the metadata\r\ndocvars(afghanistan_corpus, field = \"term\") <- afghanistan_articles$term\r\n\r\n\r\n\r\nAnd create a dataframe of tokens:\r\n\r\n\r\nafghanistan_tokens <- tokens(afghanistan_corpus)\r\nprint(afghanistan_tokens)\r\n\r\n\r\nTokens consisting of 3,442 documents and 12 docvars.\r\ntext1 :\r\n [1] \"\\\"\"      \"He's\"    \"a\"       \"Rambo\"   \"version\" \"of\"     \r\n [7] \"the\"     \"same\"    \"story\"   \"Trump\"   \"has\"     \"been\"   \r\n[ ... and 28 more ]\r\n\r\ntext2 :\r\n [1] \"Sign\"            \"up\"              \"for\"            \r\n [4] \"our\"             \"Watching\"        \"newsletter\"     \r\n [7] \"to\"              \"get\"             \"recommendations\"\r\n[10] \"on\"              \"the\"             \"best\"           \r\n[ ... and 14 more ]\r\n\r\ntext3 :\r\n [1] \"At\"             \"least\"          \"87\"            \r\n [4] \"pro-government\" \"forces\"         \"and\"           \r\n [7] \"12\"             \"civilians\"      \"were\"          \r\n[10] \"killed\"         \"in\"             \"Afghanistan\"   \r\n[ ... and 158 more ]\r\n\r\ntext4 :\r\n [1] \"TOKYO\"  \"-\"      \"Carlos\" \"Ghosn\"  \"was\"    \"aided\"  \"in\"    \r\n [8] \"his\"    \"escape\" \"from\"   \"Japan\"  \"by\"    \r\n[ ... and 44 more ]\r\n\r\ntext5 :\r\n [1] \"You're\"     \"reading\"    \"this\"       \"week's\"     \"At\"        \r\n [6] \"War\"        \"newsletter\" \".\"          \"Sign\"       \"up\"        \r\n[11] \"here\"       \"to\"        \r\n[ ... and 13 more ]\r\n\r\ntext6 :\r\n [1] \"He\"        \"changed\"   \"the\"       \"shape\"     \"of\"       \r\n [6] \"the\"       \"Syrian\"    \"civil\"     \"war\"       \"and\"      \r\n[11] \"tightened\" \"Iran's\"   \r\n[ ... and 48 more ]\r\n\r\n[ reached max_ndoc ... 3,436 more documents ]\r\n\r\nNow I’ll use quanteda to generate the document-feature matrix from the corpus object:\r\n\r\n\r\nafghanistan_dfm <- dfm(tokens(afghanistan_corpus))\r\nafghanistan_dfm\r\n\r\n\r\nDocument-feature matrix of: 3,442 documents, 14,101 features (99.75% sparse) and 12 docvars.\r\n       features\r\ndocs    \" he's a rambo version of the same story trump\r\n  text1 2    1 1     1       1  1   4    1     1     1\r\n  text2 0    0 0     0       0  0   1    0     0     0\r\n  text3 0    0 3     0       0  1   9    0     0     0\r\n  text4 0    0 2     0       0  1   4    0     0     0\r\n  text5 0    0 0     0       0  0   0    0     0     0\r\n  text6 0    0 0     0       0  4   5    0     0     0\r\n[ reached max_ndoc ... 3,436 more documents, reached max_nfeat ... 14,091 more features ]\r\n\r\nThis is a bit more difficult to navigate given that my data is not subdivided by chapter, rather there is an individual record for each of the 3,442 articles. Perhaps I will do some pre-processing as part of the matrix creation. Going Removing punctuation, numbers, capitalization, and stopwords as a comparison:\r\n\r\n\r\n# create the dfm\r\nafghanistan_dfm <- tokens(afghanistan_corpus,\r\n                                    remove_punct = TRUE,\r\n                                    remove_numbers = TRUE) %>%\r\n                           dfm(tolower=TRUE) %>%\r\n                           dfm_remove(stopwords('english'))\r\n# find out a quick summary of the dfm\r\nafghanistan_dfm\r\n\r\n\r\nDocument-feature matrix of: 3,442 documents, 13,624 features (99.84% sparse) and 12 docvars.\r\n       features\r\ndocs    rambo version story trump telling deep state trying screw\r\n  text1     1       1     1     1       1    1     1      1     1\r\n  text2     0       0     0     0       0    0     0      0     0\r\n  text3     0       0     0     0       0    0     0      0     0\r\n  text4     0       0     0     0       0    0     0      0     0\r\n  text5     0       0     0     0       0    0     0      0     0\r\n  text6     0       0     0     0       0    0     0      0     0\r\n       features\r\ndocs    media\r\n  text1     1\r\n  text2     0\r\n  text3     0\r\n  text4     0\r\n  text5     0\r\n  text6     0\r\n[ reached max_ndoc ... 3,436 more documents, reached max_nfeat ... 13,614 more features ]\r\n\r\nWith the more simplified version, I can look at the most frequent terms (features), starting with the top 20 frequent terms.\r\n\r\n\r\ntopfeatures(afghanistan_dfm, 20)\r\n\r\n\r\nafghanistan   president     taliban  washington    american \r\n        963         624         507         460         436 \r\n      kabul         new      united    military         u.s \r\n        404         388         378         375         364 \r\n       said       biden         war      states         get \r\n        358         357         352         343         302 \r\n     afghan         two         one      people       trump \r\n        290         280         257         247         236 \r\n\r\nI would really like to be able to look at the most frequent terms by month/year/section, but have not been able to get that command to run properly yet.\r\n\r\n\r\n#world_words <- as.vector(colSums(afghanistan_dfm) == afghanistan_dfm$section.name[\"World\"])\r\n#head(colnames(afghanistan_dfm)[world_words])\r\n\r\n\r\n\r\nLooking at word frequencies\r\n\r\n\r\n# programs often work with random initializations, yielding different outcomes.\r\n# we can set a standard starting point though to ensure the same output.\r\nset.seed(1234)\r\n# draw the wordcloud\r\ntextplot_wordcloud(afghanistan_dfm, min_count = 50, random_order = FALSE)\r\n\r\n\r\n\r\n\r\nTaking a look at the distribution of word frequencies, I first create a dataframe. Unfortunately, I still need to figure out how to remove the top 2 more frequent occurrences as not relevant to the analysis, likely an import failure from the NYT.\r\n\r\n\r\n# first, we need to create a word frequency variable and the rankings\r\nword_counts <- as.data.frame(sort(colSums(afghanistan_dfm),dec=T))\r\ncolnames(word_counts) <- c(\"Frequency\")\r\nword_counts$Rank <- c(1:ncol(afghanistan_dfm))\r\nhead(word_counts)\r\n\r\n\r\n            Frequency Rank\r\nafghanistan       963    1\r\npresident         624    2\r\ntaliban           507    3\r\nwashington        460    4\r\namerican          436    5\r\nkabul             404    6\r\n\r\nUntil I do so, plotting the results isn’t very informative.\r\n\r\n\r\nggplot(word_counts, mapping = aes(x = Rank, y = Frequency)) + \r\n  geom_point() +\r\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nPerhaps I can update the dataframe to exclude those top jibberish results.\r\n\r\n\r\n# trim based on the overall frequency (i.e., the word counts) with a max at the top \"non-gibberish\" term.\r\nsmaller_dfm <- dfm_trim(afghanistan_dfm, max_termfreq = 1043)\r\n# trim based on the proportion of documents that the feature appears in; here, \r\n# the feature needs to appear in more than 5% of documents (articles)\r\nsmaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = 0.05, docfreq_type = \"prop\")\r\nsmaller_dfm\r\n\r\n\r\nDocument-feature matrix of: 3,442 documents, 31 features (90.95% sparse) and 12 docvars.\r\n       features\r\ndocs    trump people get afghanistan week taliban one afghan two\r\n  text1     1      1   0           0    0       0   0      0   0\r\n  text2     0      0   1           0    0       0   0      0   0\r\n  text3     0      0   0           1    2       2   1      1   1\r\n  text4     0      0   0           0    0       0   0      0   0\r\n  text5     0      0   1           0    0       0   0      0   0\r\n  text6     0      0   0           0    0       0   0      0   1\r\n       features\r\ndocs    american\r\n  text1        0\r\n  text2        0\r\n  text3        1\r\n  text4        1\r\n  text5        0\r\n  text6        1\r\n[ reached max_ndoc ... 3,436 more documents, reached max_nfeat ... 21 more features ]\r\n\r\nNow I can take a look again at the wordcloud and word frequency metrics:\r\n\r\n\r\n# programs often work with random initializations, yielding different outcomes.\r\n# we can set a standard starting point though to ensure the same output.\r\nset.seed(1234)\r\n# draw the wordcloud\r\ntextplot_wordcloud(smaller_dfm, min_count = 1, random_order = FALSE)\r\n\r\n\r\n\r\n\r\n\r\n\r\n# first, we need to create a word frequency variable and the rankings\r\nword_counts <- as.data.frame(sort(colSums(smaller_dfm),dec=T))\r\ncolnames(word_counts) <- c(\"Frequency\")\r\nword_counts$Rank <- c(1:ncol(smaller_dfm))\r\nword_counts\r\n\r\n\r\n            Frequency Rank\r\nafghanistan       963    1\r\npresident         624    2\r\ntaliban           507    3\r\nwashington        460    4\r\namerican          436    5\r\nkabul             404    6\r\nnew               388    7\r\nunited            378    8\r\nmilitary          375    9\r\nu.s               364   10\r\nsaid              358   11\r\nbiden             357   12\r\nwar               352   13\r\nstates            343   14\r\nget               302   15\r\nafghan            290   16\r\ntwo               280   17\r\none               257   18\r\npeople            247   19\r\ntrump             236   20\r\ncountry           234   21\r\nofficials         234   22\r\nwant              231   23\r\ngovernment        226   24\r\ncoronavirus       226   25\r\nyears             225   26\r\nlast              220   27\r\ntroops            218   28\r\nweek              216   29\r\nfirst             213   30\r\nsign-up           212   31\r\n\r\n\r\n\r\nggplot(word_counts, mapping = aes(x = Rank, y = Frequency)) + \r\n  geom_point() +\r\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nOn initial review, I have successfully reduced the sparsity from over 99$ to ~90%. But that’s still quite a sparsity percentage. I could drop terms found in the updated word count list such as “one”, “two” and “get”, if I’m sure they are not relevant to the context of my research.\r\nFeature Co-Occurrence Matrix\r\nI’m going to again try to exclude the jibberish, and increase the number of articles being evaluated to represent those in more than 1% of the articles rather than 5%.\r\nNow I can take a look at this network of feature co-occurrences:\r\n\r\n\r\n# create fcm from dfm\r\nsmaller_fcm <- fcm(smaller_dfm)\r\n# check the dimensions (i.e., the number of rows and the number of columnns)\r\n# of the matrix we created\r\ndim(smaller_fcm)\r\n\r\n\r\n[1] 31 31\r\n\r\n# pull the top features\r\nmyFeatures <- names(topfeatures(smaller_fcm, 30))\r\n# retain only those top features as part of our matrix\r\neven_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\r\n# check dimensions\r\ndim(even_smaller_fcm)\r\n\r\n\r\n[1] 30 30\r\n\r\n# compute size weight for vertices in network\r\nsize <- log(colSums(even_smaller_fcm))\r\n# create plot\r\ntextplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)\r\n\r\n\r\n\r\n\r\nI am still not confident that the models I am creating are truly able to assist me with my original project topic, but this tutorial and process has definitely expanded my knowledge of the area of representing texts through these methods.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-05-dacss-697d-post-5/distill-preview.png",
    "last_modified": "2022-04-08T19:37:51-05:00",
    "input_file": "dacss-697d-post-5.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-dacss-603-assignment-3/",
    "title": "DACSS 603 Assignment 3",
    "description": "Assignment 3 for DACSS 603 course 'Quantitative Data Analysis': \"Multiple Regression\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-03-31",
    "categories": [
      "statistics",
      "homework"
    ],
    "contents": "\r\nQuestion 1\r\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is “ŷ = −10,536 + 53.8x1 + 2.84x2”.\r\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\r\n\r\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\r\n\r\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\r\n\r\nResponse 1\r\nA: Using the prediction equation, the predicted selling price of this home would be $107,296. Since the home sold for $145,000, the residual is $37,704. I interpret this result to mean that the home seller was able to get a much better price for their home than the market prediction.\r\n\r\n\r\nShow code\r\n\r\nx1 <- 1240 #square feet of the home\r\nx2 <- 18000 #square feet of the lot\r\ny <- 145000 #selling price of the home\r\n\r\n# using the prediction equation here:\r\n\r\nybar1 <- (-10536)+(53.8*x1)+(2.84*x2)\r\n\r\nresidual1 <- y-ybar1\r\n\r\nybar1\r\n\r\n\r\n[1] 107296\r\n\r\nShow code\r\n\r\nresidual1\r\n\r\n\r\n[1] 37704\r\n\r\nB: The explanatory variable “square feet of the home” has a slope coefficient of 53.8. This means that for every increase in that variable, the predicted price of the home will increase by $53.80. I can confirm that this is the case by changing my calculations accordingly:\r\n\r\n\r\nShow code\r\n\r\nx1b <- 1241 #square feet of the home\r\nx2b <- 18000 #square feet of the lot\r\nyb <- 145000 #selling price of the home\r\n\r\nybar1b <- (-10536)+(53.8*x1b)+(2.84*x2b)\r\n\r\nresidual1b <- yb-ybar1b\r\n\r\nybar1b-ybar1 #difference in predicted home price given increase in quare feet of the home by \"1\"\r\n\r\n\r\n[1] 53.8\r\n\r\nC: The explanatory variable “square feet of the lot” has a slope coefficient of 2.84. This means that for every increase in that variable, the predicted price of the home will increase by $2.84. I can then calculate that it would take an increase of ~18.94 in lot size to have a proportionate increase in price to an increase of one square foot in home size.\r\n\r\n\r\nShow code\r\n\r\nneed <- 53.8/2.84\r\n\r\nneed\r\n\r\n\r\n[1] 18.94366\r\n\r\nQuestion 2\r\nThe data file (alr4 R Package) concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\r\nResponse 2\r\nFirst I’m loading the “salary” data and inspecting it.\r\n\r\n\r\nShow code\r\n\r\ndata(\"salary\") \r\ndim(salary)\r\n\r\n\r\n[1] 52  6\r\n\r\nShow code\r\n\r\nhead(salary)\r\n\r\n\r\n   degree rank    sex year ysdeg salary\r\n1 Masters Prof   Male   25    35  36350\r\n2 Masters Prof   Male   13    22  35350\r\n3 Masters Prof   Male   10    23  28200\r\n4 Masters Prof Female    7    27  26775\r\n5     PhD Prof   Male   19    30  33696\r\n6 Masters Prof   Male   16    21  28516\r\n\r\nPart A.\r\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\r\nClarifying the null and alternative hypotheses:\r\nH0: The mean salary for men and women is the same, without regard to any other variable but sex.\r\nHa: The mean salary for men and women is NOT the same, without regard to any other variable but sex.\r\nWe have n > 30, so I can assume a normal distribution.\r\nThe t-test result indicates that the mean salary for men and women is not equal ($24,696.79 for men, $21,357.14 for women). However, the p-value is 0.0706. Since this p-value indicates a significance level of % (p > 0.05), I fail to reject the null hypothesis.\r\n\r\n\r\nShow code\r\n\r\n# Conduct t.test to determine confidence level at default of 0.95\r\nt.test(salary~sex, data = salary, var.equal = TRUE)\r\n\r\n\r\n\r\n    Two Sample t-test\r\n\r\ndata:  salary by sex\r\nt = 1.8474, df = 50, p-value = 0.0706\r\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\r\n95 percent confidence interval:\r\n -291.257 6970.550\r\nsample estimates:\r\n  mean in group Male mean in group Female \r\n            24696.79             21357.14 \r\n\r\nPart B.\r\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\r\nI ran the multiple linear regression using the “lm()” function using all predictors and salary as the outcome variable. Then, the function “confint()” produced the corresponding confidence intervals from the model. This produced a confidence interval, at the default of 0.95, that the difference in salary between males and females is [(-$697.82) to ($3,030.56)]\r\n\r\n\r\nShow code\r\n\r\n#Linear regression on all predictors\r\n\r\nmlm2 <- lm(salary~., data = salary)\r\n\r\nsummary(mlm2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ ., data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nShow code\r\n\r\nconfint(mlm2, \"sexFemale\")\r\n\r\n\r\n              2.5 %   97.5 %\r\nsexFemale -697.8183 3030.565\r\n\r\nPart C.\r\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variable\r\nI can look at these relationships looking at both the output from the “lm()” function in Part B and the “confint()” function. However, I also found a great solution to represent these relationships using the “broom” package. Using the function “tidy()” from the “broom” package, I can create a tibble from the results of my “lm()” call.\r\n\r\n\r\nShow code\r\n\r\n#Manipulate the output tibble and convert scientific notation of p-value to decimals\r\n\r\ntidymlm2 <- tidy(mlm2, conf.int = TRUE)\r\n\r\noptions(scipen = 999)\r\n\r\ntidymlm2\r\n\r\n\r\n# A tibble: 7 x 7\r\n  term        estimate std.error statistic  p.value conf.low conf.high\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\r\n1 (Intercept)   15746.     800.      19.7  9.76e-24   14134.   17358. \r\n2 degreePhD      1389.    1019.       1.36 1.80e- 1    -663.    3440. \r\n3 rankAssoc      5292.    1145.       4.62 3.22e- 5    2985.    7599. \r\n4 rankProf      11119.    1352.       8.23 1.62e-10    8396.   13841. \r\n5 sexFemale      1166.     926.       1.26 2.14e- 1    -698.    3031. \r\n6 year            476.      94.9      5.02 8.65e- 6     285.     667. \r\n7 ysdeg          -125.      77.5     -1.61 1.15e- 1    -281.      31.5\r\n\r\nReviewing this information I can see:\r\nFor the predictor variable “degreePhD”, the statistical significance of its’ relationship to salary is an increase in salary of ~$1,388.61 given a PhD. The p-value of 0.18 indicates that this is not statistically significant.\r\nFor the predictor variable “rankAssoc”, the statistical significance of its’ relationship to salary is an increase in salary of ~$5,292.36 given achieving the rank of Associate. The p-value of 0.0000322 indicates that this result is statistically significant to the 0.99 confidence level.\r\nFor the predictor variable “rankProf”, the statistical significance of its’ relationship to salary is an increase in salary of ~$11,118.76.36 given achieving the rank of Professor. The p-value of 0.000000000162 indicates that this result is statistically significant to the 0.99 confidence level.\r\nFor the predictor variable “sexFemale”, the statistical significance of its’ relationship to salary is an increase in salary of ~$1,166.37 given the salary being for a female. The p-value of 0.214 indicates that this result is not statistically significant.\r\nFor the predictor variable “year”, the statistical significance of its’ relationship to salary is an increase in salary of ~$476.31 given the salary for each year of experience in current rank. The p-value of 0.00000865 indicates that this result is statistically significant.\r\nFor the predictor variable “ysdeg”, the statistical significance of its’ relationship to salary is a decrease in salary of ~$124.57 given the salary for each year since highest degree achieved. The p-value of 0.12 indicates that this result is not statistically significant.\r\nSummarizing, the predictor variables, “degreePhD”, “sexFemale”, and “ysdeg” are not statistically significant, while predictor variables “rankAssoc”, “rankProf”, and “year” are statistically significant to the 99% confidence level. In addition, all of the predictor variables have a positive linear relationship except for the variable “ysdeg” to salary, which has a negative linear relationship.\r\nPart D.\r\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\r\n\r\n\r\nShow code\r\n\r\n# change baseline rank\r\nnew2d <- relevel(salary$rank, \"Prof\")\r\n\r\n# fit model again\r\nmlm2d <- lm(salary ~ degree + sex + year + ysdeg + new2d, data = salary)\r\n\r\n# get summary\r\nsummary(mlm2d)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + sex + year + ysdeg + new2d, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept)  26864.81    1375.29  19.534 < 0.0000000000000002 ***\r\ndegreePhD     1388.61    1018.75   1.363                0.180    \r\nsexFemale     1166.37     925.57   1.260                0.214    \r\nyear           476.31      94.91   5.018       0.000008653790 ***\r\nysdeg         -124.57      77.49  -1.608                0.115    \r\nnew2dAsst   -11118.76    1351.77  -8.225       0.000000000162 ***\r\nnew2dAssoc   -5826.40    1012.93  -5.752       0.000000727809 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 0.00000000000000022\r\n\r\nChanging the baseline for “rank” and looking at the coefficients of variables, the “Asst” rank has an estimate of a lower salary of ~$11,118.76, and the “Assoc” rank has an estimate of a lower salary of ~$5,826.40. Both are statistically significant to the 99% confidence level. This is the same information from the fit test in part C. We have just changed the base reference from “Asst” to “Prof”.\r\nPart E.\r\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.\r\n\r\n\r\nShow code\r\n\r\n#Linear regression without the rank\r\n\r\nmlm2e <- lm(salary ~ sex + degree + year + ysdeg, data = salary)\r\nsummary(mlm2e)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ sex + degree + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-8146.9 -2186.9  -491.5  2279.1 11186.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept) 17183.57    1147.94  14.969 < 0.0000000000000002 ***\r\nsexFemale   -1286.54    1313.09  -0.980             0.332209    \r\ndegreePhD   -3299.35    1302.52  -2.533             0.014704 *  \r\nyear          351.97     142.48   2.470             0.017185 *  \r\nysdeg         339.40      80.62   4.210             0.000114 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3744 on 47 degrees of freedom\r\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \r\nF-statistic: 20.11 on 4 and 47 DF,  p-value: 0.000000001048\r\n\r\nShow code\r\n\r\n#Manipulate the output tibble and convert scientific notation of p-value to decimals\r\n\r\ntidymlm2e <- tidy(mlm2e, conf.int = TRUE)\r\noptions(scipen = 999)\r\ntidymlm2e\r\n\r\n\r\n# A tibble: 5 x 7\r\n  term        estimate std.error statistic  p.value conf.low conf.high\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\r\n1 (Intercept)   17184.    1148.     15.0   1.66e-19  14874.     19493.\r\n2 sexFemale     -1287.    1313.     -0.980 3.32e- 1  -3928.      1355.\r\n3 degreePhD     -3299.    1303.     -2.53  1.47e- 2  -5920.      -679.\r\n4 year            352.     142.      2.47  1.72e- 2     65.3      639.\r\n5 ysdeg           339.      80.6     4.21  1.14e- 4    177.       502.\r\n\r\nFor the predictor variable “sexFemale”, the statistical significance of its’ relationship to salary is a decrease in salary of ~$1,286.54 given the salary being for a female. The p-value of 0.332 indicates that this result is not statistically significant.\r\nFor the predictor variable “degreePhD”, the statistical significance of its’ relationship to salary is a decrease in salary of ~$3,299.35 given the salary being for a female. The p-value of 0.0147 indicates that this result is statistically significant to the .95 confidence level.\r\nFor the predictor variable “year”, the statistical significance of its’ relationship to salary is an increase in salary of ~$351.97 given the salary for each year of experience in current rank. The p-value of 0.0147 indicates that this result is statistically significant to the .95 confidence level.\r\nFor the predictor variable “ysdeg”, the statistical significance of its’ relationship to salary is an increase in salary of ~$339.40 given the salary for each year since highest degree achieved. The p-value of 0.000114 indicates that this result is statistically significant to the .99 confidence level.\r\nSummarizing, eliminating the “rank” variable, the predictor variables, degreePhD”, “year”, and “ysdeg” are statistically significant to at least the 95% confidence level, while predictor variable “sexFemale” is not statistically significant. The predictor variables “year” and “ysdeg” have a positive linear relationship and the predictor variables “sexFemale” and “degreePhD” have a negative linear relationship.\r\nPractically, this tells me that eliminating “rank” before comparing salaries between males and females shows a different linear relationship than when “rank” was involved (negative vs. positive). However, it also tells me that the relationship remains statistically not significant to a reasonable level of confidence.\r\nPart F.\r\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\r\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\r\nI am creating a new variable for the dummy variable indicating whether it was “Dean 1” or “Dean 2” doing the hiring. “Dean 1” represents the “old” Dean and “Dean 2” represents the “new” Dean appointed 15 years ago. I will run the model with as few predictor variables as is practical to reduce the concern of multicollinearity, or the phenomemon of predictor variables being correlated with one another and contributing to unreliable inferences.\r\nClarifying the null and alternative hypotheses:\r\nH0: The mean salary for hires of Dean 2 are higher than the mean salary for hires of Dean 1\r\nHa: The mean salary for hires of Dean 2 are equal to or less than than the mean salary for hires of Dean 1\r\nWe have n > 30, so I can assume a normal distribution.\r\n\r\n\r\nShow code\r\n\r\n# create new variable\r\ndf2f <- salary %>%\r\n  mutate(dean = case_when(\r\n    ysdeg >= 1 & ysdeg <= 15 ~ \"2\",\r\n    ysdeg >= 16 ~ \"1\"\r\n  ))\r\n\r\nmlm2f <- lm(salary ~ ., data = df2f)\r\n\r\nsummary(mlm2f)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ ., data = df2f)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3621.2 -1336.8  -271.6   530.1  9247.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value       Pr(>|t|)    \r\n(Intercept) 13767.69    1744.31   7.893 0.000000000575 ***\r\ndegreePhD    1135.00    1031.16   1.101          0.277    \r\nrankAssoc    5234.01    1138.47   4.597 0.000035985932 ***\r\nrankProf    11411.45    1362.02   8.378 0.000000000116 ***\r\nsexFemale    1084.09     921.49   1.176          0.246    \r\nyear          460.35      95.09   4.841 0.000016263785 ***\r\nysdeg         -47.86      97.71  -0.490          0.627    \r\ndean2        1749.09    1372.83   1.274          0.209    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2382 on 44 degrees of freedom\r\nMultiple R-squared:  0.8602,    Adjusted R-squared:  0.838 \r\nF-statistic: 38.68 on 7 and 44 DF,  p-value: < 0.00000000000000022\r\n\r\nBased on this summary, I can see that the hires of Dean 2 are expected to make a salary of ~$1,749.09 than the hires of Dean 1. This result has a p-value of 0.209. Since this p-value indicates a significance level of % (p > 0.05), I fail to reject the null hypothesis.\r\n\r\n\r\nShow code\r\n\r\n# Removing the variable \"ysdeg\"\r\n\r\nmlm2g <- lm(salary ~ . - ysdeg, data = df2f)\r\n\r\nsummary(mlm2g)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ . - ysdeg, data = df2f)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3403.3 -1387.0  -167.0   528.2  9233.8 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value         Pr(>|t|)    \r\n(Intercept) 13328.38    1483.38   8.985 0.00000000001330 ***\r\ndegreePhD     818.93     797.48   1.027           0.3100    \r\nrankAssoc    4972.66     997.17   4.987 0.00000961362451 ***\r\nrankProf    11096.95    1191.00   9.317 0.00000000000454 ***\r\nsexFemale     907.14     840.54   1.079           0.2862    \r\nyear          434.85      78.89   5.512 0.00000164625970 ***\r\ndean2        2163.46    1072.04   2.018           0.0496 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2362 on 45 degrees of freedom\r\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \r\nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 0.00000000000000022\r\n\r\nRunning the model with less variables does change the adjusted R-squared and goodness of fit; though some more than others, after running this model with many different variable combinations. The best fit is seemingly the one including the new “Dean” variable but without the “ysdeg” variable.\r\nQuestion 3\r\nUsing the data file in the SMSS R package “house.selling.price”:\r\nAnswer 3\r\nI’m loading the “house.selling.price” data and inspecting it.\r\n\r\n\r\nShow code\r\n\r\ndata(\"house.selling.price\") \r\nhsp <- house.selling.price\r\ndim(hsp)\r\n\r\n\r\n[1] 100   7\r\n\r\nShow code\r\n\r\nhead(hsp)\r\n\r\n\r\n  case Taxes Beds Baths New  Price Size\r\n1    1  3104    4     2   0 279900 2048\r\n2    2  1173    2     1   0 146500  912\r\n3    3  3076    4     2   0 237700 1654\r\n4    4  1608    3     2   0 200000 2068\r\n5    5  1454    3     3   0 159900 1477\r\n6    6  2997    3     2   1 499900 3153\r\n\r\nPart A.\r\nA. Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\r\n\r\n\r\nShow code\r\n\r\nmlm3a <- lm(Price ~ Size + New, data = house.selling.price)\r\n\r\nsummary(mlm3a)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738              0.00737 ** \r\nSize           116.132      8.795  13.204 < 0.0000000000000002 ***\r\nNew          57736.283  18653.041   3.095              0.00257 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 0.00000000000000022\r\n\r\nPart B.\r\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\r\nThe coefficient for homes that are “new” indicates that the “new” variable results in a price increase of ~$57,736.28.\r\nThe coefficient for home “size” indicates that for each square foot of home size, the price of a home increases by ~$116.13.\r\nThe p-values of 0.00257 for “new” and indicates a significance level of (p < 0.01), indicating the results are statistically significant to the 99% confidence level.\r\nThe equation to indicate price for new homes:\r\n(-40230.87) + (116.13)(x) + (57,736.18)\r\nThe equation to indicate price for not new homes:\r\n(-40230.87) + (116.13)(x)\r\nPart C.\r\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n\r\n\r\nShow code\r\n\r\n-40230.87+(116.13*3000)+57736.18\r\n\r\n\r\n[1] 365895.3\r\n\r\nShow code\r\n\r\n-40230.87+(116.13*3000)\r\n\r\n\r\n[1] 308159.1\r\n\r\nFor a 3000 square foot home that is new, the price can be estimated to be $365,895.30.\r\nFor a 3000 square foot home that is not new, the price can be estimated to be $308,159.10.\r\nPart D.\r\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\r\n\r\n\r\nShow code\r\n\r\nmlm3d <- lm(Price ~ Size + New + Size*New, house.selling.price)\r\n\r\nsummary(mlm3d)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-175748  -28979   -6260   14693  192519 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept) -22227.808  15521.110  -1.432              0.15536    \r\nSize           104.438      9.424  11.082 < 0.0000000000000002 ***\r\nNew         -78527.502  51007.642  -1.540              0.12697    \r\nSize:New        61.916     21.686   2.855              0.00527 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 52000 on 96 degrees of freedom\r\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \r\nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 0.00000000000000022\r\n\r\nFitting the model where the “size” variable interacts with the “new” variable, the result is statistically significant with a p-value of 0.00527.\r\nPart E.\r\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\r\n\r\n\r\nShow code\r\n\r\nqplot(x = Price, y = Size, facets = ~ New, data = mlm3d) +\r\n  geom_smooth(method = \"lm\", se=TRUE,fullrange=TRUE,color=\"goldenrod\") + \r\n     labs(title= \"Price and Size of Homes Given Not New and New\",\r\n        x= \"Price\",\r\n        y = \"Size in Square Feet\") +\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\nIn the course of investigation options for graphics on visualizing multiple regression models, I also found an interesting package that uses “ggPredict()” to make a really visually pleasing representation, if not as practical:\r\n\r\n\r\nShow code\r\n\r\nggPredict(mlm3d,se=TRUE,interactive=TRUE)\r\n\r\n\r\n\r\n{\"x\":{\"html\":\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<svg xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' id='svg_d8822ff7-86b5-4381-8b07-55c802e256d5' viewBox='0 0 432 360'>\\n <defs>\\n  <clipPath id='svg_d8822ff7-86b5-4381-8b07-55c802e256d5_c1'>\\n   <rect x='0' y='0' width='432' height='360'/>\\n  <\\/clipPath>\\n  <clipPath id='svg_d8822ff7-86b5-4381-8b07-55c802e256d5_c2'>\\n   <rect x='52.72' y='5.48' width='311.99' height='323.02'/>\\n  <\\/clipPath>\\n <\\/defs>\\n <g>\\n  <g clip-path='url(#svg_d8822ff7-86b5-4381-8b07-55c802e256d5_c1)'>\\n   <rect x='0' y='0' width='432' height='360' fill='#FFFFFF' stroke='#FFFFFF' stroke-width='0.75' stroke-linejoin='round' stroke-linecap='round'/>\\n   <rect x='0' y='0' width='432' height='360' fill='#FFFFFF' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='round'/>\\n  <\\/g>\\n  <g clip-path='url(#svg_d8822ff7-86b5-4381-8b07-55c802e256d5_c2)'>\\n   <rect x='52.72' y='5.48' width='311.99' height='323.02' fill='#EBEBEB' stroke='none'/>\\n   <polyline points='52.72,323.92 364.71,323.92' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,241.52 364.71,241.52' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,159.11 364.71,159.11' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,76.71 364.71,76.71' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='74.37,328.50 74.37,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='149.00,328.50 149.00,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='223.64,328.50 223.64,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='298.28,328.50 298.28,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,282.72 364.71,282.72' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,200.31 364.71,200.31' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,117.91 364.71,117.91' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,35.50 364.71,35.50' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='111.69,328.50 111.69,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='186.32,328.50 186.32,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='260.96,328.50 260.96,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='335.60,328.50 335.60,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polygon points='66.90,269.69 81.83,261.76 96.76,253.79 111.69,245.76 126.61,237.61 141.54,229.30 156.47,220.74 171.40,211.94 186.32,202.92 201.25,193.78 216.18,184.56 231.11,175.29 246.03,165.99 260.96,156.68 275.89,147.34 290.82,138.00 305.74,128.65 320.67,119.29 335.60,109.93 350.53,100.57 350.53,121.72 335.60,129.57 320.67,137.42 305.74,145.28 290.82,153.14 275.89,161.01 260.96,168.89 246.03,176.79 231.11,184.70 216.18,192.64 201.25,200.63 186.32,208.71 171.40,216.91 156.47,225.31 141.54,233.97 126.61,242.87 111.69,251.94 96.76,261.11 81.83,270.36 66.90,279.64' fill='#132B43' fill-opacity='0.2' stroke='none'/>\\n   <polyline points='66.90,269.69 81.83,261.76 96.76,253.79 111.69,245.76 126.61,237.61 141.54,229.30 156.47,220.74 171.40,211.94 186.32,202.92 201.25,193.78 216.18,184.56 231.11,175.29 246.03,165.99 260.96,156.68 275.89,147.34 290.82,138.00 305.74,128.65 320.67,119.29 335.60,109.93 350.53,100.57' fill='none' stroke='none'/>\\n   <polyline points='350.53,121.72 335.60,129.57 320.67,137.42 305.74,145.28 290.82,153.14 275.89,161.01 260.96,168.89 246.03,176.79 231.11,184.70 216.18,192.64 201.25,200.63 186.32,208.71 171.40,216.91 156.47,225.31 141.54,233.97 126.61,242.87 111.69,251.94 96.76,261.11 81.83,270.36 66.90,279.64' fill='none' stroke='none'/>\\n   <polygon points='66.90,279.81 81.83,267.58 96.76,255.32 111.69,243.02 126.61,230.67 141.54,218.24 156.47,205.70 171.40,193.00 186.32,180.09 201.25,166.86 216.18,153.26 231.11,139.27 246.03,124.93 260.96,110.32 275.89,95.52 290.82,80.58 305.74,65.56 320.67,50.47 335.60,35.33 350.53,20.16 350.53,52.55 335.60,64.80 320.67,77.08 305.74,89.40 290.82,101.80 275.89,114.28 260.96,126.89 246.03,139.70 231.11,152.78 216.18,166.20 201.25,180.02 186.32,194.21 171.40,208.71 156.47,223.43 141.54,238.31 126.61,253.30 111.69,268.36 96.76,283.48 81.83,298.64 66.90,313.82' fill='#56B1F7' fill-opacity='0.2' stroke='none'/>\\n   <polyline points='66.90,279.81 81.83,267.58 96.76,255.32 111.69,243.02 126.61,230.67 141.54,218.24 156.47,205.70 171.40,193.00 186.32,180.09 201.25,166.86 216.18,153.26 231.11,139.27 246.03,124.93 260.96,110.32 275.89,95.52 290.82,80.58 305.74,65.56 320.67,50.47 335.60,35.33 350.53,20.16' fill='none' stroke='none'/>\\n   <polyline points='350.53,52.55 335.60,64.80 320.67,77.08 305.74,89.40 290.82,101.80 275.89,114.28 260.96,126.89 246.03,139.70 231.11,152.78 216.18,166.20 201.25,180.02 186.32,194.21 171.40,208.71 156.47,223.43 141.54,238.31 126.61,253.30 111.69,268.36 96.76,283.48 81.83,298.64 66.90,313.82' fill='none' stroke='none'/>\\n   <circle cx='189.91' cy='167.39' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='1' title='1&amp;lt;br/&amp;gt;Size=2048&amp;lt;br/&amp;gt;Price=279900'/>\\n   <circle cx='105.12' cy='222.36' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='2' title='2&amp;lt;br/&amp;gt;Size=912&amp;lt;br/&amp;gt;Price=146500'/>\\n   <circle cx='160.5' cy='184.78' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='3' title='3&amp;lt;br/&amp;gt;Size=1654&amp;lt;br/&amp;gt;Price=237700'/>\\n   <circle cx='191.4' cy='200.31' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='4' title='4&amp;lt;br/&amp;gt;Size=2068&amp;lt;br/&amp;gt;Price=200000'/>\\n   <circle cx='147.29' cy='216.84' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='5' title='5&amp;lt;br/&amp;gt;Size=1477&amp;lt;br/&amp;gt;Price=159900'/>\\n   <circle cx='272.38' cy='76.75' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='6' title='6&amp;lt;br/&amp;gt;Size=3153&amp;lt;br/&amp;gt;Price=499900'/>\\n   <circle cx='138.18' cy='173.33' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='7' title='7&amp;lt;br/&amp;gt;Size=1355&amp;lt;br/&amp;gt;Price=265500'/>\\n   <circle cx='191.92' cy='163.27' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='8' title='8&amp;lt;br/&amp;gt;Size=2075&amp;lt;br/&amp;gt;Price=289900'/>\\n   <circle cx='334.85' cy='40.86' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='9' title='9&amp;lt;br/&amp;gt;Size=3990&amp;lt;br/&amp;gt;Price=587000'/>\\n   <circle cx='123.63' cy='253.88' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='10' title='10&amp;lt;br/&amp;gt;Size=1160&amp;lt;br/&amp;gt;Price=70000'/>\\n   <circle cx='128.11' cy='256.14' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='11' title='11&amp;lt;br/&amp;gt;Size=1220&amp;lt;br/&amp;gt;Price=64500'/>\\n   <circle cx='163.19' cy='213.91' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='12' title='12&amp;lt;br/&amp;gt;Size=1690&amp;lt;br/&amp;gt;Price=167000'/>\\n   <circle cx='140.05' cy='235.5' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='13' title='13&amp;lt;br/&amp;gt;Size=1380&amp;lt;br/&amp;gt;Price=114600'/>\\n   <circle cx='155.72' cy='240.28' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='14' title='14&amp;lt;br/&amp;gt;Size=1590&amp;lt;br/&amp;gt;Price=103000'/>\\n   <circle cx='115.42' cy='241.1' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='15' title='15&amp;lt;br/&amp;gt;Size=1050&amp;lt;br/&amp;gt;Price=101000'/>\\n   <circle cx='94.52' cy='253.88' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='16' title='16&amp;lt;br/&amp;gt;Size=770&amp;lt;br/&amp;gt;Price=70000'/>\\n   <circle cx='142.29' cy='247.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='17' title='17&amp;lt;br/&amp;gt;Size=1410&amp;lt;br/&amp;gt;Price=85000'/>\\n   <circle cx='116.16' cy='273.45' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='18' title='18&amp;lt;br/&amp;gt;Size=1060&amp;lt;br/&amp;gt;Price=22500'/>\\n   <circle cx='134.08' cy='245.64' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='19' title='19&amp;lt;br/&amp;gt;Size=1300&amp;lt;br/&amp;gt;Price=90000'/>\\n   <circle cx='149' cy='227.92' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='20' title='20&amp;lt;br/&amp;gt;Size=1500&amp;lt;br/&amp;gt;Price=133000'/>\\n   <circle cx='98.25' cy='245.43' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='21' title='21&amp;lt;br/&amp;gt;Size=820&amp;lt;br/&amp;gt;Price=90500'/>\\n   <circle cx='331.79' cy='44.77' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='22' title='22&amp;lt;br/&amp;gt;Size=3949&amp;lt;br/&amp;gt;Price=577500'/>\\n   <circle cx='124.37' cy='224.01' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='23' title='23&amp;lt;br/&amp;gt;Size=1170&amp;lt;br/&amp;gt;Price=142500'/>\\n   <circle cx='149' cy='216.8' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='24' title='24&amp;lt;br/&amp;gt;Size=1500&amp;lt;br/&amp;gt;Price=160000'/>\\n   <circle cx='245.29' cy='183.83' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='25' title='25&amp;lt;br/&amp;gt;Size=2790&amp;lt;br/&amp;gt;Price=240000'/>\\n   <circle cx='113.92' cy='246.87' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='26' title='26&amp;lt;br/&amp;gt;Size=1030&amp;lt;br/&amp;gt;Price=87000'/>\\n   <circle cx='130.35' cy='233.85' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='27' title='27&amp;lt;br/&amp;gt;Size=1250&amp;lt;br/&amp;gt;Price=118600'/>\\n   <circle cx='168.41' cy='225.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='28' title='28&amp;lt;br/&amp;gt;Size=1760&amp;lt;br/&amp;gt;Price=140000'/>\\n   <circle cx='152.74' cy='221.74' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='29' title='29&amp;lt;br/&amp;gt;Size=1550&amp;lt;br/&amp;gt;Price=148000'/>\\n   <circle cx='120.64' cy='254.29' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='30' title='30&amp;lt;br/&amp;gt;Size=1120&amp;lt;br/&amp;gt;Price=69000'/>\\n   <circle cx='186.32' cy='210.2' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='31' title='31&amp;lt;br/&amp;gt;Size=2000&amp;lt;br/&amp;gt;Price=176000'/>\\n   <circle cx='137.81' cy='247.08' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='32' title='32&amp;lt;br/&amp;gt;Size=1350&amp;lt;br/&amp;gt;Price=86500'/>\\n   <circle cx='174.38' cy='208.55' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='33' title='33&amp;lt;br/&amp;gt;Size=1840&amp;lt;br/&amp;gt;Price=180000'/>\\n   <circle cx='224.39' cy='208.97' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='34' title='34&amp;lt;br/&amp;gt;Size=2510&amp;lt;br/&amp;gt;Price=179000'/>\\n   <circle cx='269.17' cy='143.45' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='35' title='35&amp;lt;br/&amp;gt;Size=3110&amp;lt;br/&amp;gt;Price=338000'/>\\n   <circle cx='168.41' cy='229.16' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='36' title='36&amp;lt;br/&amp;gt;Size=1760&amp;lt;br/&amp;gt;Price=130000'/>\\n   <circle cx='164.68' cy='215.56' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='37' title='37&amp;lt;br/&amp;gt;Size=1710&amp;lt;br/&amp;gt;Price=163000'/>\\n   <circle cx='119.9' cy='231.22' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='38' title='38&amp;lt;br/&amp;gt;Size=1110&amp;lt;br/&amp;gt;Price=125000'/>\\n   <circle cx='138.56' cy='241.52' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='39' title='39&amp;lt;br/&amp;gt;Size=1360&amp;lt;br/&amp;gt;Price=100000'/>\\n   <circle cx='130.35' cy='241.52' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='40' title='40&amp;lt;br/&amp;gt;Size=1250&amp;lt;br/&amp;gt;Price=100000'/>\\n   <circle cx='130.35' cy='241.52' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='41' title='41&amp;lt;br/&amp;gt;Size=1250&amp;lt;br/&amp;gt;Price=100000'/>\\n   <circle cx='147.51' cy='222.36' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='42' title='42&amp;lt;br/&amp;gt;Size=1480&amp;lt;br/&amp;gt;Price=146500'/>\\n   <circle cx='150.5' cy='223.02' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='43' title='43&amp;lt;br/&amp;gt;Size=1520&amp;lt;br/&amp;gt;Price=144900'/>\\n   <circle cx='187.82' cy='207.32' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='44' title='44&amp;lt;br/&amp;gt;Size=2020&amp;lt;br/&amp;gt;Price=183000'/>\\n   <circle cx='112.43' cy='253.92' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='45' title='45&amp;lt;br/&amp;gt;Size=1010&amp;lt;br/&amp;gt;Price=69900'/>\\n   <circle cx='159.45' cy='258' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='46' title='46&amp;lt;br/&amp;gt;Size=1640&amp;lt;br/&amp;gt;Price=60000'/>\\n   <circle cx='107.21' cy='230.39' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='47' title='47&amp;lt;br/&amp;gt;Size=940&amp;lt;br/&amp;gt;Price=127000'/>\\n   <circle cx='154.98' cy='247.28' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='48' title='48&amp;lt;br/&amp;gt;Size=1580&amp;lt;br/&amp;gt;Price=86000'/>\\n   <circle cx='101.24' cy='262.12' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='49' title='49&amp;lt;br/&amp;gt;Size=860&amp;lt;br/&amp;gt;Price=50000'/>\\n   <circle cx='143.03' cy='226.27' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='50' title='50&amp;lt;br/&amp;gt;Size=1420&amp;lt;br/&amp;gt;Price=137000'/>\\n   <circle cx='131.84' cy='232.74' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='51' title='51&amp;lt;br/&amp;gt;Size=1270&amp;lt;br/&amp;gt;Price=121300'/>\\n   <circle cx='110.19' cy='249.35' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='52' title='52&amp;lt;br/&amp;gt;Size=980&amp;lt;br/&amp;gt;Price=81000'/>\\n   <circle cx='208.71' cy='205.26' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='53' title='53&amp;lt;br/&amp;gt;Size=2300&amp;lt;br/&amp;gt;Price=188000'/>\\n   <circle cx='143.78' cy='247.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='54' title='54&amp;lt;br/&amp;gt;Size=1430&amp;lt;br/&amp;gt;Price=85000'/>\\n   <circle cx='140.05' cy='226.27' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='55' title='55&amp;lt;br/&amp;gt;Size=1380&amp;lt;br/&amp;gt;Price=137000'/>\\n   <circle cx='129.6' cy='222.98' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='56' title='56&amp;lt;br/&amp;gt;Size=1240&amp;lt;br/&amp;gt;Price=145000'/>\\n   <circle cx='120.64' cy='254.29' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='57' title='57&amp;lt;br/&amp;gt;Size=1120&amp;lt;br/&amp;gt;Price=69000'/>\\n   <circle cx='120.64' cy='237.68' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='58' title='58&amp;lt;br/&amp;gt;Size=1120&amp;lt;br/&amp;gt;Price=109300'/>\\n   <circle cx='178.86' cy='228.54' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='59' title='59&amp;lt;br/&amp;gt;Size=1900&amp;lt;br/&amp;gt;Price=131500'/>\\n   <circle cx='218.42' cy='200.31' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='60' title='60&amp;lt;br/&amp;gt;Size=2430&amp;lt;br/&amp;gt;Price=200000'/>\\n   <circle cx='117.66' cy='248.97' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='61' title='61&amp;lt;br/&amp;gt;Size=1080&amp;lt;br/&amp;gt;Price=81900'/>\\n   <circle cx='137.81' cy='245.14' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='62' title='62&amp;lt;br/&amp;gt;Size=1350&amp;lt;br/&amp;gt;Price=91200'/>\\n   <circle cx='165.42' cy='231.42' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='63' title='63&amp;lt;br/&amp;gt;Size=1720&amp;lt;br/&amp;gt;Price=124500'/>\\n   <circle cx='339.33' cy='190.01' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='64' title='64&amp;lt;br/&amp;gt;Size=4050&amp;lt;br/&amp;gt;Price=225000'/>\\n   <circle cx='149' cy='226.48' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='65' title='65&amp;lt;br/&amp;gt;Size=1500&amp;lt;br/&amp;gt;Price=136500'/>\\n   <circle cx='229.69' cy='125.74' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='66' title='66&amp;lt;br/&amp;gt;Size=2581&amp;lt;br/&amp;gt;Price=381000'/>\\n   <circle cx='195.28' cy='179.71' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='67' title='67&amp;lt;br/&amp;gt;Size=2120&amp;lt;br/&amp;gt;Price=250000'/>\\n   <circle cx='241.93' cy='136.49' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='68' title='68&amp;lt;br/&amp;gt;Size=2745&amp;lt;br/&amp;gt;Price=354900'/>\\n   <circle cx='150.5' cy='225.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='69' title='69&amp;lt;br/&amp;gt;Size=1520&amp;lt;br/&amp;gt;Price=140000'/>\\n   <circle cx='132.58' cy='245.68' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='70' title='70&amp;lt;br/&amp;gt;Size=1280&amp;lt;br/&amp;gt;Price=89900'/>\\n   <circle cx='157.96' cy='226.27' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='71' title='71&amp;lt;br/&amp;gt;Size=1620&amp;lt;br/&amp;gt;Price=137000'/>\\n   <circle cx='150.5' cy='240.28' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='72' title='72&amp;lt;br/&amp;gt;Size=1520&amp;lt;br/&amp;gt;Price=103000'/>\\n   <circle cx='188.56' cy='207.32' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='73' title='73&amp;lt;br/&amp;gt;Size=2030&amp;lt;br/&amp;gt;Price=183000'/>\\n   <circle cx='140.79' cy='225.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='74' title='74&amp;lt;br/&amp;gt;Size=1390&amp;lt;br/&amp;gt;Price=140000'/>\\n   <circle cx='177.37' cy='216.8' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='75' title='75&amp;lt;br/&amp;gt;Size=1880&amp;lt;br/&amp;gt;Price=160000'/>\\n   <circle cx='252.83' cy='103.9' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='76' title='76&amp;lt;br/&amp;gt;Size=2891&amp;lt;br/&amp;gt;Price=434000'/>\\n   <circle cx='137.06' cy='229.16' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='77' title='77&amp;lt;br/&amp;gt;Size=1340&amp;lt;br/&amp;gt;Price=130000'/>\\n   <circle cx='107.21' cy='232.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='78' title='78&amp;lt;br/&amp;gt;Size=940&amp;lt;br/&amp;gt;Price=123000'/>\\n   <circle cx='80.34' cy='274.07' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='79' title='79&amp;lt;br/&amp;gt;Size=580&amp;lt;br/&amp;gt;Price=21000'/>\\n   <circle cx='142.29' cy='247.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='80' title='80&amp;lt;br/&amp;gt;Size=1410&amp;lt;br/&amp;gt;Price=85000'/>\\n   <circle cx='122.88' cy='253.92' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='81' title='81&amp;lt;br/&amp;gt;Size=1150&amp;lt;br/&amp;gt;Price=69900'/>\\n   <circle cx='140.05' cy='231.22' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='82' title='82&amp;lt;br/&amp;gt;Size=1380&amp;lt;br/&amp;gt;Price=125000'/>\\n   <circle cx='146.77' cy='215.72' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='83' title='83&amp;lt;br/&amp;gt;Size=1470&amp;lt;br/&amp;gt;Price=162600'/>\\n   <circle cx='155.72' cy='218.07' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='84' title='84&amp;lt;br/&amp;gt;Size=1590&amp;lt;br/&amp;gt;Price=156900'/>\\n   <circle cx='126.61' cy='239.09' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='85' title='85&amp;lt;br/&amp;gt;Size=1200&amp;lt;br/&amp;gt;Price=105900'/>\\n   <circle cx='180.35' cy='213.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='86' title='86&amp;lt;br/&amp;gt;Size=1920&amp;lt;br/&amp;gt;Price=167500'/>\\n   <circle cx='197.52' cy='220.17' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='87' title='87&amp;lt;br/&amp;gt;Size=2150&amp;lt;br/&amp;gt;Price=151800'/>\\n   <circle cx='201.25' cy='233.98' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='88' title='88&amp;lt;br/&amp;gt;Size=2200&amp;lt;br/&amp;gt;Price=118300'/>\\n   <circle cx='101.24' cy='243.87' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='89' title='89&amp;lt;br/&amp;gt;Size=860&amp;lt;br/&amp;gt;Price=94300'/>\\n   <circle cx='128.85' cy='244.03' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='90' title='90&amp;lt;br/&amp;gt;Size=1230&amp;lt;br/&amp;gt;Price=93900'/>\\n   <circle cx='122.13' cy='214.73' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='91' title='91&amp;lt;br/&amp;gt;Size=1140&amp;lt;br/&amp;gt;Price=165000'/>\\n   <circle cx='234.84' cy='165.29' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='92' title='92&amp;lt;br/&amp;gt;Size=2650&amp;lt;br/&amp;gt;Price=285000'/>\\n   <circle cx='116.16' cy='264.18' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='93' title='93&amp;lt;br/&amp;gt;Size=1060&amp;lt;br/&amp;gt;Price=45000'/>\\n   <circle cx='169.16' cy='231.26' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='94' title='94&amp;lt;br/&amp;gt;Size=1770&amp;lt;br/&amp;gt;Price=124900'/>\\n   <circle cx='175.87' cy='222.15' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='95' title='95&amp;lt;br/&amp;gt;Size=1860&amp;lt;br/&amp;gt;Price=147000'/>\\n   <circle cx='116.16' cy='210.2' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='96' title='96&amp;lt;br/&amp;gt;Size=1060&amp;lt;br/&amp;gt;Price=176000'/>\\n   <circle cx='166.17' cy='201.76' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='97' title='97&amp;lt;br/&amp;gt;Size=1730&amp;lt;br/&amp;gt;Price=196500'/>\\n   <circle cx='139.3' cy='228.25' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='98' title='98&amp;lt;br/&amp;gt;Size=1370&amp;lt;br/&amp;gt;Price=132200'/>\\n   <circle cx='153.48' cy='246.3' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='99' title='99&amp;lt;br/&amp;gt;Size=1560&amp;lt;br/&amp;gt;Price=88400'/>\\n   <circle cx='137.06' cy='230.31' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='100' title='100&amp;lt;br/&amp;gt;Size=1340&amp;lt;br/&amp;gt;Price=127200'/>\\n   <polyline points='66.90,274.67 81.83,266.06 96.76,257.45 111.69,248.85 126.61,240.24 141.54,231.63 156.47,223.03 171.40,214.42 186.32,205.82 201.25,197.21 216.18,188.60 231.11,180.00 246.03,171.39 260.96,162.78 275.89,154.18 290.82,145.57 305.74,136.97 320.67,128.36 335.60,119.75 350.53,111.15' fill='none' stroke='#132B43' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt' title='for New=0&amp;lt;br/&amp;gt;y = 104.44*x -22227.81' data-id='1'/>\\n   <polyline points='66.90,296.82 81.83,283.11 96.76,269.40 111.69,255.69 126.61,241.98 141.54,228.27 156.47,214.57 171.40,200.86 186.32,187.15 201.25,173.44 216.18,159.73 231.11,146.02 246.03,132.31 260.96,118.61 275.89,104.90 290.82,91.19 305.74,77.48 320.67,63.77 335.60,50.06 350.53,36.36' fill='none' stroke='#56B1F7' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt' title='for New=1&amp;lt;br/&amp;gt;y = 166.35*x -100755.31' data-id='21'/>\\n  <\\/g>\\n  <g clip-path='url(#svg_d8822ff7-86b5-4381-8b07-55c802e256d5_c1)'>\\n   <text x='42.89' y='285.87' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>0<\\/text>\\n   <text x='18.41' y='203.46' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>200000<\\/text>\\n   <text x='18.41' y='121.06' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>400000<\\/text>\\n   <text x='18.41' y='38.65' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>600000<\\/text>\\n   <polyline points='49.98,282.72 52.72,282.72' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='49.98,200.31 52.72,200.31' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='49.98,117.91 52.72,117.91' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='49.98,35.50 52.72,35.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='111.69,331.24 111.69,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='186.32,331.24 186.32,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='260.96,331.24 260.96,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='335.60,331.24 335.60,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <text x='101.89' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>1000<\\/text>\\n   <text x='176.53' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>2000<\\/text>\\n   <text x='251.17' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>3000<\\/text>\\n   <text x='325.81' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>4000<\\/text>\\n   <text x='198.02' y='352.2' font-size='8.25pt' font-family='Arial'>Size<\\/text>\\n   <text transform='translate(13.36,179.52) rotate(-90.00)' font-size='8.25pt' font-family='Arial'>Price<\\/text>\\n   <rect x='375.67' y='110.47' width='50.85' height='113.03' fill='#FFFFFF' stroke='none'/>\\n   <image x='381.15' y='131.63' width='17.28' height='86.4' preserveAspectRatio='none' xlink:href='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAEsCAYAAAACUNnVAAAAmElEQVQ4ja2UQRLDMAgDd3hk/tnX5agcknFaCoa0vnkMCEnGsL12GQhDYDpPwoDrpBFlRDWi/9Wu6AHC9FkLYLrzlhD3oJ3mDvRHRYy7IC8A7bCK8gpWKYDzeQVArDcDyD2YAnSofZvzaOA6Q7Oahp/dmRuVTQ0xU5TGP2o8Waool1obVuv1q8qP9xPvg633XlGLDtfIhNUBcBeA5ss0BXMAAAAASUVORK5CYII=' xmlns:xlink='http://www.w3.org/1999/xlink'/>\\n   <text x='403.91' y='221.03' font-size='6.6pt' font-family='Arial'>0.00<\\/text>\\n   <text x='403.91' y='199.51' font-size='6.6pt' font-family='Arial'>0.25<\\/text>\\n   <text x='403.91' y='177.98' font-size='6.6pt' font-family='Arial'>0.50<\\/text>\\n   <text x='403.91' y='156.45' font-size='6.6pt' font-family='Arial'>0.75<\\/text>\\n   <text x='403.91' y='134.92' font-size='6.6pt' font-family='Arial'>1.00<\\/text>\\n   <text x='381.15' y='124.99' font-size='8.25pt' font-family='Arial'>New<\\/text>\\n   <line x1='381.15' y1='217.88' x2='384.6' y2='217.88' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='196.36' x2='384.6' y2='196.36' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='174.83' x2='384.6' y2='174.83' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='153.3' x2='384.6' y2='153.3' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='131.77' x2='384.6' y2='131.77' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='217.88' x2='398.43' y2='217.88' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='196.36' x2='398.43' y2='196.36' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='174.83' x2='398.43' y2='174.83' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='153.3' x2='398.43' y2='153.3' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='131.77' x2='398.43' y2='131.77' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n  <\\/g>\\n <\\/g>\\n<\\/svg>\",\"js\":null,\"uid\":\"svg_d8822ff7-86b5-4381-8b07-55c802e256d5\",\"ratio\":1.2,\"settings\":{\"tooltip\":{\"css\":\".tooltip_SVGID_ { background-color:white;font-style:italic;padding:10px;border-radius:10px 20px 10px 20px; ; position:absolute;pointer-events:none;z-index:999;}\",\"placement\":\"doc\",\"offx\":10,\"offy\":0,\"use_cursor_pos\":true,\"opacity\":0.75,\"usefill\":false,\"usestroke\":false,\"delay\":{\"over\":200,\"out\":500}},\"hover\":{\"css\":\".hover_SVGID_ { r:4px;cursor:pointer;stroke:red;stroke-width:2px; }\",\"reactive\":false},\"hoverkey\":{\"css\":\".hover_key_SVGID_ { stroke:red; }\",\"reactive\":false},\"hovertheme\":{\"css\":\".hover_theme_SVGID_ { fill:green; }\",\"reactive\":false},\"hoverinv\":{\"css\":\"\"},\"zoom\":{\"min\":1,\"max\":10},\"capture\":{\"css\":\".selected_SVGID_ { fill:#FF3333;stroke:black; }\",\"type\":\"multiple\",\"only_shiny\":true,\"selected\":[]},\"capturekey\":{\"css\":\".selected_key_SVGID_ { stroke:gray; }\",\"type\":\"single\",\"only_shiny\":true,\"selected\":[]},\"capturetheme\":{\"css\":\".selected_theme_SVGID_ { stroke:gray; }\",\"type\":\"single\",\"only_shiny\":true,\"selected\":[]},\"toolbar\":{\"position\":\"topright\",\"saveaspng\":true,\"pngname\":\"diagram\"},\"sizing\":{\"rescale\":true,\"width\":1}}},\"evals\":[],\"jsHooks\":[]}\r\nPart F.\r\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*3000)+(-78527.50*0)+(61.92*3000*0) \r\n\r\n\r\n[1] 291092.2\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*3000)+(-78527.50*1)+(61.92*3000*1) \r\n\r\n\r\n[1] 398324.7\r\n\r\nUnder the new model:\r\nFor a 3000 square foot home that is new, the price can be estimated to be $398,324.70.\r\nFor a 3000 square foot home that is not new, the price can be estimated to be $291,092.20.\r\nPart G.\r\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\r\nFor a 1500 square foot home that is new, the price can be estimated to be $148,784.70.\r\nFor a 1500 square foot home that is not new, the price can be estimated to be $134,432.20.\r\n\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*1500)+(-78527.50*0)+(61.92*1500*0) \r\n\r\n\r\n[1] 134432.2\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*1500)+(-78527.50*1)+(61.92*1500*1) \r\n\r\n\r\n[1] 148784.7\r\n\r\nIllustrating the difference and analyzing the output\r\n\r\n\r\nShow code\r\n\r\n#3,000 Square Foot House\r\n\r\nnew1 <- 398324.70\r\nold1 <- 291092.20\r\nold1/new1*100\r\n\r\n\r\n[1] 73.07912\r\n\r\nShow code\r\n\r\n#1,500 Square Foot House\r\n\r\nnew2 <- 148784.70\r\nold2 <- 134432.2\r\nold2/new2*100\r\n\r\n\r\n[1] 90.35351\r\n\r\nThe difference in value between the 3,000 square foot house and the 1,500 square foot house under this model represents a value ration of price of old to new of 73.08% vs. 90.35%. This tells me that in this model, the price of an “old” house is a smaller percentage of the price of a “new” house as the square footage increases.\r\nPart H.\r\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\r\nThe second model has a higher adjusted R squared (0.7363 v. 0.7169) than the first model, which was still an independently valid model. This gives me a hint at the statistical reliability of the model. I also think it takes into consideration the context a stronger threshold where square footage begins to diminish as a variable.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-03-dacss-603-assignment-3/dacss-603-assignment-3_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2022-04-03T17:28:54-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-dacss-697e-post-4/",
    "title": "DACSS 697D Post 4",
    "description": "Assignment 4 for DACSS 697D Course 'Text as Data': \"Preprocessing\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-03-31",
    "categories": [
      "text as data",
      "homework"
    ],
    "contents": "\r\nGetting Started\r\nI am beginning to pull in the text collection that I will be analyzing for my final project in the course. I started pulling articles by month beginning January 2020 through December 2021. For my initial text collection, I am collecting articles using the New York Times API for the search query “Afghanistan”. I am not limiting my search by any filter at this time. However, I am limited in that the article search API for the New York Times does not pull the entire article; rather, I have been able to pull the abstract/summary, lead paragraph, and snippet for each article as well as the keywords, authors, sections, and url. In addition, I can get the article titles for both the print and online versions of the article.\r\nTo pull the data, I had to reduce the queries into more workable groups that would not time out, given the NYT API limits. I was able to pull the 3,442 articles by year (2020, then 2021 in two parts), then assemble them into a dataframe. I will not run the code in this post, as it was already run and is an exhaustive process.\r\n\r\n\r\n# For articles from 2020\r\n\r\n#url2020 <- ('https://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=20200101&end_date=20201231&q=afghanistan&api-key=GTp3efxVZiGO75Iox9uZJ8ZTjIMjDWsM')\r\n\r\n#query2020 <- fromJSON(url2020)\r\n\r\n#max.pages2020 <- ceiling((query2020$response$meta$hits[1] / 10)-1) \r\n\r\n#pages2020 <- list()\r\n#for(i in 0:max.pages2020){\r\n  #search2020 <- fromJSON(paste0(url2020, \"&page=\", i), flatten = TRUE) %>% data.frame() \r\n  #message(\"Retrieving page \", i)\r\n  #pages2020[[i+1]] <- search2020\r\n  #Sys.sleep(10)\r\n  #}\r\n\r\n#pages2020[[i+1]] <- search2020 \r\n#afghanistan.articles.2020 <- rbind_pages(pages2020)\r\n\r\n#save(afghanistan.articles.2020,file=\"afghanistan_articles_2020.Rdata\")\r\n\r\n# For January to August 2021\r\n\r\n#url2021a <- ('https://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=20210101&end_date=20210831&q=afghanistan&api-key=GTp3efxVZiGO75Iox9uZJ8ZTjIMjDWsM')\r\n\r\n#query2021a <- fromJSON(url2021a)\r\n\r\n#max.pages2021a <- ceiling((query2021a$response$meta$hits[1] / 10)-1) \r\n\r\n#pages2021a <- list()\r\n#for(i in 0:max.pages2021a){\r\n  #search2021a <- fromJSON(paste0(url2021a, \"&page=\", i), flatten = TRUE) %>% data.frame() \r\n  #message(\"Retrieving page \", i)\r\n  #pages2021a[[i+1]] <- search2021a\r\n  #Sys.sleep(10) \r\n#}\r\n\r\n#pages2021a[[i+1]] <- search2021a \r\n#afghanistan.articles.2021a <- rbind_pages(pages2021a)\r\n\r\n#save(afghanistan.articles.2021a,file=\"afghanistan_articles_2021a.Rdata\")\r\n\r\n# For September-December 2021\r\n\r\nurl2021b <- ('https://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=20210901&end_date=20211231&q=afghanistan&api-key=GTp3efxVZiGO75Iox9uZJ8ZTjIMjDWsM')\r\n\r\n#query2021b <- fromJSON(url2021b)\r\n\r\n#max.pages2021b <- ceiling((query2021b$response$meta$hits[1] / 10)-1) \r\n\r\n#pages2021b <- list()\r\n#for(i in 0:max.pages2021b){\r\n  #search2021b <- fromJSON(paste0(url2021b, \"&page=\", i), flatten = TRUE) %>% data.frame() \r\n  #message(\"Retrieving page \", i)\r\n  #pages2021b[[i+1]] <- search2021b\r\n  #Sys.sleep(10) \r\n#}\r\n\r\n#pages2021b[[i+1]] <- search2021b \r\n#afghanistan.articles.2021b <- rbind_pages(pages2021b)\r\n\r\n#save(afghanistan.articles.2021b,file=\"afghanistan_articles_2021b.Rdata\")\r\n\r\n\r\n# Create shell for data\r\n\r\n#afghanistan.articles.all <- c()\r\n\r\n#afghanistan.articles.all <- rbind_pages(c(pages2020, pages2021a, pages2021b))\r\n\r\n\r\n\r\nAfter compiling the data, I re-formatted the date column and saving the formatted tibble for offline access.\r\n\r\n\r\n#afghanistan.articles.table<- as_tibble(cbind(\r\n  #date=afghanistan.articles.all$response.docs.pub_date,\r\n  #abstract=afghanistan.articles.all$response.docs.abstract,\r\n  #lead.paragraph=afghanistan.articles.all$response.docs.lead_paragraph,\r\n  #snippet=afghanistan.articles.all$response.docs.snippet,\r\n  #section.name=afghanistan.articles.all$response.docs.section_name,\r\n  #subsection.name=afghanistan.articles.all$response.docs.subsection_name,\r\n  #news.desk=afghanistan.articles.all$response.docs.news_desk,\r\n  #byline=afghanistan.articles.all$response.docs.byline.original,\r\n  #headline.main=afghanistan.articles.all$response.docs.headline.main,\r\n  #headline.print=afghanistan.articles.all$response.docs.headline.print_headline,\r\n  #headline.kicker=afghanistan.articles.all$response.docs.headline.kicker,\r\n  #material=afghanistan.articles.all$response.docs.type_of_material,\r\n  #url=afghanistan.articles.all$response.docs.web_url\r\n  #))\r\n\r\n#afghanistan.articles.table$date <- substr(afghanistan.articles.table$date, 1, nchar(afghanistan.articles.table$date)-14)\r\n\r\n#afghanistan.articles.table$date <- as.Date(afghanistan.articles.table$date, \"%Y-%m-%d\")\r\n\r\n#save(afghanistan.articles.table,file=\"afghanistan.articles.table.Rdata\")\r\n\r\n#write.table(afghanistan.articles.table, file = \"~/GitHub/DACSS.697D/Text as Data Spring22/afghanistan.articles.table.csv\", sep=\",\", row.names=FALSE)\r\n\r\n\r\n\r\nInitial data collection complete!\r\nNow to the active review of the data. Loading the data from my collection phase:\r\n\r\n\r\nload(\"afghanistan.articles.table.RData\")\r\n\r\nafghanistan_lead <- read.csv(\"lead.paragraph.table.csv\")\r\n\r\nafghanistan_articles <- as.data.frame(afghanistan_lead)\r\n\r\n\r\n\r\nCreating a corpus of the lead paragraphs of each article\r\n\r\n\r\nafghanistan_corpus <- corpus(afghanistan_articles)\r\nafghanistan_summary <- summary(afghanistan_corpus)\r\nafghanistan_summary\r\n\r\n\r\nCorpus consisting of 3442 documents, showing 100 documents:\r\n\r\n    Text Types Tokens Sentences doc.id      date         section.name\r\n   text1    32     40         1      1  1/1/2020        Today’s Paper\r\n   text2    23     26         1      2  1/2/2020                 Arts\r\n   text3   109    170         9      3  1/2/2020             Magazine\r\n   text4    49     56         1      4  1/3/2020         Business Day\r\n   text5    23     25         3      5  1/3/2020             Magazine\r\n   text6    46     60         3      6  1/3/2020           Obituaries\r\n   text7    43     55         3      7  1/3/2020              Opinion\r\n   text8    48     64         2      8  1/3/2020              Opinion\r\n   text9    41     51         1      9  1/3/2020                 U.S.\r\n  text10    31     35         1     10  1/3/2020                 U.S.\r\n  text11    38     45         2     11  1/3/2020                World\r\n  text12    11     11         1     12  1/4/2020              Opinion\r\n  text13    36     45         4     13  1/4/2020              Opinion\r\n  text14    57     68         3     14  1/4/2020                 U.S.\r\n  text15    27     29         1     15  1/4/2020                 U.S.\r\n  text16    54     61         2     16  1/4/2020                 U.S.\r\n  text17    33     37         1     17  1/4/2020                World\r\n  text18    47     51         1     18  1/4/2020                World\r\n  text19    21     24         1     19  1/4/2020                World\r\n  text20    32     42         3     20  1/5/2020              Opinion\r\n  text21    41     56         2     21  1/5/2020                 U.S.\r\n  text22    26     28         1     22  1/5/2020                World\r\n  text23    14     14         2     23  1/6/2020             Briefing\r\n  text24    14     14         2     24  1/6/2020             Briefing\r\n  text25    10     11         1     25  1/6/2020             Magazine\r\n  text26    62     76         3     26  1/6/2020              Opinion\r\n  text27     4      4         1     27  1/6/2020              Opinion\r\n  text28    24     24         1     28  1/6/2020                 U.S.\r\n  text29    35     45         2     29  1/6/2020                 U.S.\r\n  text30    33     34         1     30  1/6/2020                World\r\n  text31    56     67         2     31  1/6/2020                World\r\n  text32    42     55         1     32  1/7/2020                 Arts\r\n  text33    42     54         1     33  1/7/2020         Business Day\r\n  text34    46     61         2     34  1/7/2020             New York\r\n  text35    69     91         4     35  1/7/2020              Opinion\r\n  text36    63     78         2     36  1/7/2020              Opinion\r\n  text37    59     79         4     37  1/7/2020                 U.S.\r\n  text38    43     47         1     38  1/7/2020                 U.S.\r\n  text39    40     44         1     39  1/7/2020                World\r\n  text40    18     18         1     40  1/8/2020              Opinion\r\n  text41    30     35         2     41  1/8/2020              Opinion\r\n  text42    41     43         1     42  1/8/2020              Opinion\r\n  text43    45     50         1     43  1/8/2020                 U.S.\r\n  text44    15     17         1     44  1/8/2020                 U.S.\r\n  text45    52     61         3     45  1/8/2020                 U.S.\r\n  text46    41     49         3     46  1/8/2020                 U.S.\r\n  text47    34     38         1     47  1/8/2020                World\r\n  text48    35     40         1     48  1/8/2020                World\r\n  text49    32     36         1     49  1/8/2020                World\r\n  text50    41     47         3     50  1/8/2020                World\r\n  text51    29     35         3     51  1/8/2020                World\r\n  text52    13     13         1     52  1/8/2020                World\r\n  text53    42     49         2     53  1/8/2020                World\r\n  text54   111    171        10     54  1/9/2020             Magazine\r\n  text55    54     76         4     55  1/9/2020              Opinion\r\n  text56     4      4         1     56  1/9/2020              Opinion\r\n  text57    34     37         2     57  1/9/2020                Style\r\n  text58    39     46         2     58  1/9/2020                 U.S.\r\n  text59    42     63         3     59  1/9/2020                World\r\n  text60    39     41         1     60  1/9/2020                World\r\n  text61    79    105         5     61 1/10/2020              Opinion\r\n  text62    42     49         1     62 1/10/2020                 U.S.\r\n  text63    25     28         1     63 1/10/2020                 U.S.\r\n  text64    54     72         3     64 1/10/2020                 U.S.\r\n  text65    26     28         1     65 1/10/2020                 U.S.\r\n  text66    35     42         1     66 1/10/2020                World\r\n  text67    34     42         1     67 1/10/2020                World\r\n  text68    35     50         2     68 1/11/2020                World\r\n  text69    38     40         1     69 1/11/2020                World\r\n  text70    33     40         3     70 1/12/2020                World\r\n  text71    44     51         1     71 1/12/2020                World\r\n  text72    32     36         2     72 1/13/2020                Books\r\n  text73    14     14         2     73 1/13/2020             Briefing\r\n  text74    14     14         2     74 1/13/2020             Briefing\r\n  text75    10     11         1     75 1/13/2020              Opinion\r\n  text76    10     11         1     76 1/13/2020              Opinion\r\n  text77    56     70         2     77 1/13/2020                 U.S.\r\n  text78    39     47         1     78 1/13/2020                World\r\n  text79    10     10         1     79 1/14/2020              Opinion\r\n  text80     4      4         1     80 1/14/2020              Opinion\r\n  text81    41     51         2     81 1/14/2020                 U.S.\r\n  text82    41     51         2     82 1/14/2020                 U.S.\r\n  text83    39     41         1     83 1/14/2020                World\r\n  text84    45     62         3     84 1/15/2020                 Arts\r\n  text85   110    157         7     85 1/15/2020                 Arts\r\n  text86    65     93         3     86 1/15/2020                 Food\r\n  text87    66     92         2     87 1/15/2020             Magazine\r\n  text88    52     70         4     88 1/15/2020              Opinion\r\n  text89    52     70         4     89 1/15/2020              Opinion\r\n  text90    10     11         1     90 1/15/2020              Opinion\r\n  text91    11     11         1     91 1/15/2020              Opinion\r\n  text92    18     20         2     92 1/15/2020 The Learning Network\r\n  text93    35     38         1     93 1/15/2020                 U.S.\r\n  text94    34     41         1     94 1/15/2020                 U.S.\r\n  text95    33     39         1     95 1/15/2020                 U.S.\r\n  text96    37     37         1     96 1/15/2020                 U.S.\r\n  text97    43     50         2     97 1/15/2020                 U.S.\r\n  text98    42     48         2     98 1/15/2020                 U.S.\r\n  text99    10     11         1     99 1/16/2020              Opinion\r\n text100    11     12         1    100 1/16/2020              Opinion\r\n  news.desk\r\n    Summary\r\n    Weekend\r\n   Magazine\r\n   Business\r\n   Magazine\r\n    Foreign\r\n       OpEd\r\n       OpEd\r\n   Politics\r\n Washington\r\n    Foreign\r\n       OpEd\r\n       OpEd\r\n Washington\r\n    Express\r\n Washington\r\n Washington\r\n    Foreign\r\n    Foreign\r\n       OpEd\r\n   Politics\r\n    Foreign\r\n     NYTNow\r\n     NYTNow\r\n   Magazine\r\n       OpEd\r\n    Letters\r\n Washington\r\n Washington\r\n Washington\r\n Washington\r\n    Culture\r\n   Business\r\n      Metro\r\n       OpEd\r\n       OpEd\r\n   National\r\n Washington\r\n Washington\r\n       OpEd\r\n  Editorial\r\n       OpEd\r\n Washington\r\n Washington\r\n Washington\r\n Washington\r\n    Foreign\r\n Washington\r\n    Foreign\r\n    Express\r\n    Foreign\r\n    Foreign\r\n    Foreign\r\n   Magazine\r\n       OpEd\r\n    Letters\r\n     Styles\r\n Washington\r\n    Foreign\r\n Washington\r\n       OpEd\r\n   National\r\n   Politics\r\n   National\r\n   Politics\r\n    Foreign\r\n    Foreign\r\n    Foreign\r\n    Express\r\n    Foreign\r\n    Foreign\r\n BookReview\r\n     NYTNow\r\n     NYTNow\r\n       OpEd\r\n       OpEd\r\n   Politics\r\n    Foreign\r\n  Editorial\r\n    Letters\r\n   Politics\r\n   Politics\r\n    Foreign\r\n    Culture\r\n    Culture\r\n     Dining\r\n   Magazine\r\n       OpEd\r\n       OpEd\r\n  Editorial\r\n  Editorial\r\n   Learning\r\n Washington\r\n Washington\r\n Washington\r\n   Politics\r\n   National\r\n   Politics\r\n  Editorial\r\n  Editorial\r\n                                                                           byline\r\n                                                                             <NA>\r\n                                                                     By Mike Hale\r\n                                                   By Fahim Abed and Fatima Faizi\r\n                                            By Ben Dooley and David Yaffe-Bellany\r\n                                                                 By C. J. Chivers\r\n                                     By Tim Arango, Ronen Bergman and Ben Hubbard\r\n                                                                  By Timothy Egan\r\n                                                                By Barbara Slavin\r\n                                                               By Alexander Burns\r\n                                                               By Catie Edmondson\r\n                                                               By Steven Erlanger\r\n                                                                 By Susan E. Rice\r\n                                                            By Thomas L. Friedman\r\n                                                                 By Zach Montague\r\n                                                                By Mariel Padilla\r\n                                                               By Catie Edmondson\r\n                                                                 By Mark Mazzetti\r\n                                                By Hari Kumar and Maria Abi-Habib\r\n                                                          By David D. Kirkpatrick\r\n                                                               By Ryan C. Crocker\r\n                                                               By Reid J. Epstein\r\n                                                                             <NA>\r\n                                                                 By Melina Delkic\r\n                                                                By Chris Stanford\r\n                                                                By David Marchese\r\n                                                                By Azadeh Moaveni\r\n                                                                             <NA>\r\n                                                                    By Lara Jakes\r\n                                                           By Jennifer Steinhauer\r\n                                                           By Thomas Gibbons-Neff\r\n                                                                    By Lara Jakes\r\n                                                                  By Jason Farago\r\n                                                           By Michael M. Grynbaum\r\n                                                      By Kimiko de Freytas-Tamura\r\n                                         By Elizabeth Cobbs and Kimberly C. Field\r\n                                                                  By Ross Douthat\r\n                                                                 By Dave Philipps\r\n                                               By Peter Baker and Maggie Haberman\r\n                  By Alissa J. Rubin, Farnaz Fassihi, Eric Schmitt and Vivian Yee\r\n                                                              By Nicholas Kristof\r\n                                                           By The Editorial Board\r\n                                                            By Thomas L. Friedman\r\n By Julian E. Barnes, Catie Edmondson, Thomas Gibbons-Neff and Rukmini Callimachi\r\n                                                            By The New York Times\r\n                                               By Annie Karni and Maggie Haberman\r\n                                                                     By Linda Qiu\r\n                                                                  By Mujib Mashal\r\n                                                By Helene Cooper and Eric Schmitt\r\n                                                               By Alissa J. Rubin\r\n                                                                  By Jacey Fortin\r\n                                                   By Dan Bilefsky and Ian Austen\r\n                                                                  By Mark Landler\r\n                                                            By The New York Times\r\n                                                               By Robert F. Worth\r\n                                                                 By Farhad Manjoo\r\n                                                                             <NA>\r\n                                              By Caity Weaver and Elizabeth Paton\r\n                                                           By Jennifer Steinhauer\r\n                                                                   By Sarah Lyall\r\n                                                               By David E. Sanger\r\n                                                                    By Tom Cotton\r\n                                                                 By Dave Philipps\r\n                                                           By Giovanni Russonello\r\n                                                  By Dave Philipps and Tim Arango\r\n                                                              By Jeremy W. Peters\r\n                                               By Salman Masood and Zia ur-Rehman\r\n                                                                             <NA>\r\n                                                                    By Fahim Abed\r\n                                                                  By Karen Zraick\r\n                                            By Thomas Gibbons-Neff and Fahim Abed\r\n                                                                  By Ruth Maclean\r\n                                                                             <NA>\r\n                                           By Victoria Shannon and Hiroko Masuike\r\n                                                                 By Melina Delkic\r\n                                                           By The Editorial Board\r\n                                                           By The Editorial Board\r\n                                                              By Jeremy W. Peters\r\n                                                By Farnaz Fassihi and Ben Hubbard\r\n                                                           By The Editorial Board\r\n                                                                             <NA>\r\n                                              By Reid J. Epstein and Katie Glueck\r\n                                              By Reid J. Epstein and Katie Glueck\r\n                                                                 By Helene Cooper\r\n                                                                  By Farah Nayeri\r\n                                                                     By Gabe Cohn\r\n                                                                  By Besha Rodell\r\n                                                                    By John Ismay\r\n                                                            By Thomas L. Friedman\r\n                                                            By Thomas L. Friedman\r\n                                                           By The Editorial Board\r\n                                                           By The Editorial Board\r\n                                                                  By Jeremy Engle\r\n                                                                  By Katie Rogers\r\n                                                               By Catie Edmondson\r\n                                                            By The New York Times\r\n                                                               By David E. Sanger\r\n                                               By Tim Arango and Neil MacFarquhar\r\n                                                                  By Maggie Astor\r\n                                                           By The Editorial Board\r\n                                                           By The Editorial Board\r\n                                                                                  headline.main\r\n                              Quotation of the Day: Ex-SEAL Now Pitching Products and President\r\n                                                  The 50 TV Shows You Need to Watch This Winter\r\n                                                       Afghan War Casualty Report: January 2020\r\n                              Carlos Ghosn Was Aided in Flight From Japan by Former Green Beret\r\n                                                                  A History of War in Six Drugs\r\n           Qassim Suleimani, Master of Iran’s Intrigue, Built a Shiite Axis of Power in Mideast\r\n                                                                    The Case for a One-Term Joe\r\n                                                  Qassim Suleimani’s Killing Will Unleash Chaos\r\n                                   Airstrike Pushes National Security to Forefront of 2020 Race\r\n                       Congressional Leaders Call for Details After U.S. Airstrike on Suleimani\r\n                                        Suleimani’s Gone, and the Iran Nuclear Deal May Be Next\r\n                                            The Dire Consequences of Trump’s Suleimani Decision\r\n                                                      Trump Kills Iran’s Most Overrated Warrior\r\n                             Pence Links Suleimani to 9/11. The Public Record Doesn’t Back Him.\r\n                                    Antiwar Protesters Across U.S. Condemn Killing of Suleimani\r\n                       With Split Decision on Impeachment, Maine Democrat Risks a Dual Backlash\r\n           In Era of Perpetual Conflict, a Volatile President Grabs Expanded Powers to Make War\r\n                                  Muslims Organize Huge Protests Across India, Challenging Modi\r\n                                                Conflict With Iran Threatens Fight Against ISIS\r\n                                                                      The Long Battle With Iran\r\n                      Buttigieg’s Bet: After Iran Strike, Military Experience Matters Even More\r\n                       Outrage in Iran After Killing of Suleimani: Here’s What You Need to Know\r\n                                  Iran, Australia Fires, Taiwan election: Your Tuesday Briefing\r\n                                    Iran, Golden Globes, Harvey Weinstein: Your Monday Briefing\r\n                                                Will Hurd Wants to Improve the Republican Brand\r\n                                                               The Day After War Begins in Iran\r\n                                                      A Threat to Iran’s Rich Cultural Heritage\r\n                     John Bass, U.S. Envoy to Afghanistan, Steps Down on Cusp of New Peace Deal\r\n                    Fox Host’s ‘America First’ Shift Makes an Exception for Trump’s Iran Strike\r\n                                 How U.S. Troops Are Preparing for the Worst in the Middle East\r\n                Defenders of History Take Aim at Trump’s Threat to Strike Iran’s Cultural Sites\r\n                                Targeting Cultural Sites in War Is Illegal. It’s Also Barbaric.\r\n                         Tucker Carlson Dissents as Right-Wing Media Weighs Trump’s Iran Strike\r\n                                Just 700 Speak This Language (50 in the Same Brooklyn Building)\r\n                                                               Why Did the U.S. Kill Suleimani?\r\n                                                             Andrew Jackson in the Persian Gulf\r\n                          He Fled Iran as a Child. Now He’s Commanding a U.S. Aircraft Carrier.\r\n                        Pentagon Rules Out Striking Iranian Cultural Sites, Contradicting Trump\r\n                      Iran Fires on U.S. Forces at 2 Bases in Iraq, Calling It ‘Fierce Revenge’\r\n                                                            Trump Has a Bizarre Idea of Winning\r\n                                     Can Trump Make Foreign Policy a Democratic Campaign Issue?\r\n                                       Did Trump and Iran Just Bury the Hatchet, or the Future?\r\n                        Pressed for Details on Suleimani Strike, Trump Administration Gives Few\r\n                                             Full Transcript: President Trump’s Address on Iran\r\n                                 Suleimani’s Killing Creates New Uncertainty for Trump Campaign\r\n                                     Trump’s Inaccurate Statements About the Conflict With Iran\r\n                          As Iran and U.S. Trade Blows, Afghanistan Sweats Between the 2 Powers\r\n                                    At the Edge of a War, the U.S. and Iran Appear to Step Back\r\n                                Missile Strike Damage Appears Limited, but Iran May Not Be Done\r\n                               The Ukraine Plane Crash in Iran: Black Boxes and Other Questions\r\n                                    Iran Plane Crash Victims Came From at Least Seven Countries\r\n              Prince Harry and Meghan to ‘Step Back’ From Royal Duties in Extraordinary Retreat\r\n                                             Confrontation in Congress Looms Over Iran Conflict\r\n                                   Mohammed bin Zayed’s Dark Vision of the Middle East’s Future\r\n                                                              We Really Must Stop Starting Wars\r\n                                                  In the Mideast, U.S. Foreign Policy Gone Awry\r\n                                               Prince Harry and Meghan Stepping Back, Explained\r\n                                  Weary Veterans Exemplify a Nation Reluctant for War With Iran\r\n                                  For Prince Harry and His Wife, Meghan, a Tricky Balancing Act\r\n                              Trump’s Iran Strategy: A Cease-Fire Wrapped in a Strategic Muddle\r\n                                                          The Case for Killing Qassim Suleimani\r\n                          Army Denies Request by Soldier Pardoned by Trump, Setting Up Showdown\r\n                                               How the Public Feels About Trump’s Iran Strategy\r\n                         Who Signs Up to Fight? Makeup of U.S. Recruits Shows Glaring Disparity\r\n                                   Why Pete Buttigieg Has Made Religion Central to His Campaign\r\n                Bomb Blast Claimed by ISIS Kills at Least 15 in Pakistan Mosque Used by Taliban\r\n                  In Plane Crash, a Day of Blame: Ukraine Weighs In and Iran Mulls Announcement\r\n                                                 Two U.S. Service Members Killed in Afghanistan\r\n                                                Jet Crash in Iran Has Eerie Historical Parallel\r\n                                     A Growing U.S. Base Made This Afghan Town. Now It’s Dying.\r\n                                      France Summons African Leaders, Threatening Troop Pullout\r\n ‘Tightrope: Americans Reaching for Hope,’ by Nicholas D. Kristof and Sheryl WuDunn: An Excerpt\r\n                                   Oscars, Iran, College Football: Your Monday Evening Briefing\r\n                        Iran Protests, Harry and Meghan, Australia Fires: Your Tuesday Briefing\r\n                                                 Tom Steyer’s Top Priority Isn’t Climate Change\r\n                                                       Bernie Sanders Wants to Change Your Mind\r\n                                            Trump’s Iran Strategy May Cost Him in 2020 Election\r\n                         Iran Protests Rage Over Downed Jet, as Lawmakers Demand Accountability\r\n                                                                    Cory Booker Has More to Say\r\n                                               An Aide’s View: Jimmy Carter as ‘the Anti-Trump’\r\n                                            6 Takeaways From the January 2020 Democratic Debate\r\n                                            6 Takeaways From the January 2020 Democratic Debate\r\n                             Plan to Cut U.S. Troops in West Africa Draws Criticism From Europe\r\n                                             In Afghanistan, Being an Artist Is a Dangerous Job\r\n                                   What’s on TV Wednesday: An Adventure Series and ‘68 Whiskey’\r\n                                                 Looking Toward a New Era of Australian Cuisine\r\n                              A Myth That Won’t Die About a Gulf War Weapon, and Why It Matters\r\n                                                                       Trump’s Code of Dishonor\r\n                                                                       Trump’s Code of Dishonor\r\n                                                         Amy Klobuchar on Plans vs. Pipe Dreams\r\n                                                                       Andrew Yang Is Listening\r\n                                           Would You Consider Serving in the U.S. Armed Forces?\r\n                         Pence Observes Transfer of Remains of 2 Soldiers Killed in Afghanistan\r\n                                  Jason Crow: Impeachment Manager Who Pressed to Launch Inquiry\r\n                                                    Fact-Checking the January Democratic Debate\r\n                             The Democrats All Disagree With Trump on Iran. Then It Gets Mushy.\r\n                           Grief and Fear in Sacramento Over a Death That Set the World on Edge\r\n                                                           Who Won the Debate? Experts Weigh In\r\n                                                    Pete Buttigieg Says He’s More Than a Résumé\r\n                                            Deval Patrick Says the Race Is Just Getting Started\r\n                                                              headline.print\r\n                                                            Quote of the Day\r\n                                           50 TV Shows To Watch  This Winter\r\n                                                                        <NA>\r\n          A Green Beret and a Turkish Jet Company Aided in a Stunning Escape\r\n                                                                        <NA>\r\n                                 A Mastermind Of Iran’s Clout In the Mideast\r\n                                     The Case for a One-Term President Biden\r\n                                                                        <NA>\r\n               Airstrike Pushes Foreign Policy To Front of Presidential Race\r\n   Majority Leader Says a Briefing for the Senate Is Scheduled for Next Week\r\n                     U.S. Strike May Have Dealt a Fatal Blow to Nuclear Deal\r\n                                            The Fallout of Killing Suleimani\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n          With Split Vote on Impeachment, Maine Democrat Risks Dual Backlash\r\n                            More Powers, Few Limits And a Volatile President\r\n                      Huge Protests in India Keep Steadfast Pressure on Modi\r\n         As Tehran and Washington Stare Each Other Down, ISIS Stands to Gain\r\n                                                                        <NA>\r\n       Buttigieg’s Bet: After Iran Strike, a Military Past Matters Even More\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                             Will Hurd Wants to Improve the Republican Brand\r\n                                                                        <NA>\r\n                                   A Threat to Iran’s Rich Cultural Heritage\r\n                                  U.S. Envoy to Afghanistan Is Stepping Down\r\n    Fox Host Is ‘America First,’ but Makes Exception for Trump’s Iran Strike\r\n                                  U.S. Troops Preparing for Worst in Mideast\r\n                           ‘You Can’t Just Start Shooting Anything You Want’\r\n                      In Targeting Cultural Sites, a Threat to Global Values\r\n          The Prospect of a New Military Conflict Divides Right-Wing Pundits\r\n                                       Carefully Nourishing a Dying Language\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                          He Once Fled Iran. Now He  Leads an American Crew.\r\n                                   Pentagon Rules Out Strikes on Antiquities\r\n                       Iranians Attack U.S. Forces in Iraq in Act of Revenge\r\n                                        Trump Has  A Bizarre Idea Of Winning\r\n                                      Foreign Policy Should Matter to Voters\r\n                                                                        <NA>\r\n                       Democrats Press for Details on Threats Cited by Trump\r\n                      U.S. ‘Is Ready to Embrace  Peace With All Who Seek It’\r\n   Iranian’s Killing Delivers Jolt of Uncertainty to Trump Campaign Strategy\r\n                                References In Address To 2013 Deal Had Holes\r\n Washington-Tehran Friction Casts Spotlight on Afghanistan’s Vulnerabilities\r\n                While Threat of War  May Have Receded, It Hasn’t Disappeared\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                         Victims Included Citizens From at Least 7 Countries\r\n              In Stunning Step, Duke and Duchess Seek New Title: Part-Timers\r\n                                                                        <NA>\r\n                                                           The M.B.Z. Moment\r\n                                                                        <NA>\r\n                               In the Mideast, U.S. Foreign Policy Gone Awry\r\n                                           A Guide To Grasping A ‘Step Back’\r\n           Weary of War, and Wary of Embarking on Another in the Middle East\r\n                                                                        <NA>\r\n                    A President’s Mixed Messages Unsettle More Than Reassure\r\n                                                                        <NA>\r\n                       Pardoned Soldier Is Denied Bid to Rejoin Green Berets\r\n                                                                        <NA>\r\n                                The Call to Serve Is Being Unevenly Embraced\r\n                              Why Buttigieg Places Religion In the Spotlight\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                          2 U.S. Service Members Killed in Afghan Bomb Blast\r\n                                                                        <NA>\r\n  As U.S. Reduces Its Afghan Presence, a Once-Vital Town Is Left With Scraps\r\n            Citing Hostility to France in Africa, Macron Threatens a Pullout\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                                      Trump 2020? Moves on Iran Raise Doubts\r\n                  In Iran, Growing Indignation and a Push for Accountability\r\n                                                                        <NA>\r\n                            An Aide’s View: Jimmy Carter as the ‘Anti-Trump’\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                        Plan to Cut U.S. Troops In West Africa Is Criticized\r\n                            Afghan Artists Risk Everything to Reject Silence\r\n                                                         What’s On Wednesday\r\n                                           Australian Cuisine For a New Time\r\n                                                                        <NA>\r\n                                                    Trump’s Code Of Dishonor\r\n                                                    Trump’s Code Of Dishonor\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                                                                  Jason Crow\r\n                                                                  Fact Check\r\n           Candidates Critical of Trump on Iran, but Vague on What They’d Do\r\n                             His Death Rattled World And Shook His U.S. City\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n                                                                        <NA>\r\n            material X X.1 X.2\r\n               Quote          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n     Obituary (Obit)          \r\n               Op-Ed          \r\n               Op-Ed          \r\n                News          \r\n                News          \r\n                News          \r\n               Op-Ed          \r\n               Op-Ed          \r\n                News          \r\n                News          \r\n                News          \r\n       News Analysis          \r\n                News          \r\n                News          \r\n               Op-Ed          \r\n                News          \r\n                News          \r\n            briefing          \r\n            briefing          \r\n Interactive Feature          \r\n               Op-Ed          \r\n              Letter          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n               Op-Ed          \r\n               Op-Ed          \r\n                News          \r\n                News          \r\n                News          \r\n               Op-Ed          \r\n           Editorial          \r\n               Op-Ed          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n       News Analysis          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n               Op-Ed          \r\n              Letter          \r\n                News          \r\n                News          \r\n                News          \r\n       News Analysis          \r\n               Op-Ed          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n              Review          \r\n            briefing          \r\n            briefing          \r\n Interactive Feature          \r\n Interactive Feature          \r\n                News          \r\n                News          \r\n Interactive Feature          \r\n              Letter          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n               Op-Ed          \r\n               Op-Ed          \r\n Interactive Feature          \r\n Interactive Feature          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n                News          \r\n Interactive Feature          \r\n Interactive Feature          \r\n\r\nhead(afghanistan_summary)\r\n\r\n\r\n   Text Types Tokens Sentences doc.id     date  section.name\r\n1 text1    32     40         1      1 1/1/2020 Today’s Paper\r\n2 text2    23     26         1      2 1/2/2020          Arts\r\n3 text3   109    170         9      3 1/2/2020      Magazine\r\n4 text4    49     56         1      4 1/3/2020  Business Day\r\n5 text5    23     25         3      5 1/3/2020      Magazine\r\n6 text6    46     60         3      6 1/3/2020    Obituaries\r\n  news.desk                                       byline\r\n1   Summary                                         <NA>\r\n2   Weekend                                 By Mike Hale\r\n3  Magazine               By Fahim Abed and Fatima Faizi\r\n4  Business        By Ben Dooley and David Yaffe-Bellany\r\n5  Magazine                             By C. J. Chivers\r\n6   Foreign By Tim Arango, Ronen Bergman and Ben Hubbard\r\n                                                                         headline.main\r\n1                    Quotation of the Day: Ex-SEAL Now Pitching Products and President\r\n2                                        The 50 TV Shows You Need to Watch This Winter\r\n3                                             Afghan War Casualty Report: January 2020\r\n4                    Carlos Ghosn Was Aided in Flight From Japan by Former Green Beret\r\n5                                                        A History of War in Six Drugs\r\n6 Qassim Suleimani, Master of Iran’s Intrigue, Built a Shiite Axis of Power in Mideast\r\n                                                      headline.print\r\n1                                                   Quote of the Day\r\n2                                  50 TV Shows To Watch  This Winter\r\n3                                                               <NA>\r\n4 A Green Beret and a Turkish Jet Company Aided in a Stunning Escape\r\n5                                                               <NA>\r\n6                        A Mastermind Of Iran’s Clout In the Mideast\r\n         material X X.1 X.2\r\n1           Quote          \r\n2            News          \r\n3            News          \r\n4            News          \r\n5            News          \r\n6 Obituary (Obit)          \r\n\r\nNext, we move to tokenization.\r\n\r\nTokens consisting of 3,442 documents and 11 docvars.\r\ntext1 :\r\n [1] \"\\\"\"      \"He's\"    \"a\"       \"Rambo\"   \"version\" \"of\"     \r\n [7] \"the\"     \"same\"    \"story\"   \"Trump\"   \"has\"     \"been\"   \r\n[ ... and 28 more ]\r\n\r\ntext2 :\r\n [1] \"Sign\"            \"up\"              \"for\"            \r\n [4] \"our\"             \"Watching\"        \"newsletter\"     \r\n [7] \"to\"              \"get\"             \"recommendations\"\r\n[10] \"on\"              \"the\"             \"best\"           \r\n[ ... and 14 more ]\r\n\r\ntext3 :\r\n [1] \"At\"             \"least\"          \"87\"            \r\n [4] \"pro-government\" \"forces\"         \"and\"           \r\n [7] \"12\"             \"civilians\"      \"were\"          \r\n[10] \"killed\"         \"in\"             \"Afghanistan\"   \r\n[ ... and 158 more ]\r\n\r\ntext4 :\r\n [1] \"TOKYO\"  \"-\"      \"Carlos\" \"Ghosn\"  \"was\"    \"aided\"  \"in\"    \r\n [8] \"his\"    \"escape\" \"from\"   \"Japan\"  \"by\"    \r\n[ ... and 44 more ]\r\n\r\ntext5 :\r\n [1] \"You're\"     \"reading\"    \"this\"       \"week's\"     \"At\"        \r\n [6] \"War\"        \"newsletter\" \".\"          \"Sign\"       \"up\"        \r\n[11] \"here\"       \"to\"        \r\n[ ... and 13 more ]\r\n\r\ntext6 :\r\n [1] \"He\"        \"changed\"   \"the\"       \"shape\"     \"of\"       \r\n [6] \"the\"       \"Syrian\"    \"civil\"     \"war\"       \"and\"      \r\n[11] \"tightened\" \"Iran's\"   \r\n[ ... and 48 more ]\r\n\r\n[ reached max_ndoc ... 3,436 more documents ]\r\n\r\nAnd remove punctuation\r\n\r\n\r\nafghanistan_tokens <- tokens(afghanistan_corpus, \r\n    remove_punct = T)\r\nprint(afghanistan_tokens)\r\n\r\n\r\nTokens consisting of 3,442 documents and 11 docvars.\r\ntext1 :\r\n [1] \"He's\"    \"a\"       \"Rambo\"   \"version\" \"of\"      \"the\"    \r\n [7] \"same\"    \"story\"   \"Trump\"   \"has\"     \"been\"    \"telling\"\r\n[ ... and 22 more ]\r\n\r\ntext2 :\r\n [1] \"Sign\"            \"up\"              \"for\"            \r\n [4] \"our\"             \"Watching\"        \"newsletter\"     \r\n [7] \"to\"              \"get\"             \"recommendations\"\r\n[10] \"on\"              \"the\"             \"best\"           \r\n[ ... and 12 more ]\r\n\r\ntext3 :\r\n [1] \"At\"             \"least\"          \"87\"            \r\n [4] \"pro-government\" \"forces\"         \"and\"           \r\n [7] \"12\"             \"civilians\"      \"were\"          \r\n[10] \"killed\"         \"in\"             \"Afghanistan\"   \r\n[ ... and 136 more ]\r\n\r\ntext4 :\r\n [1] \"TOKYO\"  \"Carlos\" \"Ghosn\"  \"was\"    \"aided\"  \"in\"     \"his\"   \r\n [8] \"escape\" \"from\"   \"Japan\"  \"by\"     \"an\"    \r\n[ ... and 40 more ]\r\n\r\ntext5 :\r\n [1] \"You're\"     \"reading\"    \"this\"       \"week's\"     \"At\"        \r\n [6] \"War\"        \"newsletter\" \"Sign\"       \"up\"         \"here\"      \r\n[11] \"to\"         \"get\"       \r\n[ ... and 11 more ]\r\n\r\ntext6 :\r\n [1] \"He\"        \"changed\"   \"the\"       \"shape\"     \"of\"       \r\n [6] \"the\"       \"Syrian\"    \"civil\"     \"war\"       \"and\"      \r\n[11] \"tightened\" \"Iran's\"   \r\n[ ... and 42 more ]\r\n\r\n[ reached max_ndoc ... 3,436 more documents ]\r\n\r\nWith quanteda, we can remove stopwords using any of few pre-defined lists that come shipped with the package. Here, we can print that list out first, then remove the tokens:\r\n\r\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \r\n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \r\n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \r\n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \r\n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \r\n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \r\n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \r\n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \r\n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \r\n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \r\n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \r\n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \r\n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \r\n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \r\n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \r\n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \r\n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \r\n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \r\n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \r\n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \r\n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \r\n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \r\n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \r\n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \r\n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \r\n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \r\n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \r\n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \r\n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \r\n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \r\n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \r\n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \r\n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \r\n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \r\n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \r\n\r\nNext, I’ll remove those stopwords.\r\n\r\n\r\n# remove stopwords from our tokens object\r\nafghanistan_tokens <- tokens_select(afghanistan_tokens, \r\n                                           pattern = stopwords(\"en\"),\r\n                                           selection = \"remove\")\r\n\r\nprint(afghanistan_tokens)\r\n\r\n\r\nTokens consisting of 3,442 documents and 11 docvars.\r\ntext1 :\r\n [1] \"Rambo\"   \"version\" \"story\"   \"Trump\"   \"telling\" \"deep\"   \r\n [7] \"state\"   \"trying\"  \"screw\"   \"media\"   \"bad\"     \"rich\"   \r\n[ ... and 2 more ]\r\n\r\ntext2 :\r\n [1] \"Sign\"            \"Watching\"        \"newsletter\"     \r\n [4] \"get\"             \"recommendations\" \"best\"           \r\n [7] \"films\"           \"TV\"              \"shows\"          \r\n[10] \"stream\"          \"watch\"           \"delivered\"      \r\n[ ... and 1 more ]\r\n\r\ntext3 :\r\n [1] \"least\"          \"87\"             \"pro-government\"\r\n [4] \"forces\"         \"12\"             \"civilians\"     \r\n [7] \"killed\"         \"Afghanistan\"    \"past\"          \r\n[10] \"week\"           \"violent\"        \"incidents\"     \r\n[ ... and 88 more ]\r\n\r\ntext4 :\r\n [1] \"TOKYO\"       \"Carlos\"      \"Ghosn\"       \"aided\"      \r\n [5] \"escape\"      \"Japan\"       \"American\"    \"security\"   \r\n [9] \"consultant\"  \"accompanied\" \"flight\"      \"country\"    \r\n[ ... and 15 more ]\r\n\r\ntext5 :\r\n [1] \"reading\"    \"week's\"     \"War\"        \"newsletter\" \"Sign\"      \r\n [6] \"get\"        \"delivered\"  \"inbox\"      \"every\"      \"Friday\"    \r\n[11] \"Email\"      \"us\"        \r\n[ ... and 1 more ]\r\n\r\ntext6 :\r\n [1] \"changed\"   \"shape\"     \"Syrian\"    \"civil\"     \"war\"      \r\n [6] \"tightened\" \"Iran's\"    \"grip\"      \"Iraq\"      \"behind\"   \r\n[11] \"hundreds\"  \"American\" \r\n[ ... and 20 more ]\r\n\r\n[ reached max_ndoc ... 3,436 more documents ]\r\n\r\nI will install the package from our course tutorials for week 7:\r\n\r\n\r\n\r\nDictionary Analysis\r\nThe basic idea with a dictionary analysis is to identify a set of words that connect to a central concept, and to count the frequency of that set of words within a document. The set of words is the dictionary; as you might quickly realize, a more appropriate name is probably thesaurus.\r\nliwcalike()\r\nThere are a couple of ways to do this. First, the quanteda.dictionaries package contains the liwcalike() function, which takes a corpus or character vector and carries out an analysis — based on a provided dictionary — that mimics the pay-to-play software LIWC (Linguistic Inquiry and Word Count see here). The LIWC software calculates the percentage of the document that reflects a host of different characteristics. We are going to focus on positive and negative language, but keep in mind that there are lots of other dimensions that could be of interest.\r\n\r\n [1] \"docname\"      \"Segment\"      \"WPS\"          \"WC\"          \r\n [5] \"Sixltr\"       \"Dic\"          \"anger\"        \"anticipation\"\r\n [9] \"disgust\"      \"fear\"         \"joy\"          \"negative\"    \r\n[13] \"positive\"     \"sadness\"      \"surprise\"     \"trust\"       \r\n[17] \"AllPunc\"      \"Period\"       \"Comma\"        \"Colon\"       \r\n[21] \"SemiC\"        \"QMark\"        \"Exclam\"       \"Dash\"        \r\n[25] \"Quote\"        \"Apostro\"      \"Parenth\"      \"OtherP\"      \r\n\r\nPolarity: The most positive selections\r\n\r\n\r\n\r\nand negative\r\n\r\n\r\n\r\nBased on that, let’s look at those that are out in the right tail (i.e., which are greater than 15).\r\n\r\nCorpus consisting of 36 documents and 11 docvars.\r\ntext109 :\r\n\"Can a woman be president of the United States?\"\r\n\r\ntext277 :\r\n\"KABUL, Afghanistan — Momentum is building toward peace in Af...\"\r\n\r\ntext580 :\r\n\"On a holiday that usually mixes somber remembrance and bliss...\"\r\n\r\ntext608 :\r\n\"WASHINGTON — Gen. Mark A. Milley was never meant to be Presi...\"\r\n\r\ntext674 :\r\n\"For anyone trying to make sense of the Supreme Court run by ...\"\r\n\r\ntext725 :\r\n\"WASHINGTON — A female National Guard soldier graduated from ...\"\r\n\r\n[ reached max_ndoc ... 30 more documents ]\r\n\r\nand negative\r\n\r\nCorpus consisting of 28 documents and 11 docvars.\r\ntext280 :\r\n\"— Parisa, an inmate at Herat Women’s Prison\"\r\n\r\ntext290 :\r\n\"This war is different.\"\r\n\r\ntext310 :\r\n\"[Read the Afghan War Casualty Report from previous months.]\"\r\n\r\ntext455 :\r\n\"What happens when the pandemic comes to a country in conflic...\"\r\n\r\ntext479 :\r\n\"Donations flooded in to fight the virus devastating the city...\"\r\n\r\ntext504 :\r\n\"[Read the Afghan War Casualty Report from previous weeks.]\"\r\n\r\n[ reached max_ndoc ... 22 more documents ]\r\n\r\nLooking at the tutorial approach to considering addressing the polarity:\r\n\r\n\r\n\r\n\r\nCorpus consisting of 38 documents and 11 docvars.\r\ntext280 :\r\n\"— Parisa, an inmate at Herat Women’s Prison\"\r\n\r\ntext290 :\r\n\"This war is different.\"\r\n\r\ntext310 :\r\n\"[Read the Afghan War Casualty Report from previous months.]\"\r\n\r\ntext432 :\r\n\"This is not a time to die.\"\r\n\r\ntext455 :\r\n\"What happens when the pandemic comes to a country in conflic...\"\r\n\r\ntext479 :\r\n\"Donations flooded in to fight the virus devastating the city...\"\r\n\r\n[ reached max_ndoc ... 32 more documents ]\r\n\r\nOn initial review, I am unsure that I will be able to really capture the sentiment of the articles given that I only have access to the lead paragraph of each article.\r\nIt’s possible that another valid approach to analyzing the coverage that may be more viable. There is a clear difference in the way print and online article titles are framed. Perhaps analyzing the sentiment of the titles and comparing them will be the way to go. I will look at that option in my next post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-03-dacss-697e-post-4/distill-preview.png",
    "last_modified": "2022-04-05T01:19:07-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-04-dacss-697e-assignment-6/",
    "title": "DACSS 697E Assignment 6",
    "description": "Assignment 6 for DACSS 697E course 'Social and Political Network Analysis': \"Network Roles\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-03-27",
    "categories": [
      "networks",
      "homework",
      "grateful network"
    ],
    "contents": "\r\n\r\nContents\r\nNetwork Structure\r\nCreate a Data Frame\r\nStructural Equivalence\r\n“Complete”\r\n“Average”\r\n“Single”\r\n“Ward.D”\r\n\r\nPartition Using Clustering\r\nHeight= 15\r\nHeight= 10\r\nHeight= 3\r\n\r\nBlockmodel Partitions\r\n2-partition blockmodel\r\n5-partition blockmodel\r\n\r\nPlotting Network Roles\r\nStatnet\r\nIgraph\r\n\r\nCentrality by Network Role\r\nConcoR\r\nBest model\r\n\r\n\r\nI am continuing to use the Grateful Dead song writing data set that I used in previous assignments to examine co-writing links and centrality. The data set consists of the links between co-writers of songs played by the Grateful Dead over their 30-year touring career that I compiled.\r\nThere are 26 songwriters that contributed to the songs played over the course of the Grateful Dead history, resulting in 26 nodes in the dataset.\r\nThere are a total of 183 (updated and still under review!) unique songs played, and the varies combinations of co-writing combinations are now represented in a binary affiliation matrix.\r\nI plan on eventually introducing the number of times a given song was played live as a method of weighting the network, given the culture of the band and its’ community was one of collaboration and the prominence of a song being representative of the level at which it resonated with the community. I need to continue to investigate the best time and way to incorporate those weights.\r\n\r\n\r\n\r\nLoading the dataset and creating the network to begin this assignment:\r\n\r\n\r\nShow code\r\n\r\ngd_vertices <- read.csv(\"gd_nodes.csv\", header=T, stringsAsFactors=F)\r\ngd_affiliation <- read.csv(\"gd_affiliation_matrix.csv\", row.names = 1, header = TRUE, check.names = FALSE)\r\ngd_matrix <- as.matrix(gd_affiliation)\r\ngd_projection <- gd_matrix%*%t(gd_matrix)\r\n\r\n#Create igraph and statnet Objects\r\n\r\ngd_network_ig <- graph.adjacency(gd_projection,mode=\"undirected\") #igraph object\r\ngd_network_stat <- network(gd_projection, directed=F, matrix.type=\"adjacency\") #statnet object\r\n\r\n\r\n\r\nNetwork Structure\r\nThe network is an undirected, unweighted network. It has two components; one large component with one isolate.\r\nThe statnet object has a density of 0.2 and a transitivity of 0.5241.\r\n\r\n\r\nShow code\r\n\r\n#Inspect New igraph and statnet objects\r\n\r\nprint(gd_network_stat)\r\n\r\n\r\n Network attributes:\r\n  vertices = 26 \r\n  directed = FALSE \r\n  hyper = FALSE \r\n  loops = FALSE \r\n  multiple = FALSE \r\n  bipartite = FALSE \r\n  total edges= 65 \r\n    missing edges= 0 \r\n    non-missing edges= 65 \r\n\r\n Vertex attribute names: \r\n    vertex.names \r\n\r\nNo edge attributes\r\n\r\nShow code\r\n\r\nigraph::components(gd_network_ig)$no\r\n\r\n\r\n[1] 2\r\n\r\nShow code\r\n\r\ngden(gd_network_stat)\r\n\r\n\r\n[1] 0.2\r\n\r\nShow code\r\n\r\ngtrans(gd_network_stat)\r\n\r\n\r\n[1] 0.5240964\r\n\r\nCreate a Data Frame\r\nI need to create a data frame of the network node data as in previous tutorials/assignments, but having had trouble using both igraph and statnet in the same .rmd, I used igraph in previous assignments. I’ll use statnet in this assignment and inspect the differences as well.\r\n\r\n\r\nShow code\r\n\r\nget.eigen<-function(net, attr=NULL){\r\n    #set attr=\"weight\" if weighted network\r\n    eigen<-evcent(net)\r\n    mat<-as.matrix.network(net, attr=attr)\r\n    diag(mat)<-0\r\n    mat2<-mat%*%mat\r\n    rc<-diag(mat2)/rowSums(mat2)\r\n    dc<-1-rc\r\n    data.frame(name=net%v%\"vertex.names\",\r\n        eigen=eigen,\r\n        eigen.rc=eigen*rc,\r\n        eigen.dc=eigen*dc)\r\n}\r\nget.brokerage<-function(net, attr=\"attr\"){\r\n  temp<-data.frame(brokerage(net, cl = net%v%\"attr\")$z.nli)\r\n  temp$name=net%v%\"vertex.names\"\r\n  mutate(temp, broker.tot = temp$t,\r\n         broker.coord = temp$w_I,\r\n         broker.itin = temp$w_O,\r\n         broker.rep = temp$b_IO,\r\n         broker.gate = temp$b_OI,\r\n         broker.lia = temp$b_O)%>%\r\n    select(contains(\"broker\"))\r\n}\r\n\r\n\r\n\r\nBecause I am again having an issue with using igraph and statnet in one knit chunk, I need to save my data frame then recall it to knit.\r\n\r\n\r\nShow code\r\n\r\n#create dataframe with names\r\n#gd_nodes<-data.frame(name=gd_network_stat%v%\"vertex.names\",\r\n        #degree=sna::degree(gd_network_stat,gmode=\"graph\"),\r\n        #bonpow=bonpow(gd_network_stat),\r\n        #betweenness=betweenness(gd_network_stat, gmode=\"graph\"),\r\n        #close=sna::closeness(gd_network_stat, gmode=\"graph\", cmode = \"undirected\"),\r\n        #constraint=constraint(gd_network_ig))\r\n\r\n#add eigenvector centrality using custom function\r\n#gd_nodes<-full_join(gd_nodes,get.eigen(gd_network_stat), by=\"name\")\r\n\r\n#write csv for retrieval in knitting document\r\n#write.csv(gd_nodes, file = \"gd_nodes6.csv\")\r\n\r\n\r\n\r\n\r\n\r\ngd_nodes <- read.csv(\"gd_nodes6.csv\")\r\n\r\nhead(gd_nodes)\r\n\r\n\r\n  X           name degree      bonpow betweenness close constraint\r\n1 1  Eric Andersen      1  0.02612461    0.000000     0  1.0000000\r\n2 2    John Barlow      3 -1.44991592    0.750000     0  0.6706222\r\n3 3    Bob Bralove      5 -1.41072900    1.833333     0  0.4989170\r\n4 4 Andrew Charles      1 -0.48330531    0.000000     0  1.0000000\r\n5 5    John Dawson      2 -1.18866981    0.000000     0  1.2945238\r\n6 6   Willie Dixon      2  0.27430842    0.000000     0  0.7040590\r\n       eigen    eigen.rc   eigen.dc\r\n1 0.04883644 0.002872732 0.04596371\r\n2 0.07763512 0.008957898 0.06867722\r\n3 0.12770600 0.015963250 0.11174275\r\n4 0.04317463 0.003083902 0.04009073\r\n5 0.07977694 0.007597803 0.07217913\r\n6 0.06423801 0.005839819 0.05839819\r\n\r\nStructural Equivalence\r\nCreating the matrix element then taking a look at the summary using the equivalence function “sedist”, the default measure of assessing the approximate structural equivalence of actors, or “complete”.\r\n“Complete”\r\n\r\n\r\nShow code\r\n\r\n#calculate equivalence from specified distance marix\r\ngd_stat_se<-equiv.clust(gd_network_stat, equiv.fun=\"sedist\", method=\"hamming\",mode=\"graph\")\r\n\r\n\r\n\r\n\r\n\r\n#summary of object produced by sedist()\r\nsummary(gd_stat_se)\r\n\r\n\r\n               Length Class  Mode     \r\ncluster         7     hclust list     \r\nmetric          1     -none- character\r\nequiv.fun       1     -none- character\r\ncluster.method  1     -none- character\r\nglabels        26     -none- character\r\nplabels        26     -none- character\r\n\r\n#plot equivalence clustering\r\nplot(gd_stat_se,labels=gd_stat_se$glabels)\r\n\r\n\r\n\r\n\r\nI need to look at the other methods of clustering as well.\r\n“Average”\r\n\r\n\r\nShow code\r\n\r\n#with average cluster.method\r\ngd_avg_se<-equiv.clust(gd_network_stat, equiv.fun=\"sedist\", cluster.method=\"average\", method=\"hamming\",mode=\"graph\")\r\n#plot:\r\nplot(gd_avg_se,labels=gd_stat_se$glabels)\r\n\r\n\r\n\r\n\r\n“Single”\r\n\r\n\r\nShow code\r\n\r\n#with average cluster.method\r\ngd_sing_se<-equiv.clust(gd_network_stat, equiv.fun=\"sedist\", cluster.method=\"single\", method=\"hamming\",mode=\"graph\")\r\n\r\n#plot:\r\nplot(gd_sing_se,labels=gd_stat_se$glabels)\r\n\r\n\r\n\r\n\r\n“Ward.D”\r\n\r\n\r\nShow code\r\n\r\n#with average cluster.method\r\ngd_wrd_se<-equiv.clust(gd_network_stat, equiv.fun=\"sedist\", cluster.method=\"ward.D\", method=\"hamming\",mode=\"graph\")\r\n\r\n#plot:\r\nplot(gd_wrd_se,labels=gd_stat_se$glabels)\r\n\r\n\r\n\r\n\r\nIt is interesting, because none of these dendograms represent fully what I feel accurately represents this network, but it makes me want to look more deeply and understand the network.\r\nPartition Using Clustering\r\nI understand that the number of partitions (or roles) will depend on the height at which the dendrogram is cut. Using the tutorial example, I set the height at 15 and the result is 5 clusters. Using the alternate view from the tutorial, I also set the height at 10, and identify 8 distinct clusters or roles.\r\nHeight= 15\r\n\r\n\r\nShow code\r\n\r\n#plot equivalence clustering\r\nplot(gd_stat_se,labels=gd_stat_se$glabels)\r\n#partition the clusters\r\nrect.hclust(gd_stat_se$cluster,h=15)\r\n\r\n\r\n\r\n\r\nHeight= 10\r\n\r\n\r\nShow code\r\n\r\n#plot equivalence clustering\r\nplot(gd_stat_se,labels=gd_stat_se$glabels)\r\n#partition the clusters\r\nrect.hclust(gd_stat_se$cluster,h=10)\r\n\r\n\r\n\r\n\r\nHeight= 3\r\nFor my own experimenting, looking at it with an even lower height (“3”), it spreads the clusters out to 16.\r\n\r\n\r\nShow code\r\n\r\n#plot equivalence clustering\r\nplot(gd_stat_se,labels=gd_stat_se$glabels)\r\n#partition the clusters\r\nrect.hclust(gd_stat_se$cluster,h=3)\r\n\r\n\r\n\r\n\r\nBlockmodel Partitions\r\nInspecting the goodness of fit of the partitions that result from the clustering steps above using blockmodeling to try and get a better sense of how well the partitioning worked. Using the blockmodel command in statnet and specifying “k=x” means that “x” will indicate how many partitions to create, and “h=x” means that “x” will indicate the height to cut the dendogram.\r\n2-partition blockmodel\r\n\r\n\r\nShow code\r\n\r\n#blockmodel and select partitions\r\nblk_mod<-blockmodel(gd_network_stat,gd_stat_se,k=2)\r\n#print blockmodel object\r\nblk_mod\r\n\r\n\r\n\r\nNetwork Blockmodel:\r\n\r\nBlock membership:\r\n\r\n  Eric Andersen     John Barlow     Bob Bralove  Andrew Charles \r\n              1               1               1               1 \r\n    John Dawson    Willie Dixon    Jerry Garcia  Donna Godchaux \r\n              1               1               1               1 \r\n Keith Godchaux   Gerrit Graham     Frank Guida     Mickey Hart \r\n              1               1               1               1 \r\n  Bruce Hornsby   Robert Hunter Bill Kreutzmann       Ned Lagin \r\n              1               1               1               1 \r\n      Phil Lesh      Peter Monk   Brent Mydland     Dave Parker \r\n              2               1               1               1 \r\nRobert Petersen          Pigpen     Joe Royster   Rob Wasserman \r\n              1               1               1               1 \r\n       Bob Weir   Vince Welnick \r\n              2               1 \r\n\r\nReduced form blockmodel:\r\n\r\n     Eric Andersen John Barlow Bob Bralove Andrew Charles John Dawson Willie Dixon Jerry Garcia Donna Godchaux Keith Godchaux Gerrit Graham Frank Guida Mickey Hart Bruce Hornsby Robert Hunter Bill Kreutzmann Ned Lagin Phil Lesh Peter Monk Brent Mydland Dave Parker Robert Petersen Pigpen Joe Royster Rob Wasserman Bob Weir Vince Welnick \r\n          Block 1   Block 2\r\nBlock 1 0.1268116 0.6041667\r\nBlock 2 0.6041667 1.0000000\r\n\r\n\r\n\r\nShow code\r\n\r\nplot.block<-function(x=blk_mod, main=NULL, cex.lab=1){\r\n  plot.sociomatrix(x$blocked.data, labels=list(x$plabels,x$plabels),\r\n                   main=main, drawlines = FALSE, cex.lab=cex.lab)\r\n  for (j in 2:length(x$plabels)) if (x$block.membership[j] !=\r\n                                     x$block.membership[j-1]) \r\n    abline(v = j - 0.5, h = j - 0.5, lty = 3, xpd=FALSE)\r\n}\r\nplot.block(blk_mod,main=\"Grateful Dead Songwriting: 2 Partitions\", cex.lab=.4)\r\n\r\n\r\n\r\n\r\n5-partition blockmodel\r\n\r\n\r\nShow code\r\n\r\n#blockmodel and select partitions\r\nblk_mod2<-blockmodel(gd_network_stat,gd_stat_se,k=5)\r\n#print blockmodel object\r\nblk_mod2$block.model\r\n\r\n\r\n           Block 1    Block 2 Block 3   Block 4 Block 5\r\nBlock 1 0.06666667 0.05208333  0.3125 0.0625000     0.5\r\nBlock 2 0.05208333 0.93333333  1.0000 0.4166667     1.0\r\nBlock 3 0.31250000 1.00000000     NaN 1.0000000     1.0\r\nBlock 4 0.06250000 0.41666667  1.0000 1.0000000     1.0\r\nBlock 5 0.50000000 1.00000000  1.0000 1.0000000     NaN\r\n\r\nShow code\r\n\r\n#plot blockmodel partitions\r\nplot.block(blk_mod2,main=\"Grateful Dead Songwriting, 5 Partitions\", cex.lab=.5)\r\n\r\n\r\n\r\n\r\nPlotting Network Roles\r\nTo do this, I will assign “block.membership” as a vertex attribute to my 5-partition blockmodel, then use the role attribute to change the color of plotted nodes in a network plot. I am using this part of the tutorial as well because graphics are always a fun thing to do in the middle of learning new concepts and coding all day! I will definitely want to investigate another aspect of this data and graphing options more later because the results show me that this network is not exactly graphing in a familiar way.\r\n\r\n\r\nShow code\r\n\r\nblk_mod3<-blockmodel(gd_network_stat,gd_stat_se,k=5)\r\n\r\n\r\n\r\nStatnet\r\n\r\nIgraph\r\n\r\n\r\nShow code\r\n\r\nlibrary(igraph)\r\n\r\nV(gd_network_ig)$role<-blk_mod3$block.membership[match(V(gd_network_ig)$name,blk_mod3$plabels)]\r\n\r\n#plot network using \"role\" to color nodes: igraph\r\nplot(gd_network_ig, layout=layout_with_kk, vertex.color=V(gd_network_ig)$role)\r\n\r\n\r\n\r\n\r\nCentrality by Network Role\r\nI am attempting to use the “gd_nodes” table created earlier and adding the role assignments from the “blockmodel” calculations to summarise average node measures of centrality by role.\r\nYet again, I am finding that the code used for this process using both igraph and statnet worked until I knit the document, but is giving me trouble at that point, so I will revisit this process.\r\n\r\n\r\nShow code\r\n\r\nlibrary(igraph)\r\n#attach role to .nodes dataframe\r\ngd_new_nodes <- gd_nodes$role<-V(gd_network_ig)$role\r\n\r\n\r\n\r\nConcoR\r\nUtilizing the “concoR” package\r\n\r\n\r\nShow code\r\n\r\nlibrary(concoR)\r\n#select partitions with concor\r\nconcoR::concor_hca(list(gd_projection), p=2)\r\n\r\n\r\n   block          vertex\r\n1      1   Eric Andersen\r\n2      1     John Barlow\r\n7      2     Bob Bralove\r\n11     3  Andrew Charles\r\n17     4     John Dawson\r\n8      2    Willie Dixon\r\n18     4    Jerry Garcia\r\n19     4  Donna Godchaux\r\n20     4  Keith Godchaux\r\n3      1   Gerrit Graham\r\n12     3     Frank Guida\r\n21     4     Mickey Hart\r\n4      1   Bruce Hornsby\r\n22     4   Robert Hunter\r\n23     4 Bill Kreutzmann\r\n13     3       Ned Lagin\r\n24     4       Phil Lesh\r\n14     3      Peter Monk\r\n5      1   Brent Mydland\r\n25     4     Dave Parker\r\n15     3 Robert Petersen\r\n26     4          Pigpen\r\n16     3     Joe Royster\r\n9      2   Rob Wasserman\r\n6      1        Bob Weir\r\n10     2   Vince Welnick\r\n\r\nTaking this output and plotting it as I tried earlier, I get a better visualization of the blockmodeling:\r\n\r\n           Block 1   Block 2    Block 3   Block 4\r\nBlock 1 0.33333333 0.2083333 0.02777778 0.1666667\r\nBlock 2 0.20833333 0.6666667 0.00000000 0.0750000\r\nBlock 3 0.02777778 0.0000000 0.06666667 0.1000000\r\nBlock 4 0.16666667 0.0750000 0.10000000 0.6666667\r\n\r\n\r\n\r\nShow code\r\n\r\n#plot blockmodel partitions\r\nplot.block(blk_mod,main=\"Grateful Dead Songwriting, Concor 4 Partitions\", cex.lab=.5)\r\n\r\n\r\n\r\n\r\nBest model\r\nFinally, I want to look at the “optimized” 5 partition model, which in the end seems to represent the network most intuitively of the models I’ve explored so far.\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n#plot blockmodel partitions\r\nplot.block(blk_mod,main=\"Grateful Dead Songwriting, Optimized 5 Partitions\", cex.lab=.5)\r\n\r\n\r\n\r\n\r\nI have more evaluations to do but I continue to struggle with using igraph and statnet in the same .rmd file, so I will revisit this process in future posts.\r\nCitations:\r\nAllan, Alex; Grateful Dead Lyric & Song Finder: https://whitegum.com/~acsa/intro.htm\r\nASCAP. 18 March 2022.\r\nDodd, David; The Annotated Grateful Dead Lyrics: http://artsites.ucsc.edu/gdead/agdl/\r\nSchofield, Matt; The Grateful Dead Family Discography: http://www.deaddisc.com/\r\nPhoto by Grateful Dead Productions\r\nThis information is intended for private research only, and not for any commercial use. Original Grateful Dead songs are ©copyright Ice Nine Music\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-04-dacss-697e-assignment-6/distill-preview.png",
    "last_modified": "2022-04-12T01:27:40-05:00",
    "input_file": "dacss-697e-assignment-6.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-04-dacss-697e-assignment-5/",
    "title": "DACSS 697E Assignment 5",
    "description": "Assignment 5 for DACSS 697E course 'Social and Political Network Analysis': \"Brokerage and Betweenness\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-03-26",
    "categories": [
      "networks",
      "homework",
      "grateful network"
    ],
    "contents": "\r\n\r\nContents\r\nNetwork Details\r\nCentrality Scores\r\nEigenvector Centrality\r\nCloseness\r\nBetweenness\r\nTop Betweenness\r\nTop Closeness\r\nNetwork Constraint (Burt)\r\n\r\n\r\nNetwork Details\r\nI am continuing to use the Grateful Dead song writing data set that I used in previous assignments to examine co-writing links and centrality.\r\nThe data set consists of the links between co-writers of songs played by the Grateful Dead over their 30-year touring career that I compiled. One aspect of the Grateful Dead song data is that the connections between co-writers is weighted, with the weights representing the number of time each song was played live.\r\nThere are 26 songwriters that contributed to the songs played over the course of the Grateful Dead history, resulting in 26 nodes in the dataset.\r\nThere are a total of 183 (updated and still under review!) unique songs played, and the varies combinations of co-writing combinations are now represented in a binary affiliation matrix.\r\n\r\n\r\n\r\nLoading the dataset and creating the network to begin this assignment:\r\n\r\n\r\nShow code\r\n\r\ngd_vertices <- read.csv(\"gd_nodes.csv\", header=T, stringsAsFactors=F)\r\ngd_affiliation <- read.csv(\"gd_affiliation_matrix.csv\", row.names = 1, header = TRUE, check.names = FALSE)\r\ngd_matrix <- as.matrix(gd_affiliation)\r\ngd_projection <- gd_matrix%*%t(gd_matrix)\r\n\r\n#Create Igraph Object\r\n\r\ngd_network_ig <- graph.adjacency(gd_projection,mode=\"undirected\") #igraph object\r\n\r\n\r\n\r\nThis is a non-directed, unweighted igraph object. It has two components; one large component with one isolate.\r\n\r\n\r\n#Inspect New Object\r\n\r\nigraph::vertex_attr_names(gd_network_ig)\r\n\r\n\r\n[1] \"name\"\r\n\r\nigraph::edge_attr_names(gd_network_ig)\r\n\r\n\r\ncharacter(0)\r\n\r\nhead(V(gd_network_ig)$name)\r\n\r\n\r\n[1] \"Eric Andersen\"  \"John Barlow\"    \"Bob Bralove\"   \r\n[4] \"Andrew Charles\" \"John Dawson\"    \"Willie Dixon\"  \r\n\r\nis_directed(gd_network_ig)\r\n\r\n\r\n[1] FALSE\r\n\r\nis_weighted(gd_network_ig)\r\n\r\n\r\n[1] FALSE\r\n\r\nis_bipartite(gd_network_ig)\r\n\r\n\r\n[1] FALSE\r\n\r\nigraph::dyad.census(gd_network_ig)\r\n\r\n\r\n$mut\r\n[1] 738\r\n\r\n$asym\r\n[1] 0\r\n\r\n$null\r\n[1] -413\r\n\r\nigraph::triad.census(gd_network_ig)\r\n\r\n\r\n [1] 1788    0  488    0    0    0    0    0    0    0  237    0    0\r\n[14]    0    0   87\r\n\r\nCentrality Scores\r\nTo examine the centrality and power scores of the nodes, I’m creating a data frame with the centrality degree, normalized centrality, Bonacich power, Eigenvector centrality scores and the breakdown of reflected and derived centrality scores.\r\nTo calculate the reflected and derived centrality scores, I first run some operations on the adjacency matrix and keep in mind that these two scores make up the entire calculation of the Eigenvector centrality score.\r\n\r\n\r\nShow code\r\n\r\ngd_adjacency <- as.matrix(as_adjacency_matrix(gd_network_ig))\r\ngd_adjacency_2 <- gd_adjacency %*% gd_adjacency\r\n\r\n#calculate portion of reflected centrality\r\ngd_reflective <- diag(as.matrix(gd_adjacency_2))/rowSums(as.matrix(gd_adjacency_2))\r\ngd_reflective <- ifelse(is.nan(gd_reflective),0,gd_reflective)\r\n\r\n#calculate derived centrality\r\ngd_derived <- 1-diag(as.matrix(gd_adjacency_2))/rowSums(as.matrix(gd_adjacency_2))\r\ngd_derived <- ifelse(is.nan(gd_derived),1,gd_derived)\r\n\r\ncentrality_gd <-data.frame(id=1:vcount(gd_network_ig),\r\n                        name=V(gd_network_ig)$name,\r\n                        degree_all=igraph::degree(gd_network_ig),\r\n                        degree_norm=igraph::degree(gd_network_ig,normalized=T),\r\n                        BC_power=power_centrality(gd_network_ig),\r\n                        EV_cent=centr_eigen(gd_network_ig,directed = F)$vector,\r\n                        reflect_EV=gd_reflective*centr_eigen(gd_network_ig,directed = F)$vector,\r\n                        derive_EV=gd_derived*centr_eigen(gd_network_ig,directed = F)$vector)\r\n\r\nrow.names(centrality_gd)<-NULL\r\ncentrality_gd%>%\r\n  arrange(desc(degree_all))%>%\r\n  slice(1:5)\r\n\r\n\r\n  id            name degree_all degree_norm   BC_power    EV_cent\r\n1  7    Jerry Garcia        328       13.12 -0.2551417 0.96094165\r\n2 14   Robert Hunter        313       12.52 -0.1735142 1.00000000\r\n3 25        Bob Weir        213        8.52 -0.5430836 0.18725953\r\n4 17       Phil Lesh        149        5.96 -0.1806656 0.15133380\r\n5 15 Bill Kreutzmann        100        4.00 -0.7011548 0.09223647\r\n   reflect_EV  derive_EV\r\n1 0.332625452 0.62831620\r\n2 0.371327549 0.62867245\r\n3 0.040709421 0.14655011\r\n4 0.022140576 0.12919322\r\n5 0.009710558 0.08252591\r\n\r\nRight away, I see the highest degree are clearly Jerry Garcia and Robert Hunter, which makes sense given that they were a songwriting pair that were prolific in creating the Grateful Dead original songbook. Bob Weir also contributed quite a bit, though the songs he wrote with his writing partner John Barlow numbered many less than those that he wrote as part of the whole band, judging by Barlow’s absence in the top counts.\r\nThe original lineup of Jerry Garcia, Bob Weir, Phil Lesh, Bill Kreutzmann, and Pigpen as well as Robert Hunter’s presence in the formative years of the band’s most collaborative era, means that this degree ranking makes sense intuitively.\r\nEigenvector Centrality\r\nI am also interested in the Eigenvector centrality scores - Both the top as well as the lowest value scores.\r\n\r\n\r\nShow code\r\n\r\ncentrality_gd%>%\r\n  arrange(desc(EV_cent))%>%\r\n  slice(1:5)\r\n\r\n\r\n  id            name degree_all degree_norm   BC_power    EV_cent\r\n1 14   Robert Hunter        313       12.52 -0.1735142 1.00000000\r\n2  7    Jerry Garcia        328       13.12 -0.2551417 0.96094165\r\n3 25        Bob Weir        213        8.52 -0.5430836 0.18725953\r\n4 17       Phil Lesh        149        5.96 -0.1806656 0.15133380\r\n5 15 Bill Kreutzmann        100        4.00 -0.7011548 0.09223647\r\n   reflect_EV  derive_EV\r\n1 0.371327549 0.62867245\r\n2 0.332625452 0.62831620\r\n3 0.040709421 0.14655011\r\n4 0.022140576 0.12919322\r\n5 0.009710558 0.08252591\r\n\r\nRobert Hunter having the top Eigenvector centrality score is not a shock - he has long held the unofficial title of band member and as the person behind the songwriting magic of the Grateful Dead. His primary songwriting partner was Jerry Garcia, but he also wrote songs with the early, full band and later with almost all of the individual members of the band.\r\nIt is a little surprising, though, that the Eigenvector scores fall off so quickly after Robert Hunter and Jerry Garcia.\r\nCloseness\r\nThe closeness centrality of a node is defined as the sum of the geodesic distances between that node and all other nodes in a network. This works; however, I get a warning that closeness centrality is not well-defined for disconnected graphs.\r\n\r\n\r\n#calculate closeness centrality: igraph\r\nigraph::closeness(gd_network_ig)\r\n\r\n\r\n  Eric Andersen     John Barlow     Bob Bralove  Andrew Charles \r\n    0.012500000     0.012987013     0.013333333     0.012048193 \r\n    John Dawson    Willie Dixon    Jerry Garcia  Donna Godchaux \r\n    0.012048193     0.012658228     0.015625000     0.014285714 \r\n Keith Godchaux   Gerrit Graham     Frank Guida     Mickey Hart \r\n    0.014492754     0.012500000     0.011363636     0.014492754 \r\n  Bruce Hornsby   Robert Hunter Bill Kreutzmann       Ned Lagin \r\n    0.001538462     0.015873016     0.015384615     0.012048193 \r\n      Phil Lesh      Peter Monk   Brent Mydland     Dave Parker \r\n    0.016666667     0.012048193     0.013698630     0.014492754 \r\nRobert Petersen          Pigpen     Joe Royster   Rob Wasserman \r\n    0.012345679     0.015151515     0.011363636     0.013333333 \r\n       Bob Weir   Vince Welnick \r\n    0.017543860     0.013157895 \r\n\r\nIn addition to node-level centrality scores, I also want to calculate the network level centralization index for closeness centrality measures. Again, I get a warning that closeness centrality is not well-defined for disconnected graphs.\r\n\r\n\r\n#calculate closeness centralization index: igraph\r\ncentr_clo(gd_network_ig)$centralization\r\n\r\n\r\n[1] 0.2310331\r\n\r\nBetweenness\r\nBetweenness represents the number of geodesics on which a node sits.\r\n\r\n\r\n#calculate betweenness centrality: igraph\r\nigraph::betweenness(gd_network_ig, directed=FALSE)\r\n\r\n\r\n  Eric Andersen     John Barlow     Bob Bralove  Andrew Charles \r\n   0.000000e+00    6.708464e-01    1.216013e-01    0.000000e+00 \r\n    John Dawson    Willie Dixon    Jerry Garcia  Donna Godchaux \r\n   0.000000e+00    0.000000e+00    1.658436e+01    0.000000e+00 \r\n Keith Godchaux   Gerrit Graham     Frank Guida     Mickey Hart \r\n   9.345794e-03    0.000000e+00    0.000000e+00    3.738318e-02 \r\n  Bruce Hornsby   Robert Hunter Bill Kreutzmann       Ned Lagin \r\n   0.000000e+00    2.410682e+01    3.132042e+00    0.000000e+00 \r\n      Phil Lesh      Peter Monk   Brent Mydland     Dave Parker \r\n   9.039664e+01    0.000000e+00    1.306941e+00    0.000000e+00 \r\nRobert Petersen          Pigpen     Joe Royster   Rob Wasserman \r\n   0.000000e+00    4.402857e+01    0.000000e+00    9.459707e-01 \r\n       Bob Weir   Vince Welnick \r\n   1.216595e+02    0.000000e+00 \r\n\r\nTop Betweenness\r\nNow I want to add the closeness and betweenness to my centrality data frame and first, sort by and take a look at the nodes with the highest betweenness:\r\n\r\n\r\nShow code\r\n\r\ncentrality_gd <-data.frame(id=1:vcount(gd_network_ig),\r\n                        name=V(gd_network_ig)$name,\r\n                        degree_all=igraph::degree(gd_network_ig),\r\n                        degree_norm=igraph::degree(gd_network_ig,normalized=T),\r\n                        BC_power=power_centrality(gd_network_ig),\r\n                        EV_cent=centr_eigen(gd_network_ig,directed = F)$vector,\r\n                        reflect_EV=gd_reflective*centr_eigen(gd_network_ig,directed = F)$vector,\r\n                        derive_EV=gd_derived*centr_eigen(gd_network_ig,directed = F)$vector,\r\n                        close=closeness(gd_network_ig),\r\n                        between=betweenness(gd_network_ig, directed=FALSE))\r\n                        \r\n\r\nrow.names(centrality_gd)<-NULL\r\ncentrality_gd%>%\r\n  arrange(desc(between))%>%\r\n  slice(1:5)\r\n\r\n\r\n  id          name degree_all degree_norm   BC_power    EV_cent\r\n1 25      Bob Weir        213        8.52 -0.5430836 0.18725953\r\n2 17     Phil Lesh        149        5.96 -0.1806656 0.15133380\r\n3 22        Pigpen         95        3.80 -0.5257366 0.07985305\r\n4 14 Robert Hunter        313       12.52 -0.1735142 1.00000000\r\n5  7  Jerry Garcia        328       13.12 -0.2551417 0.96094165\r\n   reflect_EV  derive_EV      close   between\r\n1 0.040709421 0.14655011 0.01754386 121.65948\r\n2 0.022140576 0.12919322 0.01666667  90.39664\r\n3 0.009031643 0.07082141 0.01515152  44.02857\r\n4 0.371327549 0.62867245 0.01587302  24.10682\r\n5 0.332625452 0.62831620 0.01562500  16.58436\r\n\r\nThe most immediate observations I have is that the highest degree node (Jerry Garcia) is not the node with the highest scoring betweenness. That goes to Bob Weir, who is still a relatively high degree node, but significantly lower than Jerry Garcia given that his betweenness score is so much higher (~121 compared to Garcia’s ~16).\r\nI can make a guess that the two highest degree nodes, Jerry Garcia and Robert Hunter, having relatively low betweenness scores can be linked to the fact that the two wrote mostly together. Although the pair wrote the most songs in the originals catalog, Bob Weir wrote many songs with a variety of other songwrriters; giving him a higher level of betweenness.\r\nSimilarly, Phil Lesh and Pigpen, original band members who wrote relatively fewer songs, contributed to more songs that were written by the entire band, giving them more exposure to connections on the songs that they did write.\r\nTop Closeness\r\nNow a look at the top closeness scores:\r\n\r\n\r\nShow code\r\n\r\ncentrality_gd%>%\r\n  arrange(desc(close))%>%\r\n  slice(1:5)\r\n\r\n\r\n  id            name degree_all degree_norm   BC_power    EV_cent\r\n1 25        Bob Weir        213        8.52 -0.5430836 0.18725953\r\n2 17       Phil Lesh        149        5.96 -0.1806656 0.15133380\r\n3 14   Robert Hunter        313       12.52 -0.1735142 1.00000000\r\n4  7    Jerry Garcia        328       13.12 -0.2551417 0.96094165\r\n5 15 Bill Kreutzmann        100        4.00 -0.7011548 0.09223647\r\n   reflect_EV  derive_EV      close    between\r\n1 0.040709421 0.14655011 0.01754386 121.659478\r\n2 0.022140576 0.12919322 0.01666667  90.396640\r\n3 0.371327549 0.62867245 0.01587302  24.106816\r\n4 0.332625452 0.62831620 0.01562500  16.584364\r\n5 0.009710558 0.08252591 0.01538462   3.132042\r\n\r\nThis evaluation is more difficult as the range is made up of much less clearly defined scores.\r\nNetwork Constraint (Burt)\r\nConstraint is a measure of the redundancy of a node’s connections. It is bound between 0 and 1, with 0 being a complete lack, and 1 being complete redundancy.\r\n\r\n\r\nShow code\r\n\r\nconstraint(gd_network_ig)\r\n\r\n\r\n  Eric Andersen     John Barlow     Bob Bralove  Andrew Charles \r\n      1.0000000       0.6706222       0.4989170       1.0000000 \r\n    John Dawson    Willie Dixon    Jerry Garcia  Donna Godchaux \r\n      1.2945238       0.7040590       0.5061908       0.4514219 \r\n Keith Godchaux   Gerrit Graham     Frank Guida     Mickey Hart \r\n      0.5143887       1.0000000       0.8224000       0.5294014 \r\n  Bruce Hornsby   Robert Hunter Bill Kreutzmann       Ned Lagin \r\n      0.0000000       0.6332636       0.5159787       1.0000000 \r\n      Phil Lesh      Peter Monk   Brent Mydland     Dave Parker \r\n      0.4521996       1.0000000       0.9325133       0.5591083 \r\nRobert Petersen          Pigpen     Joe Royster   Rob Wasserman \r\n      0.7134697       0.5404552       0.8224000       0.4756234 \r\n       Bob Weir   Vince Welnick \r\n      0.3367355       0.5216319 \r\n\r\nFinally, I’m going to save all of this data into a .csv file for future analysis.\r\n\r\n\r\nShow code\r\n\r\ncentrality_gd <-data.frame(id=1:vcount(gd_network_ig),\r\n                        name=V(gd_network_ig)$name,\r\n                        degree_all=igraph::degree(gd_network_ig),\r\n                        degree_norm=igraph::degree(gd_network_ig,normalized=T),\r\n                        BC_power=power_centrality(gd_network_ig),\r\n                        EV_cent=centr_eigen(gd_network_ig,directed = F)$vector,\r\n                        reflect_EV=gd_reflective*centr_eigen(gd_network_ig,directed = F)$vector,\r\n                        derive_EV=gd_derived*centr_eigen(gd_network_ig,directed = F)$vector,\r\n                        close=closeness(gd_network_ig),\r\n                        between=betweenness(gd_network_ig, directed=FALSE),\r\n                        burt=constraint(gd_network_ig))\r\n\r\nwrite.csv(centrality_gd, file = \"centrality_df.csv\")\r\n\r\n\r\n\r\nCitations:\r\nAllan, Alex; Grateful Dead Lyric & Song Finder: https://whitegum.com/~acsa/intro.htm\r\nASCAP. 18 March 2022.\r\nDodd, David; The Annotated Grateful Dead Lyrics: http://artsites.ucsc.edu/gdead/agdl/\r\nSchofield, Matt; The Grateful Dead Family Discography: http://www.deaddisc.com/\r\nThis information is intended for private research only, and not for any commercial use. Original Grateful Dead songs are ©copyright Ice Nine Music\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-12T00:51:31-05:00",
    "input_file": "dacss-697e-assignment-5.knit.md"
  },
  {
    "path": "posts/2022-04-04-dacss-697e-assignment-4/",
    "title": "DACSS 697E Assignment 4",
    "description": "Assignment 4 for DACSS 697E course 'Social and Political Network Analysis': \"Status & Eigenvector Centrality\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-03-25",
    "categories": [
      "networks",
      "homework",
      "grateful network"
    ],
    "contents": "\r\n\r\nContents\r\nNetwork Details\r\nAffiliation Matrix\r\nBipartite Projection\r\nNetwork Creation\r\nDyad & Triad Census\r\nTransitivity\r\nGeodesic Distance\r\nComponents\r\nDensity\r\n\r\nDegree Centrality\r\nIgraph\r\nStatnet\r\nSummary Statistics\r\nStatnet v. Igraph Degree Treatment\r\nOverall Eigenvector Score\r\nBonacich Power\r\nGraphing Centrality Scores\r\n\r\n\r\n\r\nNetwork Details\r\nI am continuing to use the Grateful Dead song writing data set that I am using in this series of posts to examine co-writing links and centrality.\r\nThe data set consists of the links between co-writers of songs played by the Grateful Dead over their 30-year touring career that I compiled.\r\nThere are 26 songwriters that contributed to the songs played over the course of the Grateful Dead history, resulting in 26 nodes in the dataset.\r\nThere are a total of 183 (updated and still under review!) unique songs played, and the varies combinations of co-writing combinations are now represented in a binary affiliation matrix.\r\nI have not weighted this version of the data; I am trying to build it from a binary affiliation matrix first, and hope to later add the number of times a given song was played live as a weight.\r\n\r\n\r\n\r\nAffiliation Matrix\r\nLoading the dataset and creating the network to begin this assignment:\r\n\r\n\r\nShow code\r\n\r\ngd_vertices <- read.csv(\"gd_nodes.csv\", header=T, stringsAsFactors=F)\r\ngd_affiliation <- read.csv(\"gd_affiliation_matrix.csv\", row.names = 1, header = TRUE, check.names = FALSE)\r\ngd_matrix <- as.matrix(gd_affiliation)\r\n\r\n\r\n\r\nInspecting the first 8 columns of the data structure in the affiliation matrix format:\r\n\r\n\r\nShow code\r\n\r\ndim(gd_matrix)\r\n\r\n\r\n[1]  26 183\r\n\r\nShow code\r\n\r\ngd_matrix[1:10, 1:4]\r\n\r\n\r\n               Alabama Getaway Alice D Millionaire Alligator Althea\r\nEric Andersen                0                   0         0      0\r\nJohn Barlow                  0                   0         0      0\r\nBob Bralove                  0                   0         0      0\r\nAndrew Charles               0                   0         0      0\r\nJohn Dawson                  0                   0         0      0\r\nWillie Dixon                 0                   0         0      0\r\nJerry Garcia                 1                   1         0      1\r\nDonna Godchaux               0                   0         0      0\r\nKeith Godchaux               0                   0         0      0\r\nGerrit Graham                0                   0         0      0\r\n\r\nBipartite Projection\r\nNow I can create the single mode network and examine the bipartite projection. After converting the matrix to a square adjacency matrix, I can look at the full matrix.\r\nI can also call the adjacency matrix count for co-writing incidences between certain songwriters, such as between writing partners Jerry Garcia and Robert Hunter and between John Barlow and Bob Weir.\r\n\r\n\r\nShow code\r\n\r\ngd_projection <- gd_matrix%*%t(gd_matrix)\r\ndim(gd_projection)\r\n\r\n\r\n[1] 26 26\r\n\r\nShow code\r\n\r\ngd_projection[1:10, 1:4]\r\n\r\n\r\n               Eric Andersen John Barlow Bob Bralove Andrew Charles\r\nEric Andersen              1           0           0              0\r\nJohn Barlow                0          26           1              0\r\nBob Bralove                0           1           3              0\r\nAndrew Charles             0           0           0              1\r\nJohn Dawson                0           0           0              0\r\nWillie Dixon               0           0           0              0\r\nJerry Garcia               0           0           0              0\r\nDonna Godchaux             0           0           0              0\r\nKeith Godchaux             0           0           0              0\r\nGerrit Graham              0           0           0              0\r\n\r\nShow code\r\n\r\ngd_projection[\"Jerry Garcia\", \"Robert Hunter\"]\r\n\r\n\r\n[1] 78\r\n\r\nShow code\r\n\r\ngd_projection[\"John Barlow\", \"Bob Weir\"]\r\n\r\n\r\n[1] 21\r\n\r\nNetwork Creation\r\nNow I will use this adjacency matrix to create both igraph and statnet network objects and take a look at their resulting features. This is a non-directed, unweighted dataset.\r\n\r\n\r\nShow code\r\n\r\n#Create Igraph and Statnet Objects\r\n\r\ngd_network_ig <- graph.adjacency(gd_projection,mode=\"undirected\") #igraph object\r\ngd_network_stat <- network(gd_projection, directed=F, matrix.type=\"adjacency\") #statnet object\r\n\r\n#Inspect New Objects\r\nprint(gd_network_stat)\r\n\r\n\r\n Network attributes:\r\n  vertices = 26 \r\n  directed = FALSE \r\n  hyper = FALSE \r\n  loops = FALSE \r\n  multiple = FALSE \r\n  bipartite = FALSE \r\n  total edges= 65 \r\n    missing edges= 0 \r\n    non-missing edges= 65 \r\n\r\n Vertex attribute names: \r\n    vertex.names \r\n\r\nNo edge attributes\r\n\r\nShow code\r\n\r\nigraph::vertex_attr_names(gd_network_ig)\r\n\r\n\r\n[1] \"name\"\r\n\r\nShow code\r\n\r\nigraph::edge_attr_names(gd_network_ig)\r\n\r\n\r\ncharacter(0)\r\n\r\nShow code\r\n\r\nhead(V(gd_network_ig)$name)\r\n\r\n\r\n[1] \"Eric Andersen\"  \"John Barlow\"    \"Bob Bralove\"   \r\n[4] \"Andrew Charles\" \"John Dawson\"    \"Willie Dixon\"  \r\n\r\nShow code\r\n\r\nis_directed(gd_network_ig)\r\n\r\n\r\n[1] FALSE\r\n\r\nShow code\r\n\r\nis_weighted(gd_network_ig)\r\n\r\n\r\n[1] FALSE\r\n\r\nShow code\r\n\r\nis_bipartite(gd_network_ig)\r\n\r\n\r\n[1] FALSE\r\n\r\nDyad & Triad Census\r\nLooking at the dyad/triad census info in igraph and statnet:\r\n\r\n\r\nShow code\r\n\r\nigraph::dyad.census(gd_network_ig)\r\n\r\n\r\n$mut\r\n[1] 738\r\n\r\n$asym\r\n[1] 0\r\n\r\n$null\r\n[1] -413\r\n\r\nShow code\r\n\r\nigraph::triad.census(gd_network_ig)\r\n\r\n\r\n [1] 1788    0  488    0    0    0    0    0    0    0  237    0    0\r\n[14]    0    0   87\r\n\r\nShow code\r\n\r\nsna::dyad.census(gd_network_stat)\r\n\r\n\r\n     Mut Asym Null\r\n[1,]  65    0  260\r\n\r\nShow code\r\n\r\nsna::triad.census(gd_network_stat)\r\n\r\n\r\n      003 012 102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U\r\n[1,] 1451   0 825    0    0    0    0    0    0    0 237    0    0\r\n     120C 210 300\r\n[1,]    0   0  87\r\n\r\nKnowing this network has 26 vertices, I want to see if the triad census is working correctly by comparing the following data, which I can confirm it is here!\r\n\r\n\r\nShow code\r\n\r\n#possible triads in network\r\n26*25*24/6\r\n\r\n\r\n[1] 2600\r\n\r\nShow code\r\n\r\nsum(igraph::triad.census(gd_network_ig))\r\n\r\n\r\n[1] 2600\r\n\r\nTransitivity\r\nLooking next at the global v. average local transitivity of the network in igraph and confirming global transitivity in statnet and igraph (Statnet and igraph network transitivity = 0.5241, igraph local transitivity = 0.7756)\r\n\r\n\r\nShow code\r\n\r\n#network transitivity: statnet\r\ngtrans(gd_network_stat)\r\n\r\n\r\n[1] 0.5240964\r\n\r\nShow code\r\n\r\n#global clustering cofficient: igraph\r\ntransitivity(gd_network_ig, type=\"global\")\r\n\r\n\r\n[1] 0.5240964\r\n\r\nShow code\r\n\r\n#average local clustering coefficient: igraph\r\ntransitivity(gd_network_ig, type=\"average\")\r\n\r\n\r\n[1] 0.7755587\r\n\r\nThese transitivity results tells me that the average local network transitivity is significantly higher than the global transitivity, indicating, again from my still naive network knowledge, that the overall network is generally more loose, and that there is a more connected sub-network.\r\nGeodesic Distance\r\nLooking at the geodesic distance tells me that on average, I can confirm that the path length is just over 2, so on average, each node is two “stops” from each other on the geodesic path.\r\n\r\n\r\nShow code\r\n\r\naverage.path.length(gd_network_ig,directed=F)\r\n\r\n\r\n[1] 2.01\r\n\r\nComponents\r\nGetting a look at the components of the network comfirms that there are 2 components in the network, and 25 of the 26 nodes make up the giant component with 1 isolate.\r\n\r\n\r\nShow code\r\n\r\nnames(igraph::components(gd_network_ig))\r\n\r\n\r\n[1] \"membership\" \"csize\"      \"no\"        \r\n\r\nShow code\r\n\r\nigraph::components(gd_network_ig)$no \r\n\r\n\r\n[1] 2\r\n\r\nShow code\r\n\r\nigraph::components(gd_network_ig)$csize\r\n\r\n\r\n[1] 25  1\r\n\r\nDensity\r\nThe network density measure: First with just the call “graph.density” and then with adding “loops=TRUE”. In igraph, I know that its’ default output assumes that loops are not included but does not remove them, which wwe had corrected with the addition of “loops=TRUE” per the course tutorials when comparing output to statnet. In this case, the statnet output is far different, so I am not sure what is happening with this aspect of the network.\r\n\r\n\r\nShow code\r\n\r\ngraph.density(gd_network_ig, loops=TRUE)\r\n\r\n\r\n[1] 2.102564\r\n\r\nShow code\r\n\r\nnetwork.density(gd_network_stat)\r\n\r\n\r\n[1] 0.2\r\n\r\nDegree Centrality\r\nThe network degree measure: This gives me a clear output showing the degree of each particular node (songwriter). It is not surprising, knowing my subject matter, that Jerry Garcia is the highest degree node in this network as the practical and figurative head of the band. The other band members’ degree measures are not necessarily what I expected, though. I did not anticipate that his songwriting partner, Robert Hunter, would have a lower degree than band members Phil Lesh and Bob Weir. Further, I did not anticipate that the degree measure of band member ‘Pigpen’ would be so high given his early death in the first years of the band’s touring life.\r\n\r\n\r\nShow code\r\n\r\nigraph::degree(gd_network_ig)\r\n\r\n\r\n  Eric Andersen     John Barlow     Bob Bralove  Andrew Charles \r\n              3              81              14               3 \r\n    John Dawson    Willie Dixon    Jerry Garcia  Donna Godchaux \r\n              4               4             328              12 \r\n Keith Godchaux   Gerrit Graham     Frank Guida     Mickey Hart \r\n             16               3               4              36 \r\n  Bruce Hornsby   Robert Hunter Bill Kreutzmann       Ned Lagin \r\n              4             313             100               3 \r\n      Phil Lesh      Peter Monk   Brent Mydland     Dave Parker \r\n            149               3              41               7 \r\nRobert Petersen          Pigpen     Joe Royster   Rob Wasserman \r\n             13              95               4              10 \r\n       Bob Weir   Vince Welnick \r\n            213              13 \r\n\r\nShow code\r\n\r\nsna::degree(gd_network_stat)\r\n\r\n\r\n [1]  2  6 10  2  4  4 20 12 14  2  4 14  0 22 18  2 28  2  8 10  4 16\r\n[23]  4 10 34  8\r\n\r\nTo look further I will create a dataframe in igraph first, then statnet.\r\nIgraph\r\n\r\n\r\nShow code\r\n\r\nig_nodes<-data.frame(name=V(gd_network_ig)$name, degree=igraph::degree(gd_network_ig))\r\nhead(ig_nodes)\r\n\r\n\r\n                         name degree\r\nEric Andersen   Eric Andersen      3\r\nJohn Barlow       John Barlow     81\r\nBob Bralove       Bob Bralove     14\r\nAndrew Charles Andrew Charles      3\r\nJohn Dawson       John Dawson      4\r\nWillie Dixon     Willie Dixon      4\r\n\r\nStatnet\r\n\r\n\r\nShow code\r\n\r\nstat_nodes<-data.frame(name=gd_network_stat%v%\"vertex.names\", degree=sna::degree(gd_network_stat))\r\nhead(stat_nodes)\r\n\r\n\r\n            name degree\r\n1  Eric Andersen      2\r\n2    John Barlow      6\r\n3    Bob Bralove     10\r\n4 Andrew Charles      2\r\n5    John Dawson      4\r\n6   Willie Dixon      4\r\n\r\nThe igraph and statnet dataframes give very different results.\r\nSummary Statistics\r\nA quick look at the summary statistics confirms for me the minimum, maximum, median, and mean node degree data using each package.\r\n\r\n\r\nShow code\r\n\r\nsummary(ig_nodes)\r\n\r\n\r\n     name               degree      \r\n Length:26          Min.   :  3.00  \r\n Class :character   1st Qu.:  4.00  \r\n Mode  :character   Median : 12.50  \r\n                    Mean   : 56.77  \r\n                    3rd Qu.: 71.00  \r\n                    Max.   :328.00  \r\n\r\nShow code\r\n\r\nsummary(stat_nodes)\r\n\r\n\r\n     name               degree  \r\n Length:26          Min.   : 0  \r\n Class :character   1st Qu.: 4  \r\n Mode  :character   Median : 8  \r\n                    Mean   :10  \r\n                    3rd Qu.:14  \r\n                    Max.   :34  \r\n\r\nStatnet v. Igraph Degree Treatment\r\nI’m taking a look at the dataframe of the degree nodes, though since it is not a directed network the in and out degrees are not measured or relevant to our network. But it is still interesting to look at how igraph and statnet handle these datasets differently.\r\nStatnet\r\n\r\n\r\nShow code\r\n\r\n#create a dataframe of the total, in and out-degree of nodes in the stat network\r\ngd_stat_nodes <- data.frame(name=gd_network_stat%v%\"vertex.names\",\r\n    totdegree=sna::degree(gd_network_stat),\r\n    indegree=sna::degree(gd_network_stat, cmode=\"indegree\"),\r\n    outdegree=sna::degree(gd_network_stat, cmode=\"outdegree\"))\r\n\r\n#sort the top total degree of nodes in the stat network\r\narrange(gd_stat_nodes, desc(totdegree))%>%slice(1:5)\r\n\r\n\r\n             name totdegree indegree outdegree\r\n1        Bob Weir        34       17        17\r\n2       Phil Lesh        28       14        14\r\n3   Robert Hunter        22       11        11\r\n4    Jerry Garcia        20       10        10\r\n5 Bill Kreutzmann        18        9         9\r\n\r\nIgraph\r\n\r\n\r\nShow code\r\n\r\n#create a dataframe of the total, in and out-degree of nodes in the igraph network\r\ngd_ig_nodes<-data.frame(name=V(gd_network_ig)$name, \r\n                     degree=igraph::degree(gd_network_ig), mode=\"tot\",\r\n                     degree=igraph::degree(gd_network_ig), mode=\"in\",\r\n                     degree=igraph::degree(gd_network_ig), mode=\"out\")\r\n\r\n#sort the top total degree of nodes in the igraph network\r\narrange(gd_ig_nodes, desc(degree))%>%slice(1:5)\r\n\r\n\r\n                           name degree mode degree.1 mode.1 degree.2\r\nJerry Garcia       Jerry Garcia    328  tot      328     in      328\r\nRobert Hunter     Robert Hunter    313  tot      313     in      313\r\nBob Weir               Bob Weir    213  tot      213     in      213\r\nPhil Lesh             Phil Lesh    149  tot      149     in      149\r\nBill Kreutzmann Bill Kreutzmann    100  tot      100     in      100\r\n                mode.2\r\nJerry Garcia       out\r\nRobert Hunter      out\r\nBob Weir           out\r\nPhil Lesh          out\r\nBill Kreutzmann    out\r\n\r\nOverall Eigenvector Score\r\nThe Eigenvector centrality score for each node can be accessed by calling “vector”, and I can examine the top eigenvector scores in the igraph network:\r\n\r\n\r\n#Eigenvector centrality, top 10 in igraph network\r\n\r\neigen_ig <- eigen_centrality(gd_network_ig)\r\neigen_gd_ig <- data.frame(eigen_ig)\r\narrange(eigen_gd_ig[1], desc(vector))%>%slice(1:10)\r\n\r\n\r\n                    vector\r\nRobert Hunter   1.00000000\r\nJerry Garcia    0.96094165\r\nBob Weir        0.18725953\r\nPhil Lesh       0.15133380\r\nBill Kreutzmann 0.09223647\r\nPigpen          0.07985305\r\nMickey Hart     0.02523896\r\nJohn Barlow     0.01773746\r\nKeith Godchaux  0.01382256\r\nVince Welnick   0.01192303\r\n\r\nBonacich Power\r\nThe Bonacich power centrality score for each node can be accessed first just using defaults, including setting the index to “1”; then, I can “rescale” so that all of the scores sum “1”.\r\nTo display my results, I have to run the calculations and save the results as a dataframe to recall, since the command “bonpow()” is the same in igraph and statnet, which is causing trouble in running then knitting this file.\r\nI need to understand more nuance to the Bonacich power measure in order to fully understand what these two measures say about my specific network.\r\n\r\n\r\nShow code\r\n\r\n#Compute Bonpow scores\r\n\r\n#bp_ig1 <- bonpow(gd_network_ig) #with a default index of \"1\"\r\n#bonpow_gd_ig1 <- data.frame(bp_ig1)\r\n#write.csv(bonpow_gd_ig1, file = \"bonpow_gd_ig1.csv\")\r\n\r\n#Rescaled so that they sum to \"1\"\r\n\r\n#bp_ig2 <- bonpow(gd_network_ig, rescale = TRUE) #with a default index of \"1\"\r\n#bonpow_gd_ig2 <- data.frame(bp_ig2)\r\n#write.csv(bonpow_gd_ig2, file = \"bonpow_gd_ig2.csv\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n#Read in dataframe from previous chunk\r\n\r\nbon1 <- read.csv(\"bonpow_gd_ig1.csv\")\r\nbon2 <- read.csv(\"bonpow_gd_ig2.csv\")\r\n\r\ntotalbonpow <- merge(bon1,bon2)\r\n\r\ntotalbonpow\r\n\r\n\r\n                 X      bp_ig1      bp_ig2\r\n1   Andrew Charles  0.08220268  0.01522717\r\n2  Bill Kreutzmann -0.70115475 -0.12988143\r\n3      Bob Bralove -0.22064550 -0.04087222\r\n4         Bob Weir -0.54308358 -0.10060043\r\n5    Brent Mydland  0.52651322  0.09753095\r\n6    Bruce Hornsby  0.00000000  0.00000000\r\n7      Dave Parker -0.89144078 -0.16512988\r\n8   Donna Godchaux  1.23038839  0.22791631\r\n9    Eric Andersen -0.28021530 -0.05190689\r\n10     Frank Guida  3.07056607  0.56878957\r\n11   Gerrit Graham -0.28021530 -0.05190689\r\n12    Jerry Garcia -0.25514168 -0.04726227\r\n13     Joe Royster  3.07056607  0.56878957\r\n14     John Barlow -0.31662818 -0.05865199\r\n15     John Dawson  0.09708065  0.01798315\r\n16  Keith Godchaux  1.17992241  0.21856802\r\n17     Mickey Hart  0.15330194  0.02839755\r\n18       Ned Lagin  0.08220268  0.01522717\r\n19      Peter Monk  0.08220268  0.01522717\r\n20       Phil Lesh -0.18066559 -0.03346637\r\n21          Pigpen -0.52573655 -0.09738708\r\n22   Rob Wasserman -0.41469644 -0.07681809\r\n23   Robert Hunter -0.17351422 -0.03214166\r\n24 Robert Petersen  1.11819222  0.20713317\r\n25   Vince Welnick -0.07953575 -0.01473315\r\n26    Willie Dixon -0.43204347 -0.08003144\r\n\r\nCreating a data frame summarizing all of this information and doing basic visualization on a couple of them:\r\n\r\n\r\nShow code\r\n\r\ngd_adjacency <- as.matrix(as_adjacency_matrix(gd_network_ig))\r\ngd_adjacency_2 <- gd_adjacency %*% gd_adjacency\r\n\r\n#calculate portion of reflected centrality\r\ngd_reflective <- diag(as.matrix(gd_adjacency_2))/rowSums(as.matrix(gd_adjacency_2))\r\ngd_reflective <- ifelse(is.nan(gd_reflective),0,gd_reflective)\r\n\r\n#calculate derived centrality\r\ngd_derived <- 1-diag(as.matrix(gd_adjacency_2))/rowSums(as.matrix(gd_adjacency_2))\r\ngd_derived <- ifelse(is.nan(gd_derived),1,gd_derived)\r\n#create data frame of centrality measures\r\ncentrality_gd <-data.frame(id=1:vcount(gd_network_ig),\r\n                        name=V(gd_network_ig)$name,\r\n                        degree_all=igraph::degree(gd_network_ig),\r\n                        BC_power=power_centrality(gd_network_ig),\r\n                        degree_norm=igraph::degree(gd_network_ig,normalized=T),\r\n                        EV_cent=centr_eigen(gd_network_ig,directed = F)$vector,\r\n                        reflect_EV=gd_reflective*centr_eigen(gd_network_ig,directed = F)$vector,\r\n                        derive_EV=gd_derived*centr_eigen(gd_network_ig,directed = F)$vector)\r\n\r\nrow.names(centrality_gd)<-NULL\r\ncentrality_gd\r\n\r\n\r\n   id            name degree_all    BC_power degree_norm      EV_cent\r\n1   1   Eric Andersen          3 -0.28021530        0.12 6.852805e-04\r\n2   2     John Barlow         81 -0.31662818        3.24 1.773746e-02\r\n3   3     Bob Bralove         14 -0.22064550        0.56 8.992246e-03\r\n4   4  Andrew Charles          3  0.08220268        0.12 5.538095e-04\r\n5   5     John Dawson          4  0.09708065        0.16 7.176110e-03\r\n6   6    Willie Dixon          4 -0.43204347        0.16 7.041156e-04\r\n7   7    Jerry Garcia        328 -0.25514168       13.12 9.609417e-01\r\n8   8  Donna Godchaux         12  1.23038839        0.48 5.313952e-03\r\n9   9  Keith Godchaux         16  1.17992241        0.64 1.382256e-02\r\n10 10   Gerrit Graham          3 -0.28021530        0.12 6.852805e-04\r\n11 11     Frank Guida          4  3.07056607        0.16 2.932974e-04\r\n12 12     Mickey Hart         36  0.15330194        1.44 2.523896e-02\r\n13 13   Bruce Hornsby          4  0.00000000        0.16 2.574501e-17\r\n14 14   Robert Hunter        313 -0.17351422       12.52 1.000000e+00\r\n15 15 Bill Kreutzmann        100 -0.70115475        4.00 9.223647e-02\r\n16 16       Ned Lagin          3  0.08220268        0.12 5.538095e-04\r\n17 17       Phil Lesh        149 -0.18066559        5.96 1.513338e-01\r\n18 18      Peter Monk          3  0.08220268        0.12 5.538095e-04\r\n19 19   Brent Mydland         41  0.52651322        1.64 2.659589e-03\r\n20 20     Dave Parker          7 -0.89144078        0.28 5.385443e-03\r\n21 21 Robert Petersen         13  1.11819222        0.52 2.274921e-03\r\n22 22          Pigpen         95 -0.52573655        3.80 7.985305e-02\r\n23 23     Joe Royster          4  3.07056607        0.16 2.932974e-04\r\n24 24   Rob Wasserman         10 -0.41469644        0.40 5.146870e-03\r\n25 25        Bob Weir        213 -0.54308358        8.52 1.872595e-01\r\n26 26   Vince Welnick         13 -0.07953575        0.52 1.192303e-02\r\n     reflect_EV    derive_EV\r\n1  8.512801e-06 0.0006767677\r\n2  4.171627e-03 0.0135658315\r\n3  2.393769e-04 0.0087528693\r\n4  9.466828e-06 0.0005443426\r\n5  4.752391e-05 0.0071285863\r\n6  1.242557e-05 0.0006916900\r\n7  3.326255e-01 0.6283162014\r\n8  1.213231e-04 0.0051926286\r\n9  2.487863e-04 0.0135737779\r\n10 8.512801e-06 0.0006767677\r\n11 1.113787e-05 0.0002821595\r\n12 1.390973e-03 0.0238479844\r\n13 1.593739e-17 0.0000000000\r\n14 3.713275e-01 0.6286724511\r\n15 9.710558e-03 0.0825259133\r\n16 9.466828e-06 0.0005443426\r\n17 2.214058e-02 0.1291932241\r\n18 9.466828e-06 0.0005443426\r\n19 6.119022e-04 0.0020476869\r\n20 4.829994e-05 0.0053371431\r\n21 1.438169e-04 0.0021311045\r\n22 9.031643e-03 0.0708214079\r\n23 1.113787e-05 0.0002821595\r\n24 1.077879e-04 0.0050390825\r\n25 4.070942e-02 0.1465501114\r\n26 3.311952e-04 0.0115918330\r\n\r\nGraphing Centrality Scores\r\n\r\n\r\nShow code\r\n\r\nattach(centrality_gd)\r\nbreaks<-round(vcount(gd_network_ig))\r\nhist(degree_all,breaks=breaks,\r\n     main=paste(\"Distribution of Total Degree Scores in GD Songwriters \",sep=\"\"),\r\n     xlab=\"Total Degree Score\")\r\n\r\n\r\n\r\nShow code\r\n\r\nhist(EV_cent,breaks=breaks,\r\n     main=paste(\"Distribution of Eigenvector Centrality Scores in GD Songwriters \",sep=\"\"),\r\n    xlab=\"Eigenvector Centrality Score\")\r\n\r\n\r\n\r\nShow code\r\n\r\nhist(BC_power,breaks=breaks,\r\n     main=paste(\"Distribution of Bonacich Power Scores in GD Songwriters\",sep=\"\"),\r\n     xlab=\"Bonacich Power Score\")\r\n\r\n\r\n\r\n\r\nI can independently look at the correlations between all scores now. Using prompts from this week’s tutorial, it looks that all of the variables except Bonacich power are strongly correlated, so I think I’ll want to begin subsetting my network to get more meaningful interpretations.\r\n\r\n\r\nShow code\r\n\r\nnames(centrality_gd) #Find the columns we want to run the correlation on\r\n\r\n\r\n[1] \"id\"          \"name\"        \"degree_all\"  \"BC_power\"   \r\n[5] \"degree_norm\" \"EV_cent\"     \"reflect_EV\"  \"derive_EV\"  \r\n\r\nShow code\r\n\r\ncols<-c(3:8) #All except the id and name in this instance\r\ncorMat<-cor(centrality_gd[,cols],use=\"complete.obs\") #Specify those in the bracket\r\ncorMat #Let's look at it, which variables are most strongly correlated?\r\n\r\n\r\n            degree_all   BC_power degree_norm    EV_cent reflect_EV\r\ndegree_all   1.0000000 -0.2782755   1.0000000  0.9131592  0.8729045\r\nBC_power    -0.2782755  1.0000000  -0.2782755 -0.1782509 -0.1481903\r\ndegree_norm  1.0000000 -0.2782755   1.0000000  0.9131592  0.8729045\r\nEV_cent      0.9131592 -0.1782509   0.9131592  1.0000000  0.9946549\r\nreflect_EV   0.8729045 -0.1481903   0.8729045  0.9946549  1.0000000\r\nderive_EV    0.9314936 -0.1943027   0.9314936  0.9983162  0.9869907\r\n             derive_EV\r\ndegree_all   0.9314936\r\nBC_power    -0.1943027\r\ndegree_norm  0.9314936\r\nEV_cent      0.9983162\r\nreflect_EV   0.9869907\r\nderive_EV    1.0000000\r\n\r\nHowever, I will also make a pretty visualization of the correlation matrix, just because.\r\n\r\n\r\nlibrary(corrplot)\r\ncorrplot(corMat)\r\n\r\n\r\n\r\n\r\nCitations:\r\nAllan, Alex; Grateful Dead Lyric & Song Finder: https://whitegum.com/~acsa/intro.htm\r\nASCAP. 18 March 2022.\r\nDodd, David; The Annotated Grateful Dead Lyrics: http://artsites.ucsc.edu/gdead/agdl/\r\nSchofield, Matt; The Grateful Dead Family Discography: http://www.deaddisc.com/\r\nPhoto by Grateful Dead Productions\r\nThis information is intended for private research only, and not for any commercial use. Original Grateful Dead songs are ©copyright Ice Nine Music\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-04-dacss-697e-assignment-4/distill-preview.png",
    "last_modified": "2022-04-12T00:27:31-05:00",
    "input_file": "dacss-697e-assignment-4.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-dacss-697e-assignment-3/",
    "title": "DACSS 697E Assignment 3",
    "description": "Assignment 3 for DACSS 697E course 'Social and Political Network Analysis': \"Grateful Research: Creating a Network\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-03-18",
    "categories": [
      "networks",
      "homework",
      "grateful network"
    ],
    "contents": "\r\n\r\nContents\r\nNetwork Creation\r\nPurpose\r\nFirst Try - A Miss\r\nRegouping\r\nNetwork Creation\r\n\r\nNetwork Details\r\nVisualizing the Network\r\nDyad and Triad Census\r\nTransitivity\r\nGeodesic Distance\r\nComponents\r\nDensity\r\nDegree\r\nSummary Statistics\r\nPlotting the Network\r\nBetter Plotting\r\nAdding More Detail\r\n\r\n\r\n\r\nNetwork Creation\r\nPurpose\r\nFor my final project, I am using a data set that is somewhat similar in structure to that of my final project to try and get a feel for the process of creating the appropriate network. After recovering data from thousands of New York Times articles pulled through their API on Afghanistan from a 2-year period, I will be analyzing the network of article authorship and themes of articles. To understand the process, I am using for my assignment a data set of co-writers of songs played by the Grateful Dead over their 30-year touring career that I compiled. While compiling the data, I added an attribute that represents the connections between co-writers as songs with the added observation of the number of times each song was played live. The nature of the band was that of a collaborative subculture where the energy of the live shows reflected the crowd’s buy-in to the songs being played. Since the band was primarily one whose popularity was measured by ticket sales, not album sales. I’m still not sure if that will serve appropriately as a ‘weight’ for the network data, so I need to explore the process more thoroughly.\r\n\r\n\r\n\r\nFirst Try - A Miss\r\nUnderstanding this as an affiliation network, I first created a matrix linking actors (songwriters) to an event (songs). I began by assigning a unique ID to each actor and event. Taking the data I have pulled from my research, I created an affiliation spreadsheet with the songwriters as rows and the songs as columns. When a songwriter was affiliated with a song, there was a number in that matrix spot. However, I struggled to get this affiliation data into a network in R because I was using weights incorrectly, so I took a different approach.\r\nRegouping\r\nIn this example, I used a node list where unique IDs are numbers which correspond to the name of a songwriter.\r\nThe edgelist is in a separate spreadsheet where the first two columns are the IDs of the source and the target node (songwriter ID), regardless of whether the network is directed, for each edge. Each row contains an observation of a connection between writers for a given song, and since there are multiple collaborations, there may be multiple rows of writer combinations for a given song ID. If there was only one writer on a song, that songwriter’s ID is indicated in both the source and target column for that song.\r\nThe following columns are edge attributes. In my edgelist, I have the two songwriters representing the co-writing relationship in columns “1” and “2”, the song ID in column “3”, the song name in column “4”, and the number of times the corresponding song was played live is indicated in column “5”.\r\nI have NOT utilized the number of times the song was played live as a network weight at this point. Additionally, this edgelist format is not the ideal format, but it is the first step in the process I am working through to utilize different methods of working through the data.\r\nNetwork Creation\r\n\r\n\r\n# Loading nodes and vertices\r\ngd_vertices <- read.csv(\"gd_nodes.csv\")\r\ngd_edgelist <- read.csv(\"gd_clean_data.csv\")\r\n\r\n\r\n\r\nConverting network data into igraph objects using the “graph.data.frame: function, which takes two data frames: d and vertices.\r\n“d” describes the edges of the network and “vertices” the nodes.\r\n\r\n\r\nset.seed(1234)\r\ngrateful_data <- graph_from_data_frame(d = gd_edgelist, vertices = gd_vertices, directed = FALSE)\r\n\r\n\r\n\r\nNetwork Details\r\nNow to check the vertices and edges in the graph I’ve created to ensure they represent the data accurately, and confirm that all of the attributes have been represented properly:\r\n\r\n\r\nhead(V(grateful_data)$name)\r\n\r\n\r\n[1] \"Eric Andersen\"  \"John Barlow\"    \"Bob Bralove\"   \r\n[4] \"Andrew Charles\" \"John Dawson\"    \"Willie Dixon\"  \r\n\r\nhead(E(grateful_data)$song.id)\r\n\r\n\r\n[1] 1 2 2 2 2 2\r\n\r\nhead(E(grateful_data)$song.name)\r\n\r\n\r\n[1] \"Alabama Getaway\"     \"Alice D Millionaire\" \"Alice D Millionaire\"\r\n[4] \"Alice D Millionaire\" \"Alice D Millionaire\" \"Alice D Millionaire\"\r\n\r\nhead(E(grateful_data)$weight)\r\n\r\n\r\nNULL\r\n\r\nis_directed(grateful_data)\r\n\r\n\r\n[1] FALSE\r\n\r\nis_weighted(grateful_data)\r\n\r\n\r\n[1] FALSE\r\n\r\nis_bipartite(grateful_data)\r\n\r\n\r\n[1] FALSE\r\n\r\nigraph::vertex_attr_names(grateful_data)\r\n\r\n\r\n[1] \"name\"\r\n\r\nigraph::edge_attr_names(grateful_data)\r\n\r\n\r\n[1] \"song.id\"      \"song.name\"    \"times.played\"\r\n\r\nVisualizing the Network\r\n\r\n\r\nShow code\r\n\r\nplot(grateful_data)\r\n\r\n\r\n\r\n\r\nIt’s basically plotting what I want it to illustrate, though I will need to do a lot more work to make the graph represent anything meaningful!\r\nDyad and Triad Census\r\nFinishing the look at the basic network information such as the dyad and triad census: I have 558 mutual dyads and null value of “-233”, with a warning that calling a dyad census on an undirected graph. This does indicate to me that the edgelist format is not the best representation of this data.\r\n\r\n\r\nShow code\r\n\r\nigraph::dyad.census(grateful_data)\r\n\r\n\r\n$mut\r\n[1] 558\r\n\r\n$asym\r\n[1] 0\r\n\r\n$null\r\n[1] -233\r\n\r\nShow code\r\n\r\nigraph::triad.census(grateful_data)\r\n\r\n\r\n [1] 2043    0  233    0    0    0    0    0    0    0  237    0    0\r\n[14]    0    0   87\r\n\r\nKnowing this network has 26 vertices, I want to see if the triad census is working correctly by comparing the following data, which I can confirm using this calculation.\r\n\r\n\r\nShow code\r\n\r\n#possible triads in network\r\n26*25*24/6\r\n\r\n\r\n[1] 2600\r\n\r\nShow code\r\n\r\nsum(igraph::triad.census(grateful_data))\r\n\r\n\r\n[1] 2600\r\n\r\nTransitivity\r\nLooking next at the global v. average local transitivity of the network:\r\n\r\n\r\nShow code\r\n\r\n#get global clustering cofficient: igraph\r\ntransitivity(grateful_data, type=\"global\")\r\n\r\n\r\n[1] 0.5240964\r\n\r\nShow code\r\n\r\n#get average local clustering coefficient: igraph\r\ntransitivity(grateful_data, type=\"average\")\r\n\r\n\r\n[1] 0.7755587\r\n\r\nThis transitivity tells me that the average network transitivity is significantly higher than the global transitivity, indicating, from my still naive network knowledge, that the overall network is generally more loose, and that there is a more connected sub-network.\r\nGeodesic Distance\r\nLooking at the geodesic distance tells me that on average, the path length is just over 2.\r\n\r\n\r\nShow code\r\n\r\naverage.path.length(grateful_data,directed=F)\r\n\r\n\r\n[1] 2.01\r\n\r\nComponents\r\nGetting a look at the components of the network shows that there are 2 components in the network, and 25 of the 26 nodes make up the giant component with 1 isolate.\r\n\r\n\r\nShow code\r\n\r\nnames(igraph::components(grateful_data))\r\n\r\n\r\n[1] \"membership\" \"csize\"      \"no\"        \r\n\r\nShow code\r\n\r\nigraph::components(grateful_data)$no \r\n\r\n\r\n[1] 2\r\n\r\nShow code\r\n\r\nigraph::components(grateful_data)$csize\r\n\r\n\r\n[1] 25  1\r\n\r\nThis is a great start - now I can get to looking at the network density, centrality, and centralization.\r\nDensity\r\nThe network density measure: First with just the call “graph.density” and then with adding “loops=TRUE”. Since I’m using igraph, I know that its’ default output assumes that loops are not included but does not remove them, which can be corrected with the addition of “loops=TRUE” per the course tutorials when comparing output to statnet. This gives me confidence that my network density is closer to 1.58.\r\n\r\n\r\nShow code\r\n\r\ngraph.density(grateful_data)\r\n\r\n\r\n[1] 1.716923\r\n\r\nShow code\r\n\r\ngraph.density(grateful_data, loops=TRUE)\r\n\r\n\r\n[1] 1.589744\r\n\r\nDegree\r\nThe network degree measure: This gives me a clear output showing the degree of each particular node (songwriter). It is not suprising, knowing my subject matter, that Jerry Garcia is the highest degree node in this network as the practical and figurative head of the band. The other band members’ degree measures are not necessarily what I expected, though. I did not anticipate that his songwriting partner, Robert Hunter, would have a lower degree than band members Phil Lesh and Bob Weir. Further, I did not anticipate that the degree measure of band member ‘Pigpen’ would be so high given his early death in the first years of the band’s touring life.\r\n\r\n\r\nShow code\r\n\r\nigraph::degree(grateful_data)\r\n\r\n\r\n  Eric Andersen     John Barlow     Bob Bralove  Andrew Charles \r\n              1              30              12               1 \r\n    John Dawson    Willie Dixon    Jerry Garcia  Donna Godchaux \r\n              2               2             215              16 \r\n Keith Godchaux   Gerrit Graham     Frank Guida     Mickey Hart \r\n             19               1               2              25 \r\n  Bruce Hornsby   Robert Hunter Bill Kreutzmann       Ned Lagin \r\n              4             136             121               1 \r\n      Phil Lesh      Peter Monk   Brent Mydland     Dave Parker \r\n            158               1              24              10 \r\nRobert Petersen          Pigpen     Joe Royster   Rob Wasserman \r\n              5             119               2              10 \r\n       Bob Weir   Vince Welnick \r\n            188              11 \r\n\r\nTo look further I will create a dataframe for easier review going forward.\r\n\r\n\r\nShow code\r\n\r\ngrateful_nodes<-data.frame(name=V(grateful_data)$name, degree=igraph::degree(grateful_data))\r\nhead(grateful_nodes)\r\n\r\n\r\n                         name degree\r\nEric Andersen   Eric Andersen      1\r\nJohn Barlow       John Barlow     30\r\nBob Bralove       Bob Bralove     12\r\nAndrew Charles Andrew Charles      1\r\nJohn Dawson       John Dawson      2\r\nWillie Dixon     Willie Dixon      2\r\n\r\nSummary Statistics\r\nA quick look at the summary statistics confirms for me the minimum, maximum, median, and mean node degree data.\r\n\r\n\r\nShow code\r\n\r\nsummary(grateful_nodes)\r\n\r\n\r\n     name               degree      \r\n Length:26          Min.   :  1.00  \r\n Class :character   1st Qu.:  2.00  \r\n Mode  :character   Median : 10.50  \r\n                    Mean   : 42.92  \r\n                    3rd Qu.: 28.75  \r\n                    Max.   :215.00  \r\n\r\nPlotting the Network\r\nNow I want to take a step back and try to visually represent this data better.\r\n\r\n\r\nShow code\r\n\r\n# Community detection algoritm \r\ncommunity <- cluster_louvain(grateful_data) \r\n\r\n# Attach communities to relevant vertices\r\nV(grateful_data)$color <- community$membership \r\n\r\n# Graph layout\r\nlayout <- layout.random(grateful_data) \r\n\r\n# igraph plot \r\nplot(grateful_data, layout = layout)\r\n\r\n\r\n\r\n\r\nBetter Plotting\r\nBetter, but not quite.\r\n\r\n\r\nShow code\r\n\r\nggraph(grateful_data, layout = \"fr\") +\r\n  geom_edge_link() + \r\n  geom_node_point(aes(color = factor(color))) + \r\n  geom_node_text(aes(label = name), repel = TRUE) +\r\n  theme_void() +\r\n  theme(legend.position = \"none\") \r\n\r\n\r\n\r\n\r\nAdding More Detail\r\nThat is starting to look more meaningful!\r\n\r\n\r\nShow code\r\n\r\n# Set size to degree centrality \r\nV(grateful_data)$size = degree(grateful_data)\r\n\r\n# Additional customisation for better legibility \r\nggraph(grateful_data, layout = \"fr\") +\r\n  geom_edge_arc(strength = 0.2, width = 0.5, alpha = 0.15) + \r\n  geom_node_point(aes(size = size, color = factor(color))) + \r\n  geom_node_text(aes(label = name, size = size), repel = TRUE) +\r\n  theme_void() +\r\n  theme(legend.position = \"none\") \r\n\r\n\r\n\r\n\r\nThere is a lot more to do, but this is a great start.\r\nCitations:\r\nAllan, Alex; Grateful Dead Lyric & Song Finder: https://whitegum.com/~acsa/intro.htm\r\nASCAP. 18 March 2022.\r\nDodd, David; The Annotated Grateful Dead Lyrics: http://artsites.ucsc.edu/gdead/agdl/\r\nSchofield, Matt; The Grateful Dead Family Discography: http://www.deaddisc.com/\r\nThis information is intended for private research only, and not for any commercial use. Original Grateful Dead songs are ©copyright Ice Nine Music\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-03-dacss-697e-assignment-3/distill-preview.png",
    "last_modified": "2022-04-12T00:06:56-05:00",
    "input_file": "dacss-697e-assignment-3.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-dacss-603-assignment-2/",
    "title": "DACSS 603 Assignment 2",
    "description": "Assignment 2 for DACSS 603 course 'Quantitative Data Analysis': \"Regression Background, Simple Linear Regression, & Multiple Regression\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-03-11",
    "categories": [
      "statistics",
      "homework"
    ],
    "contents": "\r\nQuestion 1\r\nUnited Nations\r\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\n1.1.1. Identify the predictor and the response.\r\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\nAnswer 1\r\nFirst, I load in the appropriate data. Then I look at the head() info to preview the dataset, and use the function “?UN11” to get details on this data.\r\nThe defining measure of the variables I am examining are:\r\n\r\nppgdp, representing “per capita gross domestic product in US dollars”, and fertility, representing “number of children per woman”.\r\n\r\n1.1.1: The predictor variable is the ppgdp and the response variable is fertility.\r\nNext, I create the scatterplot and add a theoretical regression line using the smooth() function and applying the ‘lm’ method in ggplot2. Using “smooth()” in this way allows me to see patterns in the graph representing a potential linear regression line.\r\n\r\n\r\nShow code\r\n\r\n#First, I loaded the data, used the head() function to preview the dataset, and used the \"?UN11\" function to get additional details on the data represented in the dataset. \r\n\r\ndata(\"UN11\")\r\n\r\n#Then, I created a dataframe with the relevant variables and plotted them using ggplot2\r\n\r\ndf1 <- UN11 %>% \r\n  select(c(ppgdp, fertility))\r\n\r\ngg1a <- ggplot(df1, aes(x=ppgdp, y=fertility)) +\r\n  geom_point() +\r\n  geom_smooth(method=lm,se=FALSE,fullrange=TRUE,color=\"goldenrod\") +\r\n   labs(title= \"Fertility and GDP\",\r\n        x= \"GNP (Per Person in US Dollars)\", \r\n        y = \"Fertility (Births Per Woman)\") +\r\n   theme_classic()\r\n\r\ngg1a\r\n\r\n\r\n\r\n\r\n1.1.2: The scatterplot indicates that this graph does not support a straight-line mean function.\r\nFinally, I add the log() functions to analyze the linear model and again use “smooth()” to look at patterns in the graph representing a regression line on a linear model.\r\n\r\n\r\nShow code\r\n\r\ngg1b <- ggplot(df1, aes(x=log(ppgdp), y=log(fertility))) +\r\n  geom_point() +\r\n  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"goldenrod\") +\r\n   labs(title= \"Log: Fertility and GDP\",\r\n        x= \"GNP (Per Person in US Dollars)\",\r\n        y = \"Fertility (Births Per Woman)\") +\r\n   theme_classic()\r\n\r\ngg1b\r\n\r\n\r\n\r\n\r\n1.1.3: It seems the simple linear regression model is plausible. The consistent downward line would be the best fit for most of the data points.\r\nAdditionally, the summary() call of the linear model “lm()” function allows me to confirm that the p-value is statistically significant to a high level of confidence (< 0.001)\r\n\r\n\r\nShow code\r\n\r\nfit1 <- lm(fertility ~ ppgdp, data = df1)\r\nsummary(fit1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = fertility ~ ppgdp, data = df1)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\r\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.206 on 197 degrees of freedom\r\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \r\nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\r\n\r\nQuestion 2\r\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\nHow, if at all, does the slope of the prediction equation change?\r\n\r\nHow, if at all, does the correlation change?\r\n\r\nAnswer 2\r\nThe slope will change - if slope = “s”, the new slope becomes (s/1.33). This is because we are looking at the response variable. When converting an explanatory variable, there is an inverse relationship to the slope and we would multiply the old slope by 1.33.\r\n\r\nThe correlation will not change. This is because correlation is not affected by a change in units.\r\n\r\nQuestion 3\r\nWater Runoff in the Sierras\r\nCan Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM.\r\nDraw the scatterplot matrix for these data and summarize the information available from these plots.\r\nAnswer 3\r\nFirst, I load in the appropriate data. Then I look at the head() info to preview the dataset, and use the function “?water” to get details on this “California water” data. I can now create a scatterplot matrix for the data using the plot() function. This allows me to make an initial look into the existence of linear relationships in the data.\r\n\r\n\r\nShow code\r\n\r\n#First I load the data\r\n\r\ndata(\"water\")\r\n\r\n#Next, I create the dataframe object from the data\r\n\r\ndf3 <- water\r\n\r\n#Then, I make an initial scatterplot matrix\r\n\r\npairs(df3)\r\n\r\n\r\n\r\n\r\nUsing the plot() function allows me to identify visually the positive linear correlations between these variables. Those appear to me at first glance to be, leading with the strongest visual representation:\r\nOPSLAKE and OPRC\r\nOPSLAKE and OPBPC\r\nBSAAM and OPSLAKE\r\nBSAAM and OPRC\r\nBSAAM and OPBPC\r\nStill clear but weaker positive linear correlations:\r\nOPRC and OPBPC\r\nAPMAM and APSAB\r\nAPMAM and APSLAKE\r\nAPSAB and APSLAKE\r\nThere are no apparent positive (or negative!) linear correlations between the ‘Year’ variable and any other variable.\r\nThis is helpful in visualizing relationships, but I have better opportunities to visualize this scatterplot matrix in R. I also would like to look at the statistical correlation evaluations of these relationships.\r\nOne opportunity is a package called “GGally” that builds upon ggplot2 to look at more than just the general scatterplot matrix. The primary difference between the GGally “ggpairs()” function and the pairs function of base R is that the diagonal consists of the densities of the variables and the upper panels consist of the Pearson correlation coefficients between the variables using the argument “pearson”.\r\n\r\n\r\nShow code\r\n\r\nggpairs(df3, method = c(\"everything\", \"pearson\"))\r\n\r\n\r\n\r\n\r\nUsing this representation, I can see both the visual correlations and the statistical correlations of the pairs. This allows me to compare my predictions to the correlation scores. Highest significant correlations are:\r\nOPSLAKE and OPBPC\r\nBSAAM and OPSLAKE\r\nBSAAM and OPRC\r\nOPSLAKE and OPRC\r\nAPSLAKE and APSAB\r\nBSAAM and OPBPC\r\nOPRC and OPBPC\r\nAPSAB and APMAM\r\nAPSLAKE and APMAM\r\nThe results confirm that although my visual evaluation using the initial scatterplot matrix was not necessarily 100% accurate, it served as a reliable estimate of positive linear statistical correlation.\r\nFinally, I want to use a third option for visualizing these correlations that is even more simple and graphically pleasing, the “ggcorr()” function in GGally. This method cleanly demonstrates the strong correlations we have evaluated in a simple way.\r\n\r\n\r\nShow code\r\n\r\nggcorr(df3, method = c(\"everything\", \"pearson\")) \r\n\r\n\r\n\r\n\r\nQuestion 4\r\nProfessor Ratings\r\nIn the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\nAnswer 4\r\nFirst, I load in the appropriate data. Then I look at the head() info to preview the dataset, and use the function “?Rateprof” to get details on this dataset. I create a dataframe of the variables of interest from the Rateprof dataset.\r\n\r\n\r\nShow code\r\n\r\n#First I load the data\r\n\r\ndata(\"Rateprof\")\r\n\r\n#Then create a data frame object of the relevant variables\r\n\r\ndf4 <- Rateprof %>% \r\n  select(c(quality, helpfulness, clarity, easiness, raterInterest))\r\n\r\n#I preview the data to understand what type of data is represented\r\n\r\nhead(df4)\r\n\r\n\r\n   quality helpfulness  clarity easiness raterInterest\r\n1 4.636364    4.636364 4.636364 4.818182      3.545455\r\n2 4.318182    4.545455 4.090909 4.363636      4.000000\r\n3 4.790698    4.720930 4.860465 4.604651      3.432432\r\n4 4.250000    4.458333 4.041667 2.791667      3.181818\r\n5 4.684211    4.684211 4.684211 4.473684      4.214286\r\n6 4.233333    4.266667 4.200000 4.533333      3.916667\r\n\r\nUsing the pairs() function, I am now able to create a scatterplot matrix that matches the one in the book (ALR, Problem 1.6).\r\n\r\n\r\nShow code\r\n\r\npairs(df4)\r\n\r\n\r\n\r\n\r\nThe relationships illustrated by the correlations represented in this scatterplot matrix indicate a strong, positive linear relationship between quality and helpfulness. There is also a strong, positive linear relationship between quality and clarity, but for one outlier. These tell me that as the rankings given to a professor on quality, helpfulness, and clarity each rise, they can expect the other variables in that group to rise as well. HOwever, I cannot guess or speculate as to the cause of these correlations.\r\nIn comparison, there is a moderate positive linear relationship between positive ratings of easiness with each variable except for raterInterest. The variable raterInterest has a horizontal correlation with all of the other 4 variables, indicating either no correlation or a very weak correlation between them.\r\nQuestion 5\r\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\r\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\r\nUse graphical ways to portray the individual variables and their relationship.\r\n\r\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\r\n\r\nSummarize and interpret results of inferential analyses.\r\n\r\nAnswer 5\r\nAfter installing the “smss” package and loading the relevant library, I again inspect the data and package info.\r\n\r\n\r\nShow code\r\n\r\n#First I load the data\r\n\r\ndata(\"student.survey\")\r\n\r\n#Then I create a data frame with the data set and look at the content\r\n\r\ndf5 <- student.survey\r\n\r\nhead(student.survey)\r\n\r\n\r\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi\r\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative\r\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal\r\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal\r\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate\r\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal\r\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal\r\n            re    ab    aa    ld\r\n1   most weeks FALSE FALSE FALSE\r\n2 occasionally FALSE FALSE    NA\r\n3   most weeks FALSE FALSE    NA\r\n4 occasionally FALSE FALSE FALSE\r\n5        never FALSE FALSE FALSE\r\n6 occasionally FALSE FALSE    NA\r\n\r\nFirst, I use the generalized “plot()” command to take a preliminary look at the graphical data.\r\n\r\n\r\n\r\nShow code\r\n\r\n#To plot y = political ideology and x = religiosity, the relative variable names are \"pi\" and \"re\".\r\n\r\nplot1 <- plot(pi ~ re, data = student.survey)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n#To plot y = high school GPA and x = hours of TV watching, the relative variable names are \"hi\" and \"tv\".\r\n\r\nplot2 <- plot(hi ~ tv, data = student.survey)\r\n\r\n\r\n\r\n\r\nThese initial representations really don’t tell me much about the relationships between the variables.\r\nNow I need to do some data cleaning, and convert the categorical variables for political ideology and religiosity into numeric variables. I will also rename the columns for the sake of clarity.\r\nFor political ideology (“pi”), “as.integer” represents the categorical variables with values of 1 to 7, starting with very liberal (1) and increasing in assigned value to very conservative (7). For religiousity, “as.integer” represents the categorical variables with values of 1 to 4, starting with never (attending religious services) (1) to every week (4). For religiosity (“re”) this was how often you attend religious services, “never” = 1, “occasionally” = 2, “most weeks” = 3, “every week” = 4\r\n\r\n\r\nShow code\r\n\r\ndf5 <- student.survey %>% \r\n  select(c(hi, tv, pi, re))\r\n\r\ndf5$pi <- as.integer(as.factor(df5$pi))\r\ndf5$re <- as.integer(as.factor(df5$re))\r\n\r\n\r\n\r\nThen I create a sub-frame for the particular pair of variables I want to compare. First, religiosity and political ideology; and then create a scatter plot of the correlation between religiosity and political ideology.\r\n\r\n\r\nShow code\r\n\r\ndf5a <- df5 %>%\r\n  rename(Political.Ideology = pi,\r\n         Religiosity = re) %>% \r\n  select(Political.Ideology, Religiosity)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ngg5a <- ggplot(df5a, aes(x=Religiosity, y=Political.Ideology)) +\r\n  geom_point() +\r\n  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"goldenrod\") +\r\n   labs(title= \"Religiosity & Political Ideology\",\r\n        x= \"Religiosity\", \r\n        y = \"Political Ideology\") +\r\n   theme_classic()\r\n\r\ngg5a\r\n\r\n\r\n\r\n\r\nAgain, I create a sub-frame for the specific pair of variables I want to compare. Now I look at high school GPA and hours of TV watched per week and create a scatter plot of the correlation between high school GPA and hours of TV watched per week.\r\n\r\n\r\nShow code\r\n\r\ndf5b <- student.survey %>%\r\n  rename(High.School.GPA = hi,\r\n         Hours.TV = tv) %>% \r\n  select(High.School.GPA, Hours.TV)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ngg5b <- ggplot(df5b, aes(x=Hours.TV, y=High.School.GPA)) +\r\n  geom_point() +\r\n  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"goldenrod\") +\r\n   labs(title= \"High School GPA & Average Hours of TV Watched Per Week\",\r\n        x= \"Average Number of Hours of TV Watched per Week\", \r\n        y = \"High School GPA\") +\r\n   theme_classic()\r\n\r\ngg5b\r\n\r\n\r\n\r\n\r\nI start to interpret descriptive statistics for summarizing the individual variables and their relationship using the “summary()” function.\r\n\r\n\r\n\r\nShow code\r\n\r\nsummary(df5)\r\n\r\n\r\n       hi              tv               pi              re       \r\n Min.   :2.000   Min.   : 0.000   Min.   :1.000   Min.   :1.000  \r\n 1st Qu.:3.000   1st Qu.: 3.000   1st Qu.:2.000   1st Qu.:1.750  \r\n Median :3.350   Median : 6.000   Median :2.000   Median :2.000  \r\n Mean   :3.308   Mean   : 7.267   Mean   :3.033   Mean   :2.167  \r\n 3rd Qu.:3.625   3rd Qu.:10.000   3rd Qu.:4.000   3rd Qu.:3.000  \r\n Max.   :4.000   Max.   :37.000   Max.   :7.000   Max.   :4.000  \r\n\r\nChecking the Pearson Correlation Coefficient and Linear Model Fit of Each Relationship\r\nThis basic correlation matrix gives an overview of the correlations for religiosity and political ideology, rounded to 2 decimals. This indicates a moderate positive relationship between the variables.\r\n\r\n\r\nShow code\r\n\r\nround(cor(df5a),\r\n  digits = 2 \r\n)\r\n\r\n\r\n                   Political.Ideology Religiosity\r\nPolitical.Ideology               1.00        0.58\r\nReligiosity                      0.58        1.00\r\n\r\nUsing the “lm()” formula, I can confirm that the p-value for this relationship is ~1.22.\r\n\r\n\r\nShow code\r\n\r\nfit5a <- lm(pi ~ re, data = df5)\r\n\r\nsummary(fit5a)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = pi ~ re, data = df5)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.81243 -0.87160  0.09882  1.12840  3.09882 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \r\nre            0.9704     0.1792   5.416 1.22e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.345 on 58 degrees of freedom\r\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \r\nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\r\n\r\nAgain, using a correlation matrix gives an overview of the correlations for high school GPA and hours of TV watched, rounded to 2 decimals. This indicates a weak negative correlation between the variables.\r\n\r\n\r\nShow code\r\n\r\nround(cor(df5b),\r\n  digits = 2 \r\n)\r\n\r\n\r\n                High.School.GPA Hours.TV\r\nHigh.School.GPA            1.00    -0.27\r\nHours.TV                  -0.27     1.00\r\n\r\nUsing the “lm()” formula, I can confirm that the p-value for this relationship is ~0.04\r\n\r\n\r\nShow code\r\n\r\nfit5b <- lm(tv ~ hi, data = df5)\r\n\r\nsummary(fit5b)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = tv ~ hi, data = df5)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-8.600 -3.790 -1.167  2.408 27.746 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)   \r\n(Intercept)   20.200      6.175   3.271   0.0018 **\r\nhi            -3.909      1.849  -2.114   0.0388 * \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 6.528 on 58 degrees of freedom\r\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \r\nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\r\n\r\nSummarize and interpret results of inferential analyses.\r\n\r\nThe basis analysis of these pairs of data from the “student.survey” data set tells me that if the null hypothesis are:\r\nH1: There is no correlation between religiosity and political ideology; and\r\nH2: There is no correlation between grade point average and hours of television watched,\r\nthen I can use the inferential analyses to conclude that I have sufficient evidence at the 95% confidence level to reject both H1 and H2 null hypothesis. However, if I choose to use a 99% confidence level, I would be able to to reject H1, but not to reject H2.\r\nQuestion 6\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\r\nAnswer\r\nNo, we do not have enough information to say that this result is the direct effect of the special tutoring program. The increase in scores among the tutored subset may be due to other factors, such as the students having a better day on the day of the second test, among other potential facts.\r\nMoreover, there is the statistical phenomenon at play of regression toward the mean which can help explain how extreme values correct toward the mean in repeated samples. There is a potential for the results to be statistically significant, but without additional information, the explanation that randomization and regression toward the mean remains primary.\r\nThis example could potentially be strengthened by more information, such as multiple test results over the semester rather than just two tests or a density plot of the entire class scores. It could also be strengthened by the presence of a control group.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-03-dacss-603-assignment-2/dacss-603-assignment-2_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-04-04T12:04:03-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-dacss-697e-post-3/",
    "title": "DACSS 697D Post 3",
    "description": "Assignment 3 for DACSS 697D Course 'Text as Data': \"Harry Potter House Sorting\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-03-06",
    "categories": [
      "text as data",
      "homework",
      "Harry Potter"
    ],
    "contents": "\r\nLab Session Project\r\nThis post began as a group lab project where our small group decided to try and determine house affiliations in the Harry Potter book corpus. As a group, we focused on students and faculty who are active at Hogwarts within the first book. Although we did not have time to work through this as a group, I continued on with the lab for my own knowledge and development of the concepts.\r\nThe names of the four houses to which characters will be determined to be affiliated with are: Gryffindor, Hufflepuff, Ravenclaw, and Slytherin\r\nFirst I created the corpus from the (harrypotter) library\r\n\r\n\r\nphilosophers_stone_corpus <- corpus(philosophers_stone)\r\nphilosophers_stone_summary <- summary(philosophers_stone_corpus) \r\nphilosophers_stone_summary$book <- \"Philosopher's Stone\"\r\nphilosophers_stone_summary$chapter <- as.numeric(str_extract(philosophers_stone_summary$Text, \"[0-9]+\"))\r\nphilosophers_stone_summary\r\n\r\n\r\nCorpus consisting of 17 documents, showing 17 documents:\r\n\r\n   Text Types Tokens Sentences                book chapter\r\n  text1  1273   5643       349 Philosopher's Stone       1\r\n  text2  1067   4128       237 Philosopher's Stone       2\r\n  text3  1226   4630       297 Philosopher's Stone       3\r\n  text4  1198   4761       321 Philosopher's Stone       4\r\n  text5  1820   8372       563 Philosopher's Stone       5\r\n  text6  1567   7949       566 Philosopher's Stone       6\r\n  text7  1379   5445       351 Philosopher's Stone       7\r\n  text8  1096   3594       198 Philosopher's Stone       8\r\n  text9  1426   6131       410 Philosopher's Stone       9\r\n text10  1294   5207       334 Philosopher's Stone      10\r\n text11  1113   4152       276 Philosopher's Stone      11\r\n text12  1511   6729       447 Philosopher's Stone      12\r\n text13  1078   3930       261 Philosopher's Stone      13\r\n text14  1112   4354       308 Philosopher's Stone      14\r\n text15  1386   6437       459 Philosopher's Stone      15\r\n text16  1581   8277       591 Philosopher's Stone      16\r\n text17  1490   7101       506 Philosopher's Stone      17\r\n\r\ndocvars(philosophers_stone_corpus) <- philosophers_stone_summary\r\n\r\n\r\n\r\n\r\n\r\nphilosophers_stone_tokens <- tokens(philosophers_stone_corpus, \r\n    remove_punct = T,\r\n    remove_numbers = T)\r\nprint(philosophers_stone_tokens)\r\n\r\n\r\nTokens consisting of 17 documents and 6 docvars.\r\ntext1 :\r\n [1] \"THE\"     \"BOY\"     \"WHO\"     \"LIVED\"   \"Mr\"      \"and\"    \r\n [7] \"Mrs\"     \"Dursley\" \"of\"      \"number\"  \"four\"    \"Privet\" \r\n[ ... and 4,579 more ]\r\n\r\ntext2 :\r\n [1] \"THE\"       \"VANISHING\" \"GLASS\"     \"Nearly\"    \"ten\"      \r\n [6] \"years\"     \"had\"       \"passed\"    \"since\"     \"the\"      \r\n[11] \"Dursleys\"  \"had\"      \r\n[ ... and 3,433 more ]\r\n\r\ntext3 :\r\n [1] \"THE\"         \"LETTERS\"     \"FROM\"        \"NO\"         \r\n [5] \"ONE\"         \"The\"         \"escape\"      \"of\"         \r\n [9] \"the\"         \"Brazilian\"   \"boa\"         \"constrictor\"\r\n[ ... and 3,827 more ]\r\n\r\ntext4 :\r\n [1] \"THE\"     \"KEEPER\"  \"OF\"      \"THE\"     \"KEYS\"    \"BOOM\"   \r\n [7] \"They\"    \"knocked\" \"again\"   \"Dudley\"  \"jerked\"  \"awake\"  \r\n[ ... and 3,674 more ]\r\n\r\ntext5 :\r\n [1] \"DIAGON\"   \"ALLEY\"    \"Harry\"    \"woke\"     \"early\"    \"the\"     \r\n [7] \"next\"     \"morning\"  \"Although\" \"he\"       \"could\"    \"tell\"    \r\n[ ... and 6,543 more ]\r\n\r\ntext6 :\r\n [1] \"THE\"            \"JOURNEY\"        \"FROM\"          \r\n [4] \"PLATFORM\"       \"NINE\"           \"AND\"           \r\n [7] \"THREE-QUARTERS\" \"Harry's\"        \"last\"          \r\n[10] \"month\"          \"with\"           \"the\"           \r\n[ ... and 6,270 more ]\r\n\r\n[ reached max_ndoc ... 11 more documents ]\r\n\r\nPulling the stopwords from my tokens object\r\n\r\n\r\nlength(stopwords(\"en\"))\r\n\r\n\r\n[1] 174\r\n\r\nphilosophers_stone_tokens <- tokens_select(philosophers_stone_tokens, \r\n                                           pattern = stopwords(\"en\"),\r\n                                           selection = \"remove\")\r\n\r\nlength(philosophers_stone_tokens)\r\n\r\n\r\n[1] 17\r\n\r\nprint(philosophers_stone_tokens)\r\n\r\n\r\nTokens consisting of 17 documents and 6 docvars.\r\ntext1 :\r\n [1] \"BOY\"       \"LIVED\"     \"Mr\"        \"Mrs\"       \"Dursley\"  \r\n [6] \"number\"    \"four\"      \"Privet\"    \"Drive\"     \"proud\"    \r\n[11] \"say\"       \"perfectly\"\r\n[ ... and 2,308 more ]\r\n\r\ntext2 :\r\n [1] \"VANISHING\" \"GLASS\"     \"Nearly\"    \"ten\"       \"years\"    \r\n [6] \"passed\"    \"since\"     \"Dursleys\"  \"woken\"     \"find\"     \r\n[11] \"nephew\"    \"front\"    \r\n[ ... and 1,783 more ]\r\n\r\ntext3 :\r\n [1] \"LETTERS\"      \"ONE\"          \"escape\"       \"Brazilian\"   \r\n [5] \"boa\"          \"constrictor\"  \"earned\"       \"Harry\"       \r\n [9] \"longest-ever\" \"punishment\"   \"time\"         \"allowed\"     \r\n[ ... and 2,059 more ]\r\n\r\ntext4 :\r\n [1] \"KEEPER\"   \"KEYS\"     \"BOOM\"     \"knocked\"  \"Dudley\"   \"jerked\"  \r\n [7] \"awake\"    \"cannon\"   \"said\"     \"stupidly\" \"crash\"    \"behind\"  \r\n[ ... and 1,972 more ]\r\n\r\ntext5 :\r\n [1] \"DIAGON\"   \"ALLEY\"    \"Harry\"    \"woke\"     \"early\"    \"next\"    \r\n [7] \"morning\"  \"Although\" \"tell\"     \"daylight\" \"kept\"     \"eyes\"    \r\n[ ... and 3,702 more ]\r\n\r\ntext6 :\r\n [1] \"JOURNEY\"        \"PLATFORM\"       \"NINE\"          \r\n [4] \"THREE-QUARTERS\" \"Harry's\"        \"last\"          \r\n [7] \"month\"          \"Dursleys\"       \"fun\"           \r\n[10] \"True\"           \"Dudley\"         \"now\"           \r\n[ ... and 3,284 more ]\r\n\r\n[ reached max_ndoc ... 11 more documents ]\r\n\r\nIf I engage the ‘stemming’ feature, I can see how many of the tokens are affected. In the case of this text, a first review of the resulting tokens left them less relevant than the original tokens, so I will not use that feature here.\r\n\r\n\r\n#Then pull the corpus as a character vector (which works with cleanNLP) rather than a corpus object, which does not.\r\n\r\nphilosophers_stone_char_vector <- as.character(philosophers_stone_corpus)\r\n\r\n\r\n\r\n\r\n\r\n#Return a data frame of the document-level variables\r\n\r\nhp_data <- docvars(philosophers_stone_corpus)\r\n\r\n#Now add the text to my data frame for running the annotation tool; column must be named `text`\r\n\r\nhp_data$text <- philosophers_stone_char_vector\r\n\r\n\r\n\r\nBefore I can use the cleanNLP package, I need to initialize the “spacyr” installation and run the command to initialize the back end, “cnlp_init_udpipe()” and then begin annotating and analyzing:\r\n\r\n\r\nspacy_initialize()\r\n\r\ncnlp_init_udpipe()\r\n\r\n#Now I can start analyzing the corpus:\r\n\r\nannotated <- cnlp_annotate(hp_data)\r\n\r\n\r\nProcessed document 10 of 17\r\n\r\nhead(annotated$token)\r\n\r\n\r\n# A tibble: 6 x 11\r\n  doc_id   sid tid   token token_with_ws lemma upos  xpos  feats      \r\n   <int> <int> <chr> <chr> <chr>         <chr> <chr> <chr> <chr>      \r\n1      1     1 1     THE   \"THE \"        the   DET   DT    Definite=D~\r\n2      1     1 2     BOY   \"BOY \"        Boy   NOUN  NN    Number=Sing\r\n3      1     1 3     WHO   \"WHO \"        who   PRON  WP    PronType=R~\r\n4      1     1 4     LIVED \"LIVED  \"   live  VERB  VBD   Mood=Ind|T~\r\n5      1     1 5     Mr.   \"Mr. \"        Mr.   PROPN NNP   Number=Sing\r\n6      1     1 6     and   \"and \"        and   CCONJ CC    <NA>       \r\n# ... with 2 more variables: tid_source <chr>, relation <chr>\r\n\r\nhead(annotated$document)\r\n\r\n\r\n   Text Types Tokens Sentences                book chapter doc_id\r\n1 text1  1273   5643       349 Philosopher's Stone       1      1\r\n2 text2  1067   4128       237 Philosopher's Stone       2      2\r\n3 text3  1226   4630       297 Philosopher's Stone       3      3\r\n4 text4  1198   4761       321 Philosopher's Stone       4      4\r\n5 text5  1820   8372       563 Philosopher's Stone       5      5\r\n6 text6  1567   7949       566 Philosopher's Stone       6      6\r\n\r\n#I will join the data frames into one database for analyzing all patterns.\r\n\r\nanno_hpdata <- left_join(annotated$document, annotated$token, by = \"doc_id\")\r\n\r\nhead(anno_hpdata)\r\n\r\n\r\n   Text Types Tokens Sentences                book chapter doc_id sid\r\n1 text1  1273   5643       349 Philosopher's Stone       1      1   1\r\n2 text1  1273   5643       349 Philosopher's Stone       1      1   1\r\n3 text1  1273   5643       349 Philosopher's Stone       1      1   1\r\n4 text1  1273   5643       349 Philosopher's Stone       1      1   1\r\n5 text1  1273   5643       349 Philosopher's Stone       1      1   1\r\n6 text1  1273   5643       349 Philosopher's Stone       1      1   1\r\n  tid token token_with_ws lemma  upos xpos\r\n1   1   THE          THE    the   DET   DT\r\n2   2   BOY          BOY    Boy  NOUN   NN\r\n3   3   WHO          WHO    who  PRON   WP\r\n4   4 LIVED       LIVED    live  VERB  VBD\r\n5   5   Mr.          Mr.    Mr. PROPN  NNP\r\n6   6   and          and    and CCONJ   CC\r\n                             feats tid_source  relation\r\n1        Definite=Def|PronType=Art          2       det\r\n2                      Number=Sing         18     nsubj\r\n3                     PronType=Rel          4     nsubj\r\n4 Mood=Ind|Tense=Past|VerbForm=Fin          2 acl:relcl\r\n5                      Number=Sing          4       obj\r\n6                             <NA>          7        cc\r\n\r\nNow I can really start to look for affiliations to the houses. I’ll start by looking at some of the data for annotation options. Filtering by parts of speech, I can find that the lemma “sort” is found in 27 instances as a noun,\r\n\r\n\r\nanno_hpdata %>% \r\n  filter(upos == \"VERB\") %>%\r\n  group_by(lemma) %>% \r\n  summarize(count = n()) %>%\r\n  top_n(n=250) %>%\r\n  arrange(desc(count))\r\n\r\n\r\n# A tibble: 254 x 2\r\n   lemma count\r\n   <chr> <int>\r\n 1 say     921\r\n 2 get     442\r\n 3 have    417\r\n 4 go      387\r\n 5 look    380\r\n 6 be      312\r\n 7 know    305\r\n 8 see     304\r\n 9 think   226\r\n10 do      222\r\n# ... with 244 more rows\r\n\r\nLooking at the corpus keywords using the “kwic” function and taking that information into account, it seems that the bulk of the conversation about sorting students into houses takes place in chapter 7:\r\n\r\n\r\nsorted <- kwic(philosophers_stone_corpus, pattern = \"sort*\")\r\n\r\nsorted %>% \r\n  group_by(docname)\r\n\r\n\r\n# A tibble: 44 x 7\r\n# Groups:   docname [13]\r\n   docname  from    to pre              keyword post           pattern\r\n   <chr>   <int> <int> <chr>            <chr>   <chr>          <fct>  \r\n 1 text2    2859  2859 \"Behind the gla~ sorts   of lizards an~ sort*  \r\n 2 text4     757   757 \", and began ta~ sorts   of things out~ sort*  \r\n 3 text4    4192  4192 \"letters and he~ sorts   of rubbish - ~ sort*  \r\n 4 text4    4281  4281 \"with youngster~ sort    , fer a chang~ sort*  \r\n 5 text5    2619  2619 \"you . \\\" \\\" Wh~ sort    of magic do y~ sort*  \r\n 6 text5    5241  5241 \"of him . He's ~ sort    of servant , ~ sort*  \r\n 7 text5    5278  5278 \". I heard he's~ sort    of savage - l~ sort*  \r\n 8 text5    5427  5427 \"they should le~ sort    in , do you ?  sort*  \r\n 9 text5    5818  5818 \"and there's fo~ sorta   hard ter expl~ sort*  \r\n10 text6    1883  1883 \"one another in~ sort    of way over t~ sort*  \r\n# ... with 34 more rows\r\n\r\nLooking at the names of the houses, starting with Gryffindor, I can first see the relevant context of the results, and it begins to emerge that the occasions when the sorting hat has declared a student part of a house, it is in all capital letters.\r\n\r\n\r\nkwic(philosophers_stone_corpus, pattern = \"Gryffindor*\")\r\n\r\n\r\nKeyword-in-context with 102 matches.                                                                    \r\n  [text6, 5835]                   and I hope I'm in |  Gryffindor  |\r\n  [text6, 5961]                     \" asked Harry.\" |  Gryffindor  |\r\n   [text7, 296]          The four houses are called |  Gryffindor  |\r\n  [text7, 1418]               . You might belong in |  Gryffindor  |\r\n  [text7, 1435]             nerve, and chivalry Set | Gryffindors  |\r\n  [text7, 1880]              \" became the first new |  Gryffindor  |\r\n  [text7, 2050]              the hat declared him a |  Gryffindor  |\r\n  [text7, 2074]                       on her head.\" |  GRYFFINDOR  |\r\n  [text7, 2190]                it finally shouted,\" |  GRYFFINDOR  |\r\n  [text7, 2522]             you're sure - better be |  GRYFFINDOR  |\r\n  [text7, 2548]       and walked shakily toward the |  Gryffindor  |\r\n  [text7, 2789]               , joined Harry at the |  Gryffindor  |\r\n  [text7, 2832]                   hat had shouted,\" |  GRYFFINDOR  |\r\n  [text7, 3282]          service. Resident ghost of |  Gryffindor  |\r\n  [text7, 3450]                         ,\" So - new | Gryffindors  |\r\n  [text7, 3466]       house championship this year? | Gryffindors  |\r\n  [text7, 4742]                      you trot!\" The |  Gryffindor  |\r\n  [text7, 5164]         and found themselves in the |  Gryffindor  |\r\n   [text8, 268]           always happy to point new | Gryffindors  |\r\n  [text8, 1323]    Professor McGonagall was head of |  Gryffindor  |\r\n  [text8, 2382]            point will be taken from |  Gryffindor  |\r\n  [text8, 2396]       Things didn't improve for the | Gryffindors  |\r\n  [text8, 2673]       another point you've lost for |  Gryffindor  |\r\n  [text8, 2744]            He'd lost two points for |  Gryffindor  |\r\n    [text9, 31]           Malfoy. Still, first-year | Gryffindors  |\r\n    [text9, 65]             notice pinned up in the |  Gryffindor  |\r\n    [text9, 83]          starting on Thursday - and |  Gryffindor  |\r\n   [text9, 646]               , who was passing the |  Gryffindor  |\r\n   [text9, 756]                  Ron, and the other | Gryffindors  |\r\n  [text9, 2642]             \" Wood's captain of the |  Gryffindor  |\r\n  [text9, 3544]           of the points you'll lose |  Gryffindor  |\r\n  [text9, 3765]             staircase, and into the |  Gryffindor  |\r\n  [text9, 3941]              \" Don't you care about |  Gryffindor  |\r\n  [text9, 4057]          Hermione was locked out of |  Gryffindor  |\r\n  [text9, 4958]                  got to get back to |  Gryffindor  |\r\n [text10, 1443]          the Keeper -I'm Keeper for |  Gryffindor  |\r\n [text10, 4539]         of winning fifty points for |  Gryffindor  |\r\n [text10, 4875]           points will be taken from |  Gryffindor  |\r\n [text10, 4903]             you'd better get off to |  Gryffindor  |\r\n [text10, 4954]                 troll. You each win |  Gryffindor  |\r\n   [text11, 91]            after weeks of training: |  Gryffindor  |\r\n   [text11, 96]     Gryffindor versus Slytherin. If |  Gryffindor  |\r\n  [text11, 515]                me. Five points from |  Gryffindor  |\r\n  [text11, 561]              said Ron bitterly. The |  Gryffindor  |\r\n  [text11, 881]           take any more points from |  Gryffindor  |\r\n [text11, 1355]                  , had done a large |  Gryffindor  |\r\n [text11, 1508]               This is the best team | Gryffindor's |\r\n [text11, 1752]  immediately by Angelina Johnson of |  Gryffindor  |\r\n [text11, 1877]             by an excellent move by |  Gryffindor  |\r\n [text11, 1882]      Gryffindor Keeper Wood and the | Gryffindors  |\r\n [text11, 1892]         that's Chaser Katie Bell of |  Gryffindor  |\r\n [text11, 1965]                  - nice play by the |  Gryffindor  |\r\n [text11, 2016]          Bletchley dives - misses - | GRYFFINDORS  |\r\n [text11, 2020]               - GRYFFINDORS SCORE!\" |  Gryffindor  |\r\n [text11, 2492]             of rage echoed from the | Gryffindors  |\r\n [text11, 2523]                 Foul!\" screamed the | Gryffindors  |\r\n [text11, 2542]               at the goal posts for |  Gryffindor  |\r\n [text11, 2719]            . Flint nearly kills the |  Gryffindor  |\r\n [text11, 2735]                   , so a penalty to |  Gryffindor  |\r\n [text11, 2754]               and we continue play, |  Gryffindor  |\r\n [text11, 2859]             to turn back toward the |  Gryffindor  |\r\n [text11, 3693]      happily shouting the results - |  Gryffindor  |\r\n   [text12, 99]                 to start. While the |  Gryffindor  |\r\n  [text12, 621]                 .\" Five points from |  Gryffindor  |\r\n [text12, 3298]                  to the fire in the |  Gryffindor  |\r\n [text12, 3371]            Fred and George all over |  Gryffindor  |\r\n  [text13, 414]          excuse to knock points off |  Gryffindor  |\r\n  [text13, 569]         headed straight back to the |  Gryffindor  |\r\n  [text13, 736]                      If I back out, |  Gryffindor  |\r\n  [text13, 797]                   all the way up to |  Gryffindor  |\r\n  [text13, 963]               brave enough to be in |  Gryffindor  |\r\n [text13, 1036]           Sorting Hat chose you for |  Gryffindor  |\r\n [text13, 2481]          they choose people for the |  Gryffindor  |\r\n [text13, 2914]                     won! We've won! |  Gryffindor  |\r\n [text13, 2975]             lasted five minutes. As | Gryffindors  |\r\n [text13, 3137]                   was a happy blur: | Gryffindors  |\r\n [text13, 3193]                 in the setting sun. |  Gryffindor  |\r\n  [text15, 500]                 . Potter, I thought |  Gryffindor  |\r\n  [text15, 552]           points will be taken from |  Gryffindor  |\r\n  [text15, 635]          never been more ashamed of |  Gryffindor  |\r\n  [text15, 648]               points lost. That put |  Gryffindor  |\r\n  [text15, 661]          , they'd ruined any chance |  Gryffindor  |\r\n  [text15, 741]             happen when the rest of |  Gryffindor  |\r\n  [text15, 751]                     done? At first, | Gryffindors  |\r\n [text16, 2210]                up to something. And |  Gryffindor  |\r\n [text16, 2519]      take another fifty points from |  Gryffindor  |\r\n [text16, 2786]          and your families alone if |  Gryffindor  |\r\n [text16, 3066]                   them; none of the | Gryffindors  |\r\n [text16, 3401]             you'll be caught again. |  Gryffindor  |\r\n  [text17, 286]            Snape was trying to stop |  Gryffindor  |\r\n [text17, 1469]               won the house cup for |  Gryffindor  |\r\n [text17, 5534]             Ron and Hermione at the |  Gryffindor  |\r\n [text17, 5662]                  : In fourth place, |  Gryffindor  |\r\n [text17, 5843]                 many years, I award |  Gryffindor  |\r\n [text17, 5849]                house fifty points.\" |  Gryffindor  |\r\n [text17, 5923]                    of fire, I award |  Gryffindor  |\r\n [text17, 5946]               had burst into tears. | Gryffindors  |\r\n [text17, 5992]        outstanding courage, I award |  Gryffindor  |\r\n [text17, 6014] yelling themselves hoarse knew that |  Gryffindor  |\r\n [text17, 6138]         noise that erupted from the |  Gryffindor  |\r\n [text17, 6179]                 much as a point for |  Gryffindor  |\r\n [text17, 6278]     serpent vanished and a towering |  Gryffindor  |\r\n                                   \r\n , it sounds by far                \r\n ,\" said Ron.                      \r\n , Hufflepuff, Ravenclaw,          \r\n , Where dwell the brave           \r\n apart; You might belong           \r\n , and the table on                \r\n .\" Granger, Hermione              \r\n !\" shouted the hat                \r\n ,\" Neville ran off                \r\n !\" Harry heard the                \r\n table. He was so                  \r\n table.\" Turpin,                   \r\n !\" Harry clapped loudly           \r\n Tower.\"\" I                        \r\n ! I hope you're going             \r\n have never gone so long           \r\n first years followed Percy through\r\n common room, a cozy               \r\n in the right direction,           \r\n House, but it hadn't              \r\n House for your cheek,             \r\n as the Potions lesson continued   \r\n .\" This was so                    \r\n in his very first week            \r\n only had Potions with the         \r\n common room that made them        \r\n and Slytherin would be learning   \r\n table, snatched the Remembrall    \r\n hurried down the front steps      \r\n team,\" Professor McGonagall       \r\n if you're caught, and             \r\n common room. A few                \r\n , do you only care                \r\n tower.\" Now what                  \r\n tower,\" said Ron                  \r\n . I have to fly                   \r\n faded quickly from Harry's mind   \r\n for this,\" said                   \r\n tower. Students are finishing     \r\n five points. Professor Dumbledore \r\n versus Slytherin. If Gryffindor   \r\n won, they would move              \r\n .\"\" He's just                     \r\n common room was very noisy        \r\n . He sprinted back upstairs       \r\n lion underneath. Then Hermione    \r\n had in years. We're               \r\n - what an excellent Chaser        \r\n Keeper Wood and the Gryffindors   \r\n take the Quaffle - that's         \r\n there, nice dive around           \r\n Beater, anyway, and               \r\n SCORE!\" Gryffindor cheers         \r\n cheers filled the cold air        \r\n below - Marcus Flint had          \r\n . Madam Hooch spoke angrily       \r\n . But in all the                  \r\n Seeker, which could happen        \r\n , taken by Spinner,               \r\n still in possession.\"             \r\n goal- posts - he had              \r\n had won by one hundred            \r\n common room and the Great         \r\n , Weasley, and be                 \r\n common room, where Harry          \r\n tower because they'd stolen his   \r\n !\" George Weasley really          \r\n common room, where he             \r\n can't play at all.                \r\n tower. Everyone fell over         \r\n , Malfoy's already done that      \r\n , didn't it? And                  \r\n team?\" said Malfoy                \r\n is in the lead!                   \r\n came spilling onto the field      \r\n running to lift him onto          \r\n in the lead. He'd                 \r\n meant more to you than            \r\n .\"\" Fifty?                        \r\n students.\" A hundred              \r\n in last place. In                 \r\n had had for the house             \r\n found out what they'd done        \r\n passing the giant hourglasses that\r\n really can't afford to lose       \r\n ! Yes, Weasley,                   \r\n wins the house cup?               \r\n had anything to say to            \r\n will be in even more              \r\n from winning, he did              \r\n .\" Quirrell cursed again          \r\n table and tried to ignore         \r\n , with three hundred and          \r\n house fifty points.\"              \r\n cheers nearly raised the bewitched\r\n house fifty points.\"              \r\n up and down the table             \r\n house sixty points.\"              \r\n now had four hundred and          \r\n table. Harry, Ron                 \r\n before. Harry, still              \r\n lion took its place.              \r\n\r\nkwic(\r\nphilosophers_stone_corpus,\r\npattern = \"GRYFFINDOR\",\r\ncase_insensitive = FALSE,\r\n)\r\n\r\n\r\nKeyword-in-context with 4 matches.                                                     \r\n [text7, 2074]           on her head.\" | GRYFFINDOR |\r\n [text7, 2190]    it finally shouted,\" | GRYFFINDOR |\r\n [text7, 2522] you're sure - better be | GRYFFINDOR |\r\n [text7, 2832]       hat had shouted,\" | GRYFFINDOR |\r\n                        \r\n !\" shouted the hat     \r\n ,\" Neville ran off     \r\n !\" Harry heard the     \r\n !\" Harry clapped loudly\r\n\r\nkwic(\r\nphilosophers_stone_corpus,\r\npattern = \"SLYTHERIN\",\r\ncase_insensitive = FALSE,\r\n)\r\n\r\n\r\nKeyword-in-context with 1 match.                                                                 \r\n [text7, 2246] when it screamed,\" | SLYTHERIN | !\" Malfoy went to\r\n\r\nkwic(\r\nphilosophers_stone_corpus,\r\npattern = \"HUFFLEPUFF\",\r\ncase_insensitive = FALSE,\r\n)\r\n\r\n\r\nKeyword-in-context with 3 matches.                                                                    \r\n [text7, 1762] A moments pause -\" | HUFFLEPUFF | !\" shouted the hat \r\n [text7, 1808]         , Susan!\"\" | HUFFLEPUFF | !\" shouted the hat \r\n [text7, 1991]        , Justin!\"\" | HUFFLEPUFF | !\" Sometimes, Harry\r\n\r\nkwic(\r\nphilosophers_stone_corpus,\r\npattern = \"RAVENCLAW\",\r\ncase_insensitive = FALSE,\r\n)\r\n\r\n\r\nKeyword-in-context with 1 match.                                                           \r\n [text7, 1833] , Terry!\"\" | RAVENCLAW | !\" The table second\r\n\r\nAlthough this is very promising, I need to next find an effective way to look at the entire sentence for each of these declaratory sorting statements. There is much more for me to learn regarding natural language processing!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-05T01:18:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-03-quantitative-data-analysis/",
    "title": "DACSS 603 Assignment 1",
    "description": "Assignment 1 for DACSS 603 course 'Quantitative Data Analysis': \"Descriptive Statistics, Probability, Statistical Inference, and Comparing Two Means\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-02-23",
    "categories": [
      "statistics",
      "homework"
    ],
    "contents": "\r\nQuestion 1\r\nSurgical Procedure - Representative Sample\r\nSurgical Procedure\r\nSample Size\r\nMean Wait Time\r\nStandard Deviation\r\nBypass\r\n539\r\n19\r\n10\r\nAngiography\r\n847\r\n18\r\n9\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures.\r\nIs the confidence interval narrower for angiography or bypass surgery?\r\nAnswer 1\r\nI calculated the answer by first calculating the standard error for each procedure given the mean, standard deviation, and sample size for each. I do so using 0.95 for the qnorm function so that I can determine the 5% confidence level for both the right and left side of the normal distribution, since the sample is larger than n=30. By calculating the 5% margin for each side of the distribution, this gives me the effective 90% confidence interval overall.\r\n\r\n\r\n#Calculate the actual mean wait time for the bypass:\r\n\r\nxbar1 <- 19 #sample mean\r\nsd1a <- 10 #sample standard deviation\r\nn1a <- 539 #sample size\r\n\r\nerror1a <- qnorm(0.95)*sd1a/sqrt(n1a)\r\nerror1a\r\n\r\n\r\n[1] 0.7084886\r\n\r\nlower1a <- xbar1-error1a\r\nupper1a <- xbar1+error1a\r\n\r\nlower1a\r\n\r\n\r\n[1] 18.29151\r\n\r\nupper1a\r\n\r\n\r\n[1] 19.70849\r\n\r\n\r\n\r\n#Calculate the actual mean wait time for the angiography:\r\n\r\nxbar1b <- 18 #sample mean\r\nsd1b <- 9 #sample standard deviation\r\nn1b <- 847 #sample size\r\n\r\nerror1b <- qnorm(0.95)*sd1b/sqrt(n1b)\r\nerror1b\r\n\r\n\r\n[1] 0.5086606\r\n\r\nlower1b <- xbar1b-error1b\r\nupper1b <- xbar1b+error1b\r\n\r\nlower1b\r\n\r\n\r\n[1] 17.49134\r\n\r\nupper1b\r\n\r\n\r\n[1] 18.50866\r\n\r\nNext, I created a data frame with the information.\r\n\r\n\r\nShow code\r\n\r\n#Create a data frame with the information:\r\n\r\ndf1 <- data.frame( c('Bypass', 'Angiography')\r\n                   ,c(19, 18)\r\n                   ,c(539, 847)\r\n                   ,c(10, 9)\r\n                   ,c(18.29151, 17.49134)\r\n                   ,c(19.70849, 18.50866))\r\nnames(df1) <- c('Procedure', 'Mean', 'Sample', 'SD', 'Lower', 'Upper')\r\n\r\ndf1\r\n\r\n\r\n    Procedure Mean Sample SD    Lower    Upper\r\n1      Bypass   19    539 10 18.29151 19.70849\r\n2 Angiography   18    847  9 17.49134 18.50866\r\n\r\nFinally, I created a plot to visualize the results. The visualization communicates that for each procedure, there is a range of values where we can expect 90% of the estimates to include the population mean given the sample mean and standard deviation.\r\n\r\n\r\nShow code\r\n\r\ngg1 <- ggplot(data = df1)\r\ngg1 <- gg1 + geom_point(aes(x = Procedure, y = Mean), size = 5, color = \"blue\")\r\ngg1 <- gg1 + geom_errorbar(aes(x = Procedure, y = Mean, ymin=Lower, ymax=Upper), width=.1, color = \"blue\")\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Lower, label = round(Lower, 2)), hjust = -1)\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Upper, label = round(Upper, 2)), hjust = -1)\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Mean, label = round(Mean, 2)), hjust = -0.5)\r\ngg1 <- gg1 + labs(x = \"Procedure\", y = \"Mean\")\r\ngg1 <- gg1 + labs(title = \"Chart Showing Mean With Upper and Lower Confidence Intervals at 90%\")\r\ngg1 <- gg1 + theme_classic()\r\n\r\ngg1\r\n\r\n\r\n\r\n\r\nFor the angiography, we can be 90% sure that the population mean for the wait time to the procedure falls between 17.49 and 18.51 days.\r\nFor the bypass, we can be 90% sure that the population mean for the wait time to the procedure falls between 18.29 and 19.71 days.\r\nThe confidence interval was narrower for the angiography surgery.\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success.\r\nFind the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\r\nConstruct and interpret a 95% confidence interval for p.\r\nAnswer 2\r\nTo construct the 95% confidence interval, I began by calculating the point sample estimate of the population proportion:\r\n\r\n\r\n#Calculate the point sample estimate of the population proportion using (p = x/n)\r\n\r\nx2 = 567 #affirmative response size\r\nn2 = 1031 #sample size - survey participants\r\n\r\np2 <- x2/n2\r\np2\r\n\r\n\r\n[1] 0.5499515\r\n\r\nThis tells me that the sample proportion of those who believe a college education is essential for success is ~45%.\r\nSince np >= 5 and n(1-p) >= 5, I know that I will calculate the confidence interval of that population proportion as follows: p +/- z * square root of (p) x (1-p)/n\r\n\r\n\r\n#Calculate the confidence interval for p:\r\n\r\nerror2 <-qnorm(0.975)*sqrt(p2*(1-p2)/n2)\r\nerror2\r\n\r\n\r\n[1] 0.03036761\r\n\r\nlower2 <- p2-error2\r\nupper2 <- p2+error2\r\n\r\nlower2\r\n\r\n\r\n[1] 0.5195839\r\n\r\nupper2\r\n\r\n\r\n[1] 0.5803191\r\n\r\nThis tells me that we can be 95% confident that the proportion of adult Americans who believe that a college education is essential for success lies between 51.96% and 58.03% of the population.\r\nAlternatively, using the R prop.test function, I can compare the calculation to my manual calculation and find it is the same for finding the point, but slightly different (at 4 decimal points) on the calculation of the confidence interval.\r\n\r\n\r\nprop.test(x2, n2)\r\n\r\n\r\n\r\n    1-sample proportions test with continuity correction\r\n\r\ndata:  x2 out of n2, null probability 0.5\r\nX-squared = 10.091, df = 1, p-value = 0.00149\r\nalternative hypothesis: true p is not equal to 0.5\r\n95 percent confidence interval:\r\n 0.5189682 0.5805580\r\nsample estimates:\r\n        p \r\n0.5499515 \r\n\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range.\r\nAssuming the significance level to be 5%, what should be the size of the sample?\r\nAnswer 3\r\nI will start by taking the information given and calculating the variables I need to know.\r\nIf the aid office believes the amount spent on books is between $30 and $200, I have a range of $170 to consider.\r\nI want to be 95% confident that the interval estimate contains the population mean, so my z = 1.96.\r\nI also know that the margin of error should be no more than +/- $5 on each end of the estimate, so my margin of error = 5\r\n\r\n\r\nerror3 <- (5)\r\n\r\n#I need to calculate the standard deviation at the given estimate that it is a quarter of the range:\r\n\r\nsd3 <- 170*0.25 #range * 25%\r\nsd3 #standard deviation = 42.5\r\n\r\n\r\n[1] 42.5\r\n\r\n#Now I can calculate the sample size with the formula n=(zσ/M)2.\r\n\r\nss3 <- ((1.96*sd3)/error3)^2\r\nss3 #sample size\r\n\r\n\r\n[1] 277.5556\r\n\r\nUsing these calculations, I can estimate that the financial aid office will need to use a sample size of 278 people.\r\nQuestion 4\r\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\nB. Report the P-value for Ha : μ < 500. Interpret.\r\nC. Report and interpret the P-value for H a: μ > 500.\r\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nAnswer 4\r\nA. I will start by taking the information given and determining my hypotheses:\r\nH0: The mean weekly earnings for the population of women at the company is μ=$500#\r\nHa: The mean weekly earnings for the population of women at the company is μ≠$500\r\nI cannot assume that the population distribution is normal as I have a low sample size of 9 and it is not stated it is a random sample, but a representative sample.\r\nMy other assumptions are:\r\npopulation mean (mu4) = 500\r\nsample size (n4) = 9\r\nsample mean (xbar4) = 410\r\nsample standard deviation (sd4) = 90\r\nI also need to make a decision about using a significance level of 5%.\r\nI will use the test statistic formula to find the t-value: [t = (x̄) - (μ) / (sd/sqrt(n)]\r\n\r\n\r\na4 <- 500\r\nn4 <- 9\r\nxbar4 <- 410\r\nsd4 <- 90\r\n\r\n#Test statistic:\r\n\r\nt4 <-(xbar4-a4)/(sd4/sqrt(n4))\r\n\r\nt4\r\n\r\n\r\n[1] -3\r\n\r\nGiven that my test statistic = (-3), I can determine the p-value is .00135*2 (to get the sum of both tail probabilities) or 0.0027.\r\nSince my confidence level is 0.05 and my p-value of 0.0027 < 0.05, I can reject the null hypothesis that the mean weekly earnings for the population of women at the company is μ=$500.\r\n\r\n\r\n#Then I take the t-statistic result (-3) and the degrees of freedom by taking \"sample size - 1\" or (\"9\" - 1) = 8.\r\n\r\npval4a <- pt(-3, 8)\r\n\r\npval4a\r\n\r\n\r\n[1] 0.008535841\r\n\r\nB. For the alternative hypothesis Ha: μ < 500:\r\n\r\n\r\npval4b <- pt(-3, 8, lower.tail=FALSE)\r\n\r\npval4b\r\n\r\n\r\n[1] 0.9914642\r\n\r\nPer the hint, I can confirm that these are logical answers by adding the two probabilities together and confirming they equal 1:\r\n\r\n\r\npval4a+pval4b\r\n\r\n\r\n[1] 1\r\n\r\nQuestion 5\r\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000.\r\nJones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7.\r\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\r\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nAnswer 5\r\nA. To show the t-scores, I will use the test statistic [t = (ybar) - (μ) / (se)] given the results for each:\r\n\r\n\r\n#Test statistic for Jones:\r\n\r\nybar5a <- 519.5\r\nn5 <- 1000\r\na5 <- 500\r\nse5 <- 10.0\r\n\r\nt5a <-(ybar5a-a5)/(se5)\r\n\r\nt5a\r\n\r\n\r\n[1] 1.95\r\n\r\n#Test statistic for Smith:\r\n\r\nybar5b <- 519.7\r\nn5 <- 1000\r\na5 <- 500\r\nse5 <- 10.0\r\n\r\nt5b <-(ybar5b-a5)/(se5)\r\n\r\nt5b\r\n\r\n\r\n[1] 1.97\r\n\r\nTo show the p-values, I will use the pt() function and use the t-statistic results from each of the tests and the degrees of freedom by taking “sample size - 1” or (“100” - 1) = 999. I will need to multiply each result by 2 to account for the probabilities in each tail of the normal distribution.\r\n\r\n\r\n#For Jones' results:\r\n\r\npval5a <- pt(1.95, 999, lower.tail = FALSE) * 2\r\n\r\npval5a\r\n\r\n\r\n[1] 0.05145555\r\n\r\n#For Smith's results:\r\n\r\npval5b <- pt(1.97, 999, lower.tail = FALSE) * 2\r\n\r\npval5b\r\n\r\n\r\n[1] 0.04911426\r\n\r\nB. To use α = 0.05 and look at each result and whether it is “statistically significant”, I can compare the p-values directly to the confidence level of 0.5. Jones’ results gave a p-value of 0.5145, which is just over the threshold of the confidence level given of 0.5. Smith’s results gave a p-value of 0.4911, which is just under the threshold of the confidence level given of 0.5.\r\nHypothesis tests tell us that if the p-value < α, we reject H0 and if p-value ≥ α, we do not reject H0. Given this general statistical guidance, only Smith’s results would be considered “statistically significant”.\r\nC. The results in this example could be very misleading if only whether the results were reported as simply “rejecting” or “not rejecting” H0 or being “statistically significant” or not because that is leaving out vital information on how close the results were to the confidence level used. If the actual p-values were reported instead, they would reflect how marginally “significant” the results really were. The confidence is an artificial threshold that is subjectively applied, and in this case, the results were close enough that they should not be statistically reported as significally different. This is why we need to be sure we report the full p-values and confidence intervals.\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nAnswer\r\nTo answer whether there is enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents, we need to use a left-tailed t-test. We will use the sample data in the variable “gas_taxes” in the t.test function. We know that the t.test() function uses the 95% confidence interval as a default, but it also uses a two-sided test as the default, so we need to provide the alternative argument “less”, indicating a left-sided test.\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\nt.test(gas_taxes, alternative = \"less\")\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = 18.625, df = 17, p-value = 1\r\nalternative hypothesis: true mean is less than 0\r\n95 percent confidence interval:\r\n     -Inf 44.67946\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nThis t-test gives us a confidence interval of [inf - 44.67946]. Since the confidence interval includes amounts that are all less than 45 cents, we have enough evidence to conclude that, at that confidence level, the average tax was less than 45 cents.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-03-quantitative-data-analysis/quantitative-data-analysis_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-04-03T17:41:03-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-dacss-697e-post-2/",
    "title": "DACSS 697D Post 2",
    "description": "Assignment 2 for DACSS 697D Course 'Text as Data': \"Webscraping using APIs\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-02-19",
    "categories": [
      "text as data",
      "homework"
    ],
    "contents": "\r\nOverview of Weeks 3 and 4\r\nWeek 3 Learning Curve in Webscraping using CSS\r\nIn Week 3, working with the css selector has proven frustrating. After building confidence in using the CSS Diner tutorial, working through the colab tutorial was less empowering. First, because the SelectorGadget tool is not available to be used in Chrome when logged in to the UMass environment due to blocking of the Chrome Store. So in the tutorial, I was able to find where the “#tablepress-73” was by inspecting the page but I would not have known how to find that to import it into R unless I was given that information, so I clearly need to understand this process better.\r\nWeek 3 API Usage\r\nWeek 3 included a welcome introduction to scraping data using an api, and specifically using the New York Times api was exciting given I have a research project using New York Times data that I have been conducting manually using PDF versions of the articles being used. However, I tried using the material to use a different api - one from opensecrets.org. After getting the api key, I started trying to see if I could apply some of the skills learned in the lab to search for information on lobby groups and data related to lobby groups with a focus on veteran issues. However, I ran into issues with this api, so I returned to using the New York Times api for information involving a new research topic for this week’s blog. I hope to return to troubleshooting my issues with the opensecrets.org api later.\r\nWeek 4 Natural Language Processing\r\nWeek 4 materials in NLP are extremely valuable and interesting to me. Unfortunately, due to learning curve issues, frustration, and the simple volume of information I am learning at once this semester, I have just scratched the surface of the NLP skills we are learning in this blog post. I look forward to developing them further as the semester goes on.\r\nFrustration\r\nI could not get all of the demonstrative code from all of the notebook tutorials to work on my own RStudio. Specifically, I realized that simply installing Python was not enough, and through I read that I needed to install the Anaconda platform for NLP in the tutorial, I had no idea how to functionally execute this task. Given my absolute absence of knowledge of Python and anything related, this was a long day of trying to catch up with little success. After over 20 hours of messing around with various help sites and trial and error on this, I was referred to the ‘spacyr’ package and its’ documentation by a classmate. This helped immensely, and after successfully getting the Miniconda and dependent actions completed, I was able to move forward.\r\n\r\n\r\nlibrary(httr); library(jsonlite)\r\nlibrary(dplyr)\r\nlibrary(quanteda)\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(cleanNLP)\r\nlibrary(rvest)\r\n\r\n\r\n\r\nWork Content for Weeks 3 and 4\r\nTrouble Using The opensecrets.org API\r\nI know from the API documentation that the Committee on Veterans’ Affairs is listed with committee code “HVET” (House) as well as “SVET” (Senate). The House Armed Services committee is coded “HARM”.\r\nI am looking for data from the 116th U.S. Congress, convened on January 3, 2019 and ending on January 3, 2021.\r\nSo I start with “GET” and my api call for the House Committee on Veterans Affairs, but this is not working. After literally hours of troubleshooting, I realized my error was that I failed to put a (’) mark at the beginning and end of my api call. It is so numbingly obvious now that I’ve seen it. Coding is nothing if not time consuming and absolutely aggravating. I’m beginning to feel like a real coder after the last 2 days and hours of work over a silly apostrophe!\r\nAnyway, Back To The Project…\r\n\r\n\r\ncnlp_init_udpipe()\r\n\r\n\r\n\r\n\r\n\r\nHVET <- GET('https://www.opensecrets.org/api/?method=congCmteIndus&congno=116&indus=F10&cmte=HVET&apikey=3f183582e16c4fff025509c65828bfa4&output=json')\r\n\r\nnames(HVET)\r\n\r\n\r\n [1] \"url\"         \"status_code\" \"headers\"     \"all_headers\"\r\n [5] \"cookies\"     \"content\"     \"date\"        \"times\"      \r\n [9] \"request\"     \"handle\"     \r\n\r\n\r\n\r\n#HVET_r <- fromJSON(rawToChar(HVET$content))\r\n#names(HVET_r) \r\n#HVET_r$fault\r\n\r\n\r\n\r\nI continue to receive errors indicating a lexical error, then an invalid apikey. I am struggling to understand why my api key is not working here. Since I cannot do through another delve into troubleshooting and understand how this api has returned different results, I will instead go back to the New York Times api and pull my own text relevant to another new project.\r\nNew York Times Article Search for last 7 days on Ukraine\r\nFinally, I can move a bit further after successfully retrieving data using my NYT api:\r\n\r\n\r\nukraine <- GET('https://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=20220213&end_date=20220220&q=ukraine&api-key=GTp3efxVZiGO75Iox9uZJ8ZTjIMjDWsM')\r\n\r\nnames(ukraine)\r\n\r\n\r\n [1] \"url\"         \"status_code\" \"headers\"     \"all_headers\"\r\n [5] \"cookies\"     \"content\"     \"date\"        \"times\"      \r\n [9] \"request\"     \"handle\"     \r\n\r\nNow I can take the step of transforming JSON objects into R objects:\r\n\r\n\r\nukraine_r <- fromJSON(rawToChar(ukraine$content))\r\nnames(ukraine_r) \r\n\r\n\r\n[1] \"status\"    \"copyright\" \"response\" \r\n\r\nAnd look at the response and lead paragraph headers:\r\n\r\n\r\n#ukraine_r$response\r\n\r\n\r\n\r\n\r\n\r\nukraine_r$response$docs$lead_paragraph\r\n\r\n\r\n [1] \"Follow live coverage on Russiaâ\\200\\231s invasion of Ukraine.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \r\n [2] \"A day before Ukraine announced its Defense Ministry and banking servers had been hacked, our video team toured the countryâ\\200\\231s Cybercommand Center, where officials have been preparing for this scenario for years.\"                                                                                                                                                                                                                                                                                                                                                                                                   \r\n [3] \"WASHINGTON â\\200” When Wang Yi, Chinaâ\\200\\231s foreign minister, called on Saturday for talks to resolve the crisis in Europe, he said Ukraineâ\\200\\231s sovereignty should be â\\200œrespected and safeguardedâ\\200\\235 â\\200” but also sided with Russia in saying that NATO enlargement was destabilizing the continent.\"                                                                                                                                                                                                                                                                                                                       \r\n [4] \"MUNICH â\\200” When President Biden declared on Friday he was convinced President Vladimir V. Putin of Russia had decided to attack Ukraine â\\200œin the coming week, in the coming days,â\\200\\235 the skeptics among American allies suddenly fell quiet. Hours before, Mr. Biden had informed them that American intelligence agencies had just learned that the Kremlin had given the order for Russian military units to proceed with an invasion.\"                                                                                                                                                                              \r\n [5] \"Vladimir Putin may still order an invasion of Ukraine, as President Biden said yesterday. Putin has long been obsessed with Ukraine, viewing it as part of Russiaâ\\200\\231s immediate orbit. And more than 150,000 Russian troops remain ready to pour over the border if Putin gives the order.\"                                                                                                                                                                                                                                                                                                                             \r\n [6] \"As Russian troops are dispatched to Ukraineâ\\200\\231s borders, the threat of a major assault on the country continues to escalate. Ukrainian soldiers stand guard at possible points of invasion, including sections of irradiated zones near Chernobyl. The United States has sent troops to NATO countries and has pulled most diplomats from Kyiv, Ukraine and world leaders are in contact with President Vladimir V. Putin of Russia to try to negotiate peace in the region. In a one-hour phone call on Saturday, President Biden warned Mr. Putin that an invasion would result in â\\200œswift and severeâ\\200\\235 costs.\"     \r\n [7] \"MARIUPOL, Ukraine â\\200” Paramilitary groups are actively preparing for a Russian invasion near Ukraineâ\\200\\231s front line with Russia-backed separatists.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \r\n [8] \"Big American and European companies operating on the ground in Ukraine said Friday that they had contingency plans at the ready in case of a Russian invasion but so far had not ordered the relocation of employees.\"                                                                                                                                                                                                                                                                                                                                                                                                  \r\n [9] \"KYIV, Ukraineâ\\200” Pavlo Kaliuk, a freelance property broker in Ukraineâ\\200\\231s capital, used to sell and rent properties to clients from the United States, France, Germany and Israel. Then in November, when Russia first began posting troops along the countryâ\\200\\231s border, the deals quickly dried up.\"                                                                                                                                                                                                                                                                                                                  \r\n[10] \"The New York Times traveled with a paramilitary group that says it refuses to leave the front lines in Ukraineâ\\200\\231s war in the east. But the government denies that theyâ\\200\\231re even there. What role can they have in a larger conflict with Russia?\"                                                                                                                                                                                                                                                                                                                                                                     \r\n\r\nAnd take some of the aspects of this data that are interesting to me and create a tibble:\r\n\r\n\r\nukraine_t <- as_tibble(cbind(\r\n          date=ukraine_r$response$docs$pub_date,\r\n          abstract=ukraine_r$response$docs$abstract,\r\n          lead=ukraine_r$response$docs$lead_paragraph)\r\n)\r\n\r\nukraine_t\r\n\r\n\r\n# A tibble: 10 x 3\r\n   date         abstract                     lead                     \r\n   <chr>        <chr>                        <chr>                    \r\n 1 2022-02-19T~ Hereâ€™s why Ukraineâ€™s im~ \"Follow live coverage on~\r\n 2 2022-02-16T~ A day before Ukraine announ~ \"A day before Ukraine an~\r\n 3 2022-02-20T~ The Biden administration pl~ \"WASHINGTON â€” When Wan~\r\n 4 2022-02-20T~ President Vladimir V. Putin~ \"MUNICH â€” When Preside~\r\n 5 2022-02-16T~ Three explanations for the ~ \"Vladimir Putin may stil~\r\n 6 2022-02-15T~ Andrew E. Kramer, a Times M~ \"As Russian troops are d~\r\n 7 2022-02-19T~ Times journalists followed ~ \"MARIUPOL, Ukraine â€” P~\r\n 8 2022-02-18T~ Despite warnings by Western~ \"Big American and Europe~\r\n 9 2022-02-18T~ Flights have been canceled,~ \"KYIV, Ukraineâ€” Pavlo ~\r\n10 2022-02-19T~ The New York Times traveled~ \"The New York Times trav~\r\n\r\nFinally, I can utilize the cleanNLP package:\r\n\r\n\r\ncnlp_init_udpipe()\r\n\r\nannotated <- cnlp_annotate(ukraine_t$lead)\r\n\r\n\r\nProcessed document 10 of 10\r\n\r\nhead(annotated)\r\n\r\n\r\n$token\r\n# A tibble: 501 x 11\r\n   doc_id   sid tid   token  token_with_ws  lemma upos  xpos  feats   \r\n *  <int> <int> <chr> <chr>  <chr>          <chr> <chr> <chr> <chr>   \r\n 1      1     1 1     Follow \"Follow \"      foll~ VERB  VB    Mood=Im~\r\n 2      1     1 2     live   \"live \"        live  ADJ   JJ    Degree=~\r\n 3      1     1 3     cover~ \"coverage \"    cove~ NOUN  NN    Number=~\r\n 4      1     1 4     on     \"on \"          on    ADP   IN    <NA>    \r\n 5      1     1 5     Russi~ \"Russiaâ€™s \" Russ~ PROPN NNP   Number=~\r\n 6      1     1 6     invas~ \"invasion \"    inva~ NOUN  NN    Number=~\r\n 7      1     1 7     of     \"of \"          of    ADP   IN    <NA>    \r\n 8      1     1 8     Ukrai~ \"Ukraine\"      Ukra~ PROPN NNP   Number=~\r\n 9      1     1 9     .      \".\"            .     PUNCT .     <NA>    \r\n10      2     1 1     A      \"A \"           a     DET   DT    Definit~\r\n# ... with 491 more rows, and 2 more variables: tid_source <chr>,\r\n#   relation <chr>\r\n\r\n$document\r\n   doc_id\r\n1       1\r\n2       2\r\n3       3\r\n4       4\r\n5       5\r\n6       6\r\n7       7\r\n8       8\r\n9       9\r\n10     10\r\n\r\nand the spacyr package:\r\n\r\n\r\nlibrary(spacyr)\r\n\r\nspacy_initialize(model=\"en_core_web_sm\")\r\nukraine_parsed <- spacy_parse(ukraine_t$lead, tag=TRUE, nounphrase=TRUE, entity=TRUE, lemma=TRUE)\r\n\r\nhead(ukraine_parsed)\r\n\r\n\r\n  doc_id sentence_id token_id    token    lemma   pos tag entity\r\n1  text1           1        1   Follow   follow  VERB  VB       \r\n2  text1           1        2     live     live   ADJ  JJ       \r\n3  text1           1        3 coverage coverage  NOUN  NN       \r\n4  text1           1        4       on       on   ADP  IN       \r\n5  text1           1        5 Russiaâ\\200 Russiaâ\\200 PROPN NNP       \r\n6  text1           1        6        \\231        \\231 PROPN NNP       \r\n  nounphrase whitespace\r\n1                  TRUE\r\n2        beg       TRUE\r\n3   end_root       TRUE\r\n4                  TRUE\r\n5        beg      FALSE\r\n6        mid      FALSE\r\n\r\nAnnotating Data\r\n\r\n\r\nukraine_anno <- left_join(annotated$document, annotated$token, by = \"doc_id\")\r\nhead(ukraine_anno)\r\n\r\n\r\n  doc_id sid tid      token token_with_ws      lemma  upos xpos\r\n1      1   1   1     Follow       Follow      follow  VERB   VB\r\n2      1   1   2       live         live        live   ADJ   JJ\r\n3      1   1   3   coverage     coverage    coverage  NOUN   NN\r\n4      1   1   4         on           on          on   ADP   IN\r\n5      1   1   5 Russiaâ\\200\\231s   Russiaâ\\200\\231s  Russiaâ\\200\\231s PROPN  NNP\r\n6      1   1   6   invasion     invasion    invasion  NOUN   NN\r\n                  feats tid_source relation\r\n1 Mood=Imp|VerbForm=Fin          0     root\r\n2            Degree=Pos          3     amod\r\n3           Number=Sing          1      obj\r\n4                  <NA>          6     case\r\n5           Number=Sing          6 compound\r\n6           Number=Sing          1      obl\r\n\r\nAnd finally, beginning to summarize data from the text.\r\n\r\n\r\nlibrary(magrittr)\r\n\r\nnouns <- ukraine_anno %>% \r\n  filter(upos == \"NOUN\") %>%\r\n  group_by(token) %>% \r\n  summarize(count = n()) %>%\r\n  arrange(desc(count))\r\n\r\nadjs <- ukraine_anno %>% \r\n  filter(upos == \"ADJ\") %>%\r\n  group_by(token) %>% \r\n  summarize(count = n()) %>%\r\n  arrange(desc(count))\r\n\r\npropns <- ukraine_anno %>% \r\n  filter(upos == \"PROPN\") %>%\r\n  group_by(token) %>% \r\n  summarize(count = n()) %>%\r\n  arrange(desc(count))\r\n\r\nverbs <- ukraine_anno %>% \r\n  filter(upos == \"VERB\") %>%\r\n  group_by(token) %>% \r\n  summarize(count = n()) %>%\r\n  arrange(desc(count))\r\n\r\nhead(nouns)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  token          count\r\n  <chr>          <int>\r\n1 \"invasion\"         7\r\n2 \"â\"                4\r\n3 \"troops\"           4\r\n4 \"\\u009d\"           2\r\n5 \"border\"           2\r\n6 \"countryâ€™s\"     2\r\n\r\nhead(adjs)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  token     count\r\n  <chr>     <int>\r\n1 Russian       5\r\n2 American      3\r\n3 front         2\r\n4 ready         2\r\n5 Big           1\r\n6 convinced     1\r\n\r\nhead(propns)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  token        count\r\n  <chr>        <int>\r\n1 Ukraine          8\r\n2 Putin            6\r\n3 Russia           6\r\n4 President        5\r\n5 Ukraineâ€™s     5\r\n6 Biden            4\r\n\r\nhead(verbs)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  token        count\r\n  <chr>        <int>\r\n1 said             3\r\n2 coming           2\r\n3 preparing        2\r\n4 â€œrespected     1\r\n5 announced        1\r\n6 attack           1\r\n\r\nClean-Up Needed\r\nThe characters that carried over from the NYT api import are not easily cleaned up, though I have spent some time getting familiar with some of tools to do so. I need to spend more time getting familiar with the best options for this.\r\nLooking For a Word\r\nI scratched the surface of the stringr function and looked at all of the lead paragraphs from the last week to see if there was any variation of the word “protest(s)”. Unfortunately, I did not find any instances.\r\n\r\n\r\nlibrary(stringr)\r\n\r\nstr_match(ukraine_t$lead, \" [P|p]rotest[s] \")\r\n\r\n\r\n      [,1]\r\n [1,] NA  \r\n [2,] NA  \r\n [3,] NA  \r\n [4,] NA  \r\n [5,] NA  \r\n [6,] NA  \r\n [7,] NA  \r\n [8,] NA  \r\n [9,] NA  \r\n[10,] NA  \r\n\r\nCreating a Wordcloud\r\nSince I had so much background work to do this week and wanted to end up creating something visually, I wanted to start with a wordcloud. Although there is a clear need to clean up the characters, I feel like I have covered a lot of ground in the last couple of weeks. I look forward to putting it together in a more purposeful way.\r\n\r\n\r\nlibrary(wordcloud)\r\nlibrary(RColorBrewer)\r\nlibrary(tm)\r\n\r\nukraine_words <- merge(merge(merge(\r\n  nouns,\r\n  adjs, all = TRUE),\r\n  propns, all = TRUE),\r\n  verbs, all = TRUE)\r\n\r\nset.seed(1234)\r\n\r\nwordcloud(words = ukraine_words$token, freq = ukraine_words$count, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-03-dacss-697e-post-2/distill-preview.png",
    "last_modified": "2022-04-05T01:16:13-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-dacss-697e-post-1/",
    "title": "DACSS 697D Post 1",
    "description": "Assignment 1 for DACSS 697D Course 'Text as Data'",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-02-06",
    "categories": [
      "text as data",
      "homework"
    ],
    "contents": "\r\nResearch Interests\r\nMy primary research interests lie in the area of peace & conflict studies. I am specifically interested in where how conflict is portrayed within media and society.\r\nMedia Coverage of Afghanistan\r\nOne of the research questions I have been working on in a previous course would likely translate easily to the context of this course. We looked at how the U.S. military withdrawal from Afghanistan was portrayed in the media through stratified samples of articles from both the New York Times and the Wall Street Journal. In addition to comparing sources, framing, and word usage between the two publications, there was also a time element to be examined. We began comparing the way the withdrawal was covered differently between the time period between when the withdrawal was agreed on between former President Trump and the Taliban - as compared to the time period after Joe Biden was elected. The difference in coverage between administrations is one variable that emerged as a path I would like to explore further. Specifically, I would like to examine how U.S. combat veterans were framed within media portrayals of the withdrawal from Afghanistan and how that framing changed between Presidential administrations.\r\nCombat Veteran Narratives\r\nAnother issue I am interested in exploring is in how military veterans’ narratives can contribute potential insights and inform the field of peace and conflict studies. This is less likely to be something I will explore through text as data than the former option.\r\nSocial Movements\r\nThere is another research area I am interested that has the potential to be explored through text as data methods. I am interested in nonviolent activism as a whole; specifically, I have been looking at how combat veterans interact with the social movements of protest. Potential ways this could be explored are through research into political action groups through sources such as opensecrets.org or reports in various media and social media sources featuring veterans as part of organized nonviolent protests as well as engagement in individual actions.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-09T12:19:21-05:00",
    "input_file": "dacss-697e-post-1.knit.md"
  },
  {
    "path": "posts/2022-04-03-networks/",
    "title": "DACSS 697E Assignment 2",
    "description": "Intro Blog Post Assignment for DACSS 697E course 'Social and Political Network Analysis': \"Analyzing the Enron Emails Dataset From the Network Package\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-02-04",
    "categories": [
      "networks",
      "homework"
    ],
    "contents": "\r\n“Enron Emails.R” is a file in the course repository that consists of a network of emails between enron employees from the igraphdata package. According to the import script, this is a large, un-weighted, directed network with employees as nodes and emails as edges.\r\nThe import script also indicates that there are no node attributes. I found that there was, in fact, node attributes to be found in the igraph dataset in the form of what seems to be titles as ‘Notes’ and email addresses in ‘email’ without the domain name, but that was not relevant for this assignment.\r\nAdditionally, the import script indicated that but topic and time information is stored as edge attributes. This is correct, and another thing that I learned through working on this assignment about the dataset is that there is a topical dataset directory in the LDC details data frame that serves as a codebook for the topic codes assembled in the edgelist, for future reference.\r\nThe import script has created three objects that represent the network: network_edgelist (a data frame of an edge list and edge attributes), network_igraph (an igraph object), and network_statnet (a network object compatible with statnet packages like sna & ergm).\r\nWith that contextual introduction, I’ll go back to the start, and execute the import script. I also look at the R Documentation to view the detailed information on this data set via: enron {igraphdata}\r\nI load the libraries for statnet, igraph, and igraphdata\r\nNext, I read the data into the environment. This imports the data as an adjacency matrix\r\n\r\n\r\ndata(\"enron\", package = \"igraphdata\")\r\nnetwork_igraph <- enron\r\nrm(enron)\r\n\r\n\r\n\r\nThen, I create the edgelist\r\n\r\n\r\nnetwork_edgelist <- as.data.frame(as_edgelist(network_igraph))\r\n\r\n\r\n\r\nand add edge attributes to the edge list\r\n\r\n\r\nnetwork_edgelist <-cbind(network_edgelist, Time      = E(network_igraph)$Time, \r\n                                               Reciptype = E(network_igraph)$Reciptype, \r\n                                               Topic     = E(network_igraph)$Topic, \r\n                                               LDC_topic = E(network_igraph)$LDC_topic)\r\n\r\n\r\n\r\nThis collects details about the attribute “LDC Details” into a data frame\r\n\r\n\r\nLDC_details <- data.frame(LDC_topic_name = network_igraph$LDC_names, LDC_topic_desc = network_igraph$LDC_desc, LDC_topic = 1:32)\r\n\r\n\r\n\r\nThe data frame can then be added as details to the edge list\r\n\r\n\r\nnetwork_edgelist <- merge(network_edgelist, LDC_details, by = 'LDC_topic', all.x = TRUE)\r\n\r\n\r\n\r\nand then re-ordered within the edge list\r\n\r\n\r\nnetwork_edgelist <- network_edgelist[c(2:5,1,6,7)]\r\n\r\n\r\n\r\nNow I can create a statnet network object from our edge list\r\n\r\n\r\nnetwork_statnet <- network(as.matrix(network_edgelist[1:2]), matrix.type = \"edgelist\", directed = TRUE)\r\n\r\n\r\n\r\nand add attributes to the statnet network object\r\n\r\n\r\nnetwork_statnet%e%'Time' <- as.character(network_edgelist$Time)\r\nnetwork_statnet%e%'Reciptype' <- as.character(network_edgelist$Reciptype)\r\nnetwork_statnet%e%'Topic' <- as.character(network_edgelist$Topic)\r\nnetwork_statnet%e%'LDC_topic' <- as.character(network_edgelist$LDC_topic)\r\nnetwork_statnet%e%'LDC_topic_name' <- as.character(network_edgelist$LDC_topic_name)\r\nnetwork_statnet%e%'LDC_topic_desc' <- as.character(network_edgelist$LDC_topic_desc)\r\n\r\n\r\n\r\nFinally, I can clean up and remove any unnecessary objects if I no longer need the details as a reference, as in this assignment.\r\n\r\n\r\nrm(LDC_details)\r\n\r\n\r\n\r\nNow, I’ll take a first look at the network\r\n\r\n\r\nplot(network_statnet)\r\n\r\n\r\n\r\n\r\nThat’s interesting, but doesn’t tell me much about the network yet except that I may expect to see 2 isolates.\r\nUsing tools to inspect the network data and confirm the objects created through the import script are present\r\n\r\n\r\nls()\r\n\r\n\r\n[1] \"network_edgelist\" \"network_igraph\"   \"network_statnet\" \r\n\r\nI’ll inspect vertices and edges using commands in both igraph and statnet\r\n\r\n\r\nvcount(network_igraph)\r\n\r\n\r\n[1] 184\r\n\r\necount(network_igraph)\r\n\r\n\r\n[1] 125409\r\n\r\nprint(network_statnet) \r\n\r\n\r\n Network attributes:\r\n  vertices = 184 \r\n  directed = TRUE \r\n  hyper = FALSE \r\n  loops = FALSE \r\n  multiple = FALSE \r\n  bipartite = FALSE \r\n  total edges= 3010 \r\n    missing edges= 0 \r\n    non-missing edges= 3010 \r\n\r\n Vertex attribute names: \r\n    vertex.names \r\n\r\n Edge attribute names not shown \r\n\r\nThere is quite a difference between the number of edges in the igraph network (123,409) and the statnet network (3010), which leads me to believe there is something a bit off with the way the data was processed between the two network programs.\r\nLooking at more comparisons in the two network files, I can look at the network features.\r\n\r\n\r\nis_bipartite(network_igraph)\r\n\r\n\r\n[1] FALSE\r\n\r\nis_directed(network_igraph)\r\n\r\n\r\n[1] TRUE\r\n\r\nis_weighted(network_igraph)\r\n\r\n\r\n[1] FALSE\r\n\r\nvertex_attr_names(network_igraph)\r\n\r\n\r\n[1] \"Email\" \"Name\"  \"Note\" \r\n\r\nedge_attr_names(network_igraph)\r\n\r\n\r\n[1] \"Time\"      \"Reciptype\" \"Topic\"     \"LDC_topic\"\r\n\r\nLooking at the same features of the statnet network with the appropriate commands\r\n\r\n\r\nprint(network_statnet)\r\n\r\n\r\n Network attributes:\r\n  vertices = 184 \r\n  directed = TRUE \r\n  hyper = FALSE \r\n  loops = FALSE \r\n  multiple = FALSE \r\n  bipartite = FALSE \r\n  total edges= 3010 \r\n    missing edges= 0 \r\n    non-missing edges= 3010 \r\n\r\n Vertex attribute names: \r\n    vertex.names \r\n\r\n Edge attribute names not shown \r\n\r\nnetwork::list.vertex.attributes(network_statnet)\r\n\r\n\r\n[1] \"na\"           \"vertex.names\"\r\n\r\nnetwork::list.edge.attributes(network_statnet)\r\n\r\n\r\n[1] \"LDC_topic\"      \"LDC_topic_desc\" \"LDC_topic_name\"\r\n[4] \"na\"             \"Reciptype\"      \"Time\"          \r\n[7] \"Topic\"         \r\n\r\nUsing more tools to inspect the network data:\r\n\r\n\r\n#List network attributes: igraph\r\n\r\nigraph::vertex_attr_names(network_igraph)\r\n\r\n\r\n[1] \"Email\" \"Name\"  \"Note\" \r\n\r\nigraph::edge_attr_names(network_igraph)\r\n\r\n\r\n[1] \"Time\"      \"Reciptype\" \"Topic\"     \"LDC_topic\"\r\n\r\n#List network attributes: statnet\r\n\r\nnetwork::list.vertex.attributes(network_statnet)\r\n\r\n\r\n[1] \"na\"           \"vertex.names\"\r\n\r\nnetwork::list.edge.attributes(network_statnet)\r\n\r\n\r\n[1] \"LDC_topic\"      \"LDC_topic_desc\" \"LDC_topic_name\"\r\n[4] \"na\"             \"Reciptype\"      \"Time\"          \r\n[7] \"Topic\"         \r\n\r\nI want to look at specific attribute data. First using igraph\r\n\r\n\r\nhead(V(network_igraph)$Email)\r\n\r\n\r\n[1] \"albert.meyers\" \"a..martin\"     \"andrea.ring\"   \"andrew.lewis\" \r\n[5] \"andy.zipper\"   \"a..shankman\"  \r\n\r\nhead(V(network_igraph)$Name)\r\n\r\n\r\n[1] \"Albert Meyers\"    \"Thomas Martin\"    \"Andrea Ring\"     \r\n[4] \"Andrew Lewis\"     \"Andy Zipper\"      \"Jeffrey Shankman\"\r\n\r\nhead(V(network_igraph)$Note)\r\n\r\n\r\n[1] \"Employee, Specialist\"         \"Vice President\"              \r\n[3] \"NA\"                           \"Director\"                    \r\n[5] \"Vice President, Enron Online\" \"President, Enron Global Mkts\"\r\n\r\nhead(E(network_igraph)$Time)\r\n\r\n\r\n[1] \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\"\r\n[4] \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\"\r\n\r\nhead(E(network_igraph)$Reciptype)\r\n\r\n\r\n[1] \"to\"  \"to\"  \"cc\"  \"cc\"  \"bcc\" \"bcc\"\r\n\r\nhead(E(network_igraph)$Topic)\r\n\r\n\r\n[1] 1 1 3 3 3 3\r\n\r\nhead(E(network_igraph)$LDC_topic)\r\n\r\n\r\n[1]  0 -1 -1 -1 -1 -1\r\n\r\nNext, using statnet\r\n\r\n\r\nhead(network_statnet %v% \"na\")\r\n\r\n\r\n[1] FALSE FALSE FALSE FALSE FALSE FALSE\r\n\r\nnetwork_statnet %v% \"vertex.names\"\r\n\r\n\r\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\r\n [17]  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32\r\n [33]  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48\r\n [49]  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64\r\n [65]  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80\r\n [81]  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\r\n [97]  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112\r\n[113] 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\r\n[129] 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\r\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\r\n[161] 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176\r\n[177] 177 178 179 180 181 182 183 184\r\n\r\nhead(network_statnet %e% \"LDC_topic\")\r\n\r\n\r\n[1] \"-1\" \"-1\" \"-1\" \"-1\" \"-1\" \"-1\"\r\n\r\nhead(network_statnet %e% \"LDC_topic_desc\")\r\n\r\n\r\n[1] NA NA NA NA NA NA\r\n\r\nhead(network_statnet %e% \"LDC_topic_name\")\r\n\r\n\r\n[1] NA NA NA NA NA NA\r\n\r\nhead(network_statnet %e% \"na\")\r\n\r\n\r\n[1] FALSE FALSE FALSE FALSE FALSE FALSE\r\n\r\nhead(network_statnet %e% \"Reciptype\")\r\n\r\n\r\n[1] \"to\"  \"cc\"  \"cc\"  \"bcc\" \"bcc\" \"to\" \r\n\r\nhead(network_statnet %e% \"Time\")\r\n\r\n\r\n[1] \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\"\r\n[4] \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\" \"1979-12-31 21:00:00\"\r\n\r\nhead(network_statnet %e% \"Topic\")\r\n\r\n\r\n[1] \"1\" \"3\" \"3\" \"3\" \"3\" \"3\"\r\n\r\nClearly, there are differences in how the vertices are represented in igraph v. statnet. For example, the anonymized names are node attributes in igraph, but in statnet they are represented by numbers.\r\nNext, I want to look at the dyad census in igraph\r\n\r\n\r\nigraph::dyad.census(network_igraph)\r\n\r\n\r\n$mut\r\n[1] 30600\r\n\r\n$asym\r\n[1] 64208\r\n\r\n$null\r\n[1] -77972\r\n\r\nand in statnet\r\n\r\n\r\nsna::dyad.census(network_statnet)\r\n\r\n\r\n     Mut Asym  Null\r\n[1,] 913 1184 14739\r\n\r\nThe dyad census clearly gives vastly different responses between the two programs, but I am not sure how or why they are represented so differently yet.\r\nNext I’ll look at the triad census in igraph\r\n\r\n\r\nigraph::triad.census(network_igraph)\r\n\r\n\r\n [1] 700234  19530 249694   8409   2695   5176   7060  13227   1180\r\n[10]     59   6781   1023   1137    786   2782   1611\r\n\r\nand in statnet\r\n\r\n\r\nsna::triad.census(network_statnet)\r\n\r\n\r\n        003    012    102 021D 021U 021C 111D  111U 030T 030C  201\r\n[1,] 700234 150250 118974 8409 2695 5176 7060 13227 1180   59 6781\r\n     120D 120U 120C  210  300\r\n[1,] 1023 1137  786 2782 1611\r\n\r\nIf I use the igraph data, the enron network has 184 vertices, so if I want to see if the triad census is working correctly, I want to compare the data:\r\n\r\n\r\n#possible triads in network\r\n184*183*182/6\r\n\r\n\r\n[1] 1021384\r\n\r\nsum(igraph::triad.census(network_igraph))\r\n\r\n\r\n[1] 1021384\r\n\r\nSimilarly, if I use the statnet data, the enron network has 184 vertices, so if I want to see if the triad census is working correctly, I want to compare the data:\r\n\r\n\r\n#possible triads in network\r\n184*183*182/6\r\n\r\n\r\n[1] 1021384\r\n\r\nsum(sna::triad.census(network_statnet))\r\n\r\n\r\n[1] 1021384\r\n\r\nNow I’m getting somewhere! I don’t yet know exactly how the triad census informs my interpretations fully, but I know it is accurately being represented in this area of network analysis.\r\nLooking next at the global transitivity in statnet:\r\n\r\n\r\ngtrans(network_statnet)\r\n\r\n\r\n[1] 0.3580924\r\n\r\nLooking next at the network transitivity in igraph:\r\n\r\n\r\ntransitivity(network_igraph)\r\n\r\n\r\n[1] 0.3725138\r\n\r\nThey are not the same, but not completely out of the realm of reasonable differences given the different algorithms each program uses.\r\nLooking next at the ego transitivity for the employee names that appeared in the header of the igraph node information, but I cannot get the command to run which would give me the local transitivity for specific nodes, for some reason I will need to take more time to explore.\r\n#transitivity(network_igraph, type=“local”, vids=V(network_igraph)[c(“Albert Meyers”, “Thomas Martin:, Andrea Ring”, “Andrew Lewis”, “Andy Zipper”, “Jeffrey Shankman”)])\r\nHowevwer, I can look at global v. average local transitivity\r\n\r\n\r\ntransitivity(network_igraph, type=\"global\")\r\n\r\n\r\n[1] 0.3725138\r\n\r\ntransitivity(network_igraph, type=\"average\")\r\n\r\n\r\n[1] 0.5055302\r\n\r\nThis transitivity tells me that the average network transitivity is significantly higher than the global transitivity, indicating, from my still naive network knowledge, that the overall network is generally more loose, and that there is a more connected sub-network.\r\nLooking at the geodesic distance:\r\n\r\n\r\naverage.path.length(network_igraph,directed=T)\r\n\r\n\r\n[1] 2.390464\r\n\r\nThis tells me that on average, the path length is just over 2.\r\nGetting to look at the components of the network in igraph:\r\n\r\n\r\nnames(igraph::components(network_igraph))\r\n\r\n\r\n[1] \"membership\" \"csize\"      \"no\"        \r\n\r\nigraph::components(network_igraph)$no \r\n\r\n\r\n[1] 3\r\n\r\nigraph::components(network_igraph)$csize\r\n\r\n\r\n[1] 182   1   1\r\n\r\nIt shows that there are 3 components in the network, and 182 of the 182 nodes make up the giant component with 2 isolates.\r\nFinally, I get my answer on isolates.\r\n\r\n\r\nisolates(network_statnet)\r\n\r\n\r\n[1]  72 118\r\n\r\nSince I know that the nodes are Enron employees and they are assigned numbers in the statnet network, running the isolate command tells me that employee #72 and #118 are indeed the 2 isolates viewed in the initial graphic representation of the network.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-03-networks/networks_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2022-04-03T23:08:22-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-welcome-to-kristinas-blog/",
    "title": "Welcome to Kristina's Blog",
    "description": "Detailing my academic projects in the course of pursuing a M.S. in Data Science and Computation for Social Sciences at UMass Amherst",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2021-07-12",
    "categories": [
      "personal"
    ],
    "contents": "\r\nHi! I’m Kristina Becvar, a graduate student in the Department of Social and Behavioral Sciences at University of Massachusetts Amherst. I’ve decided to start this blog to document my learning and development in data analysis.\r\nIt’s not news to anyone that coding is difficult, and it’s somewhat above and beyond my time constraints right now to learn more in order to build this blog. But I think (!) I finally got this thing going.\r\nThank you to Steve Linberg for your guidance getting the process of setting up a distill site through RStudio and for generally being a fabulous classmate!\r\nFeel free to pull up a chair, leave a comment, and join me so that we can navigate some things together.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-03T17:34:16-05:00",
    "input_file": {}
  }
]
