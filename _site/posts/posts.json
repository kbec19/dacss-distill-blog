[
  {
    "path": "posts/2022-04-03-dacss-603-assignment-3/",
    "title": "DACSS 603 Assignment 3",
    "description": "Assignment 3 for DACSS 603 course 'Quantitative Data Analysis': \"Multiple Regression\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-03-31",
    "categories": [
      "statistics",
      "homework"
    ],
    "contents": "\r\nQuestion 1\r\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is “ŷ = −10,536 + 53.8x1 + 2.84x2”.\r\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\r\n\r\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\r\n\r\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\r\n\r\nResponse 1\r\nA: Using the prediction equation, the predicted selling price of this home would be $107,296. Since the home sold for $145,000, the residual is $37,704. I interpret this result to mean that the home seller was able to get a much better price for their home than the market prediction.\r\n\r\n\r\nShow code\r\n\r\nx1 <- 1240 #square feet of the home\r\nx2 <- 18000 #square feet of the lot\r\ny <- 145000 #selling price of the home\r\n\r\n# using the prediction equation here:\r\n\r\nybar1 <- (-10536)+(53.8*x1)+(2.84*x2)\r\n\r\nresidual1 <- y-ybar1\r\n\r\nybar1\r\n\r\n\r\n[1] 107296\r\n\r\nShow code\r\n\r\nresidual1\r\n\r\n\r\n[1] 37704\r\n\r\nB: The explanatory variable “square feet of the home” has a slope coefficient of 53.8. This means that for every increase in that variable, the predicted price of the home will increase by $53.80. I can confirm that this is the case by changing my calculations accordingly:\r\n\r\n\r\nShow code\r\n\r\nx1b <- 1241 #square feet of the home\r\nx2b <- 18000 #square feet of the lot\r\nyb <- 145000 #selling price of the home\r\n\r\nybar1b <- (-10536)+(53.8*x1b)+(2.84*x2b)\r\n\r\nresidual1b <- yb-ybar1b\r\n\r\nybar1b-ybar1 #difference in predicted home price given increase in quare feet of the home by \"1\"\r\n\r\n\r\n[1] 53.8\r\n\r\nC: The explanatory variable “square feet of the lot” has a slope coefficient of 2.84. This means that for every increase in that variable, the predicted price of the home will increase by $2.84. I can then calculate that it would take an increase of ~18.94 in lot size to have a proportionate increase in price to an increase of one square foot in home size.\r\n\r\n\r\nShow code\r\n\r\nneed <- 53.8/2.84\r\n\r\nneed\r\n\r\n\r\n[1] 18.94366\r\n\r\nQuestion 2\r\nThe data file (alr4 R Package) concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\r\nResponse 2\r\nFirst I’m loading the “salary” data and inspecting it.\r\n\r\n\r\nShow code\r\n\r\ndata(\"salary\") \r\ndim(salary)\r\n\r\n\r\n[1] 52  6\r\n\r\nShow code\r\n\r\nhead(salary)\r\n\r\n\r\n   degree rank    sex year ysdeg salary\r\n1 Masters Prof   Male   25    35  36350\r\n2 Masters Prof   Male   13    22  35350\r\n3 Masters Prof   Male   10    23  28200\r\n4 Masters Prof Female    7    27  26775\r\n5     PhD Prof   Male   19    30  33696\r\n6 Masters Prof   Male   16    21  28516\r\n\r\nPart A.\r\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\r\nClarifying the null and alternative hypotheses:\r\nH0: The mean salary for men and women is the same, without regard to any other variable but sex.\r\nHa: The mean salary for men and women is NOT the same, without regard to any other variable but sex.\r\nWe have n > 30, so I can assume a normal distribution.\r\nThe t-test result indicates that the mean salary for men and women is not equal ($24,696.79 for men, $21,357.14 for women). However, the p-value is 0.0706. Since this p-value indicates a significance level of % (p > 0.05), I fail to reject the null hypothesis.\r\n\r\n\r\nShow code\r\n\r\n# Conduct t.test to determine confidence level at default of 0.95\r\nt.test(salary~sex, data = salary, var.equal = TRUE)\r\n\r\n\r\n\r\n    Two Sample t-test\r\n\r\ndata:  salary by sex\r\nt = 1.8474, df = 50, p-value = 0.0706\r\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\r\n95 percent confidence interval:\r\n -291.257 6970.550\r\nsample estimates:\r\n  mean in group Male mean in group Female \r\n            24696.79             21357.14 \r\n\r\nPart B.\r\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\r\nI ran the multiple linear regression using the “lm()” function using all predictors and salary as the outcome variable. Then, the function “confint()” produced the corresponding confidence intervals from the model. This produced a confidence interval, at the default of 0.95, that the difference in salary between males and females is [(-$697.82) to ($3,030.56)]\r\n\r\n\r\nShow code\r\n\r\n#Linear regression on all predictors\r\n\r\nmlm2 <- lm(salary~., data = salary)\r\n\r\nsummary(mlm2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ ., data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\r\ndegreePhD    1388.61    1018.75   1.363    0.180    \r\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\r\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\r\nsexFemale    1166.37     925.57   1.260    0.214    \r\nyear          476.31      94.91   5.018 8.65e-06 ***\r\nysdeg        -124.57      77.49  -1.608    0.115    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\r\n\r\nShow code\r\n\r\nconfint(mlm2, \"sexFemale\")\r\n\r\n\r\n              2.5 %   97.5 %\r\nsexFemale -697.8183 3030.565\r\n\r\nPart C.\r\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variable\r\nI can look at these relationships looking at both the output from the “lm()” function in Part B and the “confint()” function. However, I also found a great solution to represent these relationships using the “broom” package. Using the function “tidy()” from the “broom” package, I can create a tibble from the results of my “lm()” call.\r\n\r\n\r\nShow code\r\n\r\n#Manipulate the output tibble and convert scientific notation of p-value to decimals\r\n\r\ntidymlm2 <- tidy(mlm2, conf.int = TRUE)\r\n\r\noptions(scipen = 999)\r\n\r\ntidymlm2\r\n\r\n\r\n# A tibble: 7 x 7\r\n  term        estimate std.error statistic  p.value conf.low conf.high\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\r\n1 (Intercept)   15746.     800.      19.7  9.76e-24   14134.   17358. \r\n2 degreePhD      1389.    1019.       1.36 1.80e- 1    -663.    3440. \r\n3 rankAssoc      5292.    1145.       4.62 3.22e- 5    2985.    7599. \r\n4 rankProf      11119.    1352.       8.23 1.62e-10    8396.   13841. \r\n5 sexFemale      1166.     926.       1.26 2.14e- 1    -698.    3031. \r\n6 year            476.      94.9      5.02 8.65e- 6     285.     667. \r\n7 ysdeg          -125.      77.5     -1.61 1.15e- 1    -281.      31.5\r\n\r\nReviewing this information I can see:\r\nFor the predictor variable “degreePhD”, the statistical significance of its’ relationship to salary is an increase in salary of ~$1,388.61 given a PhD. The p-value of 0.18 indicates that this is not statistically significant.\r\nFor the predictor variable “rankAssoc”, the statistical significance of its’ relationship to salary is an increase in salary of ~$5,292.36 given achieving the rank of Associate. The p-value of 0.0000322 indicates that this result is statistically significant to the 0.99 confidence level.\r\nFor the predictor variable “rankProf”, the statistical significance of its’ relationship to salary is an increase in salary of ~$11,118.76.36 given achieving the rank of Professor. The p-value of 0.000000000162 indicates that this result is statistically significant to the 0.99 confidence level.\r\nFor the predictor variable “sexFemale”, the statistical significance of its’ relationship to salary is an increase in salary of ~$1,166.37 given the salary being for a female. The p-value of 0.214 indicates that this result is not statistically significant.\r\nFor the predictor variable “year”, the statistical significance of its’ relationship to salary is an increase in salary of ~$476.31 given the salary for each year of experience in current rank. The p-value of 0.00000865 indicates that this result is statistically significant.\r\nFor the predictor variable “ysdeg”, the statistical significance of its’ relationship to salary is a decrease in salary of ~$124.57 given the salary for each year since highest degree achieved. The p-value of 0.12 indicates that this result is not statistically significant.\r\nSummarizing, the predictor variables, “degreePhD”, “sexFemale”, and “ysdeg” are not statistically significant, while predictor variables “rankAssoc”, “rankProf”, and “year” are statistically significant to the 99% confidence level. In addition, all of the predictor variables have a positive linear relationship except for the variable “ysdeg” to salary, which has a negative linear relationship.\r\nPart D.\r\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\r\n\r\n\r\nShow code\r\n\r\n# change baseline rank\r\nnew2d <- relevel(salary$rank, \"Prof\")\r\n\r\n# fit model again\r\nmlm2d <- lm(salary ~ degree + sex + year + ysdeg + new2d, data = salary)\r\n\r\n# get summary\r\nsummary(mlm2d)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ degree + sex + year + ysdeg + new2d, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-4045.2 -1094.7  -361.5   813.2  9193.1 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept)  26864.81    1375.29  19.534 < 0.0000000000000002 ***\r\ndegreePhD     1388.61    1018.75   1.363                0.180    \r\nsexFemale     1166.37     925.57   1.260                0.214    \r\nyear           476.31      94.91   5.018       0.000008653790 ***\r\nysdeg         -124.57      77.49  -1.608                0.115    \r\nnew2dAsst   -11118.76    1351.77  -8.225       0.000000000162 ***\r\nnew2dAssoc   -5826.40    1012.93  -5.752       0.000000727809 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2398 on 45 degrees of freedom\r\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \r\nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 0.00000000000000022\r\n\r\nChanging the baseline for “rank” and looking at the coefficients of variables, the “Asst” rank has an estimate of a lower salary of ~$11,118.76, and the “Assoc” rank has an estimate of a lower salary of ~$5,826.40. Both are statistically significant to the 99% confidence level. This is the same information from the fit test in part C. We have just changed the base reference from “Asst” to “Prof”.\r\nPart E.\r\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.\r\n\r\n\r\nShow code\r\n\r\n#Linear regression without the rank\r\n\r\nmlm2e <- lm(salary ~ sex + degree + year + ysdeg, data = salary)\r\nsummary(mlm2e)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ sex + degree + year + ysdeg, data = salary)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-8146.9 -2186.9  -491.5  2279.1 11186.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept) 17183.57    1147.94  14.969 < 0.0000000000000002 ***\r\nsexFemale   -1286.54    1313.09  -0.980             0.332209    \r\ndegreePhD   -3299.35    1302.52  -2.533             0.014704 *  \r\nyear          351.97     142.48   2.470             0.017185 *  \r\nysdeg         339.40      80.62   4.210             0.000114 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3744 on 47 degrees of freedom\r\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \r\nF-statistic: 20.11 on 4 and 47 DF,  p-value: 0.000000001048\r\n\r\nShow code\r\n\r\n#Manipulate the output tibble and convert scientific notation of p-value to decimals\r\n\r\ntidymlm2e <- tidy(mlm2e, conf.int = TRUE)\r\noptions(scipen = 999)\r\ntidymlm2e\r\n\r\n\r\n# A tibble: 5 x 7\r\n  term        estimate std.error statistic  p.value conf.low conf.high\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\r\n1 (Intercept)   17184.    1148.     15.0   1.66e-19  14874.     19493.\r\n2 sexFemale     -1287.    1313.     -0.980 3.32e- 1  -3928.      1355.\r\n3 degreePhD     -3299.    1303.     -2.53  1.47e- 2  -5920.      -679.\r\n4 year            352.     142.      2.47  1.72e- 2     65.3      639.\r\n5 ysdeg           339.      80.6     4.21  1.14e- 4    177.       502.\r\n\r\nFor the predictor variable “sexFemale”, the statistical significance of its’ relationship to salary is a decrease in salary of ~$1,286.54 given the salary being for a female. The p-value of 0.332 indicates that this result is not statistically significant.\r\nFor the predictor variable “degreePhD”, the statistical significance of its’ relationship to salary is a decrease in salary of ~$3,299.35 given the salary being for a female. The p-value of 0.0147 indicates that this result is statistically significant to the .95 confidence level.\r\nFor the predictor variable “year”, the statistical significance of its’ relationship to salary is an increase in salary of ~$351.97 given the salary for each year of experience in current rank. The p-value of 0.0147 indicates that this result is statistically significant to the .95 confidence level.\r\nFor the predictor variable “ysdeg”, the statistical significance of its’ relationship to salary is an increase in salary of ~$339.40 given the salary for each year since highest degree achieved. The p-value of 0.000114 indicates that this result is statistically significant to the .99 confidence level.\r\nSummarizing, eliminating the “rank” variable, the predictor variables, degreePhD”, “year”, and “ysdeg” are statistically significant to at least the 95% confidence level, while predictor variable “sexFemale” is not statistically significant. The predictor variables “year” and “ysdeg” have a positive linear relationship and the predictor variables “sexFemale” and “degreePhD” have a negative linear relationship.\r\nPractically, this tells me that eliminating “rank” before comparing salaries between males and females shows a different linear relationship than when “rank” was involved (negative vs. positive). However, it also tells me that the relationship remains statistically not significant to a reasonable level of confidence.\r\nPart F.\r\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\r\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\r\nI am creating a new variable for the dummy variable indicating whether it was “Dean 1” or “Dean 2” doing the hiring. “Dean 1” represents the “old” Dean and “Dean 2” represents the “new” Dean appointed 15 years ago. I will run the model with as few predictor variables as is practical to reduce the concern of multicollinearity, or the phenomemon of predictor variables being correlated with one another and contributing to unreliable inferences.\r\nClarifying the null and alternative hypotheses:\r\nH0: The mean salary for hires of Dean 2 are higher than the mean salary for hires of Dean 1\r\nHa: The mean salary for hires of Dean 2 are equal to or less than than the mean salary for hires of Dean 1\r\nWe have n > 30, so I can assume a normal distribution.\r\n\r\n\r\nShow code\r\n\r\n# create new variable\r\ndf2f <- salary %>%\r\n  mutate(dean = case_when(\r\n    ysdeg >= 1 & ysdeg <= 15 ~ \"2\",\r\n    ysdeg >= 16 ~ \"1\"\r\n  ))\r\n\r\nmlm2f <- lm(salary ~ ., data = df2f)\r\n\r\nsummary(mlm2f)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ ., data = df2f)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3621.2 -1336.8  -271.6   530.1  9247.6 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value       Pr(>|t|)    \r\n(Intercept) 13767.69    1744.31   7.893 0.000000000575 ***\r\ndegreePhD    1135.00    1031.16   1.101          0.277    \r\nrankAssoc    5234.01    1138.47   4.597 0.000035985932 ***\r\nrankProf    11411.45    1362.02   8.378 0.000000000116 ***\r\nsexFemale    1084.09     921.49   1.176          0.246    \r\nyear          460.35      95.09   4.841 0.000016263785 ***\r\nysdeg         -47.86      97.71  -0.490          0.627    \r\ndean2        1749.09    1372.83   1.274          0.209    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2382 on 44 degrees of freedom\r\nMultiple R-squared:  0.8602,    Adjusted R-squared:  0.838 \r\nF-statistic: 38.68 on 7 and 44 DF,  p-value: < 0.00000000000000022\r\n\r\nBased on this summary, I can see that the hires of Dean 2 are expected to make a salary of ~$1,749.09 than the hires of Dean 1. This result has a p-value of 0.209. Since this p-value indicates a significance level of % (p > 0.05), I fail to reject the null hypothesis.\r\n\r\n\r\nShow code\r\n\r\n# Removing the variable \"ysdeg\"\r\n\r\nmlm2g <- lm(salary ~ . - ysdeg, data = df2f)\r\n\r\nsummary(mlm2g)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = salary ~ . - ysdeg, data = df2f)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3403.3 -1387.0  -167.0   528.2  9233.8 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value         Pr(>|t|)    \r\n(Intercept) 13328.38    1483.38   8.985 0.00000000001330 ***\r\ndegreePhD     818.93     797.48   1.027           0.3100    \r\nrankAssoc    4972.66     997.17   4.987 0.00000961362451 ***\r\nrankProf    11096.95    1191.00   9.317 0.00000000000454 ***\r\nsexFemale     907.14     840.54   1.079           0.2862    \r\nyear          434.85      78.89   5.512 0.00000164625970 ***\r\ndean2        2163.46    1072.04   2.018           0.0496 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2362 on 45 degrees of freedom\r\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \r\nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 0.00000000000000022\r\n\r\nRunning the model with less variables does change the adjusted R-squared and goodness of fit; though some more than others, after running this model with many different variable combinations. The best fit is seemingly the one including the new “Dean” variable but without the “ysdeg” variable.\r\nQuestion 3\r\nUsing the data file in the SMSS R package “house.selling.price”:\r\nAnswer 3\r\nI’m loading the “house.selling.price” data and inspecting it.\r\n\r\n\r\nShow code\r\n\r\ndata(\"house.selling.price\") \r\nhsp <- house.selling.price\r\ndim(hsp)\r\n\r\n\r\n[1] 100   7\r\n\r\nShow code\r\n\r\nhead(hsp)\r\n\r\n\r\n  case Taxes Beds Baths New  Price Size\r\n1    1  3104    4     2   0 279900 2048\r\n2    2  1173    2     1   0 146500  912\r\n3    3  3076    4     2   0 237700 1654\r\n4    4  1608    3     2   0 200000 2068\r\n5    5  1454    3     3   0 159900 1477\r\n6    6  2997    3     2   1 499900 3153\r\n\r\nPart A.\r\nA. Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)\r\n\r\n\r\nShow code\r\n\r\nmlm3a <- lm(Price ~ Size + New, data = house.selling.price)\r\n\r\nsummary(mlm3a)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-205102  -34374   -5778   18929  163866 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept) -40230.867  14696.140  -2.738              0.00737 ** \r\nSize           116.132      8.795  13.204 < 0.0000000000000002 ***\r\nNew          57736.283  18653.041   3.095              0.00257 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 53880 on 97 degrees of freedom\r\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \r\nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 0.00000000000000022\r\n\r\nPart B.\r\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\r\nThe coefficient for homes that are “new” indicates that the “new” variable results in a price increase of ~$57,736.28.\r\nThe coefficient for home “size” indicates that for each square foot of home size, the price of a home increases by ~$116.13.\r\nThe p-values of 0.00257 for “new” and indicates a significance level of (p < 0.01), indicating the results are statistically significant to the 99% confidence level.\r\nThe equation to indicate price for new homes:\r\n(-40230.87) + (116.13)(x) + (57,736.18)\r\nThe equation to indicate price for not new homes:\r\n(-40230.87) + (116.13)(x)\r\nPart C.\r\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n\r\n\r\nShow code\r\n\r\n-40230.87+(116.13*3000)+57736.18\r\n\r\n\r\n[1] 365895.3\r\n\r\nShow code\r\n\r\n-40230.87+(116.13*3000)\r\n\r\n\r\n[1] 308159.1\r\n\r\nFor a 3000 square foot home that is new, the price can be estimated to be $365,895.30.\r\nFor a 3000 square foot home that is not new, the price can be estimated to be $308,159.10.\r\nPart D.\r\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\r\n\r\n\r\nShow code\r\n\r\nmlm3d <- lm(Price ~ Size + New + Size*New, house.selling.price)\r\n\r\nsummary(mlm3d)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-175748  -28979   -6260   14693  192519 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value             Pr(>|t|)    \r\n(Intercept) -22227.808  15521.110  -1.432              0.15536    \r\nSize           104.438      9.424  11.082 < 0.0000000000000002 ***\r\nNew         -78527.502  51007.642  -1.540              0.12697    \r\nSize:New        61.916     21.686   2.855              0.00527 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 52000 on 96 degrees of freedom\r\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \r\nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 0.00000000000000022\r\n\r\nFitting the model where the “size” variable interacts with the “new” variable, the result is statistically significant with a p-value of 0.00527.\r\nPart E.\r\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\r\n\r\n\r\nShow code\r\n\r\nqplot(x = Price, y = Size, facets = ~ New, data = mlm3d) +\r\n  geom_smooth(method = \"lm\", se=TRUE,fullrange=TRUE,color=\"goldenrod\") + \r\n     labs(title= \"Price and Size of Homes Given Not New and New\",\r\n        x= \"Price\",\r\n        y = \"Size in Square Feet\") +\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\nIn the course of investigation options for graphics on visualizing multiple regression models, I also found an interesting package that uses “ggPredict()” to make a really visually pleasing representation, if not as practical:\r\n\r\n\r\nShow code\r\n\r\nggPredict(mlm3d,se=TRUE,interactive=TRUE)\r\n\r\n\r\n\r\n{\"x\":{\"html\":\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<svg xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' id='svg_d8822ff7-86b5-4381-8b07-55c802e256d5' viewBox='0 0 432 360'>\\n <defs>\\n  <clipPath id='svg_d8822ff7-86b5-4381-8b07-55c802e256d5_c1'>\\n   <rect x='0' y='0' width='432' height='360'/>\\n  <\\/clipPath>\\n  <clipPath id='svg_d8822ff7-86b5-4381-8b07-55c802e256d5_c2'>\\n   <rect x='52.72' y='5.48' width='311.99' height='323.02'/>\\n  <\\/clipPath>\\n <\\/defs>\\n <g>\\n  <g clip-path='url(#svg_d8822ff7-86b5-4381-8b07-55c802e256d5_c1)'>\\n   <rect x='0' y='0' width='432' height='360' fill='#FFFFFF' stroke='#FFFFFF' stroke-width='0.75' stroke-linejoin='round' stroke-linecap='round'/>\\n   <rect x='0' y='0' width='432' height='360' fill='#FFFFFF' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='round'/>\\n  <\\/g>\\n  <g clip-path='url(#svg_d8822ff7-86b5-4381-8b07-55c802e256d5_c2)'>\\n   <rect x='52.72' y='5.48' width='311.99' height='323.02' fill='#EBEBEB' stroke='none'/>\\n   <polyline points='52.72,323.92 364.71,323.92' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,241.52 364.71,241.52' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,159.11 364.71,159.11' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,76.71 364.71,76.71' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='74.37,328.50 74.37,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='149.00,328.50 149.00,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='223.64,328.50 223.64,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='298.28,328.50 298.28,5.48' fill='none' stroke='#FFFFFF' stroke-width='0.53' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,282.72 364.71,282.72' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,200.31 364.71,200.31' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,117.91 364.71,117.91' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='52.72,35.50 364.71,35.50' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='111.69,328.50 111.69,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='186.32,328.50 186.32,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='260.96,328.50 260.96,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='335.60,328.50 335.60,5.48' fill='none' stroke='#FFFFFF' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polygon points='66.90,269.69 81.83,261.76 96.76,253.79 111.69,245.76 126.61,237.61 141.54,229.30 156.47,220.74 171.40,211.94 186.32,202.92 201.25,193.78 216.18,184.56 231.11,175.29 246.03,165.99 260.96,156.68 275.89,147.34 290.82,138.00 305.74,128.65 320.67,119.29 335.60,109.93 350.53,100.57 350.53,121.72 335.60,129.57 320.67,137.42 305.74,145.28 290.82,153.14 275.89,161.01 260.96,168.89 246.03,176.79 231.11,184.70 216.18,192.64 201.25,200.63 186.32,208.71 171.40,216.91 156.47,225.31 141.54,233.97 126.61,242.87 111.69,251.94 96.76,261.11 81.83,270.36 66.90,279.64' fill='#132B43' fill-opacity='0.2' stroke='none'/>\\n   <polyline points='66.90,269.69 81.83,261.76 96.76,253.79 111.69,245.76 126.61,237.61 141.54,229.30 156.47,220.74 171.40,211.94 186.32,202.92 201.25,193.78 216.18,184.56 231.11,175.29 246.03,165.99 260.96,156.68 275.89,147.34 290.82,138.00 305.74,128.65 320.67,119.29 335.60,109.93 350.53,100.57' fill='none' stroke='none'/>\\n   <polyline points='350.53,121.72 335.60,129.57 320.67,137.42 305.74,145.28 290.82,153.14 275.89,161.01 260.96,168.89 246.03,176.79 231.11,184.70 216.18,192.64 201.25,200.63 186.32,208.71 171.40,216.91 156.47,225.31 141.54,233.97 126.61,242.87 111.69,251.94 96.76,261.11 81.83,270.36 66.90,279.64' fill='none' stroke='none'/>\\n   <polygon points='66.90,279.81 81.83,267.58 96.76,255.32 111.69,243.02 126.61,230.67 141.54,218.24 156.47,205.70 171.40,193.00 186.32,180.09 201.25,166.86 216.18,153.26 231.11,139.27 246.03,124.93 260.96,110.32 275.89,95.52 290.82,80.58 305.74,65.56 320.67,50.47 335.60,35.33 350.53,20.16 350.53,52.55 335.60,64.80 320.67,77.08 305.74,89.40 290.82,101.80 275.89,114.28 260.96,126.89 246.03,139.70 231.11,152.78 216.18,166.20 201.25,180.02 186.32,194.21 171.40,208.71 156.47,223.43 141.54,238.31 126.61,253.30 111.69,268.36 96.76,283.48 81.83,298.64 66.90,313.82' fill='#56B1F7' fill-opacity='0.2' stroke='none'/>\\n   <polyline points='66.90,279.81 81.83,267.58 96.76,255.32 111.69,243.02 126.61,230.67 141.54,218.24 156.47,205.70 171.40,193.00 186.32,180.09 201.25,166.86 216.18,153.26 231.11,139.27 246.03,124.93 260.96,110.32 275.89,95.52 290.82,80.58 305.74,65.56 320.67,50.47 335.60,35.33 350.53,20.16' fill='none' stroke='none'/>\\n   <polyline points='350.53,52.55 335.60,64.80 320.67,77.08 305.74,89.40 290.82,101.80 275.89,114.28 260.96,126.89 246.03,139.70 231.11,152.78 216.18,166.20 201.25,180.02 186.32,194.21 171.40,208.71 156.47,223.43 141.54,238.31 126.61,253.30 111.69,268.36 96.76,283.48 81.83,298.64 66.90,313.82' fill='none' stroke='none'/>\\n   <circle cx='189.91' cy='167.39' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='1' title='1&amp;lt;br/&amp;gt;Size=2048&amp;lt;br/&amp;gt;Price=279900'/>\\n   <circle cx='105.12' cy='222.36' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='2' title='2&amp;lt;br/&amp;gt;Size=912&amp;lt;br/&amp;gt;Price=146500'/>\\n   <circle cx='160.5' cy='184.78' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='3' title='3&amp;lt;br/&amp;gt;Size=1654&amp;lt;br/&amp;gt;Price=237700'/>\\n   <circle cx='191.4' cy='200.31' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='4' title='4&amp;lt;br/&amp;gt;Size=2068&amp;lt;br/&amp;gt;Price=200000'/>\\n   <circle cx='147.29' cy='216.84' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='5' title='5&amp;lt;br/&amp;gt;Size=1477&amp;lt;br/&amp;gt;Price=159900'/>\\n   <circle cx='272.38' cy='76.75' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='6' title='6&amp;lt;br/&amp;gt;Size=3153&amp;lt;br/&amp;gt;Price=499900'/>\\n   <circle cx='138.18' cy='173.33' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='7' title='7&amp;lt;br/&amp;gt;Size=1355&amp;lt;br/&amp;gt;Price=265500'/>\\n   <circle cx='191.92' cy='163.27' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='8' title='8&amp;lt;br/&amp;gt;Size=2075&amp;lt;br/&amp;gt;Price=289900'/>\\n   <circle cx='334.85' cy='40.86' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='9' title='9&amp;lt;br/&amp;gt;Size=3990&amp;lt;br/&amp;gt;Price=587000'/>\\n   <circle cx='123.63' cy='253.88' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='10' title='10&amp;lt;br/&amp;gt;Size=1160&amp;lt;br/&amp;gt;Price=70000'/>\\n   <circle cx='128.11' cy='256.14' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='11' title='11&amp;lt;br/&amp;gt;Size=1220&amp;lt;br/&amp;gt;Price=64500'/>\\n   <circle cx='163.19' cy='213.91' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='12' title='12&amp;lt;br/&amp;gt;Size=1690&amp;lt;br/&amp;gt;Price=167000'/>\\n   <circle cx='140.05' cy='235.5' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='13' title='13&amp;lt;br/&amp;gt;Size=1380&amp;lt;br/&amp;gt;Price=114600'/>\\n   <circle cx='155.72' cy='240.28' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='14' title='14&amp;lt;br/&amp;gt;Size=1590&amp;lt;br/&amp;gt;Price=103000'/>\\n   <circle cx='115.42' cy='241.1' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='15' title='15&amp;lt;br/&amp;gt;Size=1050&amp;lt;br/&amp;gt;Price=101000'/>\\n   <circle cx='94.52' cy='253.88' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='16' title='16&amp;lt;br/&amp;gt;Size=770&amp;lt;br/&amp;gt;Price=70000'/>\\n   <circle cx='142.29' cy='247.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='17' title='17&amp;lt;br/&amp;gt;Size=1410&amp;lt;br/&amp;gt;Price=85000'/>\\n   <circle cx='116.16' cy='273.45' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='18' title='18&amp;lt;br/&amp;gt;Size=1060&amp;lt;br/&amp;gt;Price=22500'/>\\n   <circle cx='134.08' cy='245.64' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='19' title='19&amp;lt;br/&amp;gt;Size=1300&amp;lt;br/&amp;gt;Price=90000'/>\\n   <circle cx='149' cy='227.92' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='20' title='20&amp;lt;br/&amp;gt;Size=1500&amp;lt;br/&amp;gt;Price=133000'/>\\n   <circle cx='98.25' cy='245.43' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='21' title='21&amp;lt;br/&amp;gt;Size=820&amp;lt;br/&amp;gt;Price=90500'/>\\n   <circle cx='331.79' cy='44.77' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='22' title='22&amp;lt;br/&amp;gt;Size=3949&amp;lt;br/&amp;gt;Price=577500'/>\\n   <circle cx='124.37' cy='224.01' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='23' title='23&amp;lt;br/&amp;gt;Size=1170&amp;lt;br/&amp;gt;Price=142500'/>\\n   <circle cx='149' cy='216.8' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='24' title='24&amp;lt;br/&amp;gt;Size=1500&amp;lt;br/&amp;gt;Price=160000'/>\\n   <circle cx='245.29' cy='183.83' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='25' title='25&amp;lt;br/&amp;gt;Size=2790&amp;lt;br/&amp;gt;Price=240000'/>\\n   <circle cx='113.92' cy='246.87' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='26' title='26&amp;lt;br/&amp;gt;Size=1030&amp;lt;br/&amp;gt;Price=87000'/>\\n   <circle cx='130.35' cy='233.85' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='27' title='27&amp;lt;br/&amp;gt;Size=1250&amp;lt;br/&amp;gt;Price=118600'/>\\n   <circle cx='168.41' cy='225.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='28' title='28&amp;lt;br/&amp;gt;Size=1760&amp;lt;br/&amp;gt;Price=140000'/>\\n   <circle cx='152.74' cy='221.74' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='29' title='29&amp;lt;br/&amp;gt;Size=1550&amp;lt;br/&amp;gt;Price=148000'/>\\n   <circle cx='120.64' cy='254.29' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='30' title='30&amp;lt;br/&amp;gt;Size=1120&amp;lt;br/&amp;gt;Price=69000'/>\\n   <circle cx='186.32' cy='210.2' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='31' title='31&amp;lt;br/&amp;gt;Size=2000&amp;lt;br/&amp;gt;Price=176000'/>\\n   <circle cx='137.81' cy='247.08' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='32' title='32&amp;lt;br/&amp;gt;Size=1350&amp;lt;br/&amp;gt;Price=86500'/>\\n   <circle cx='174.38' cy='208.55' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='33' title='33&amp;lt;br/&amp;gt;Size=1840&amp;lt;br/&amp;gt;Price=180000'/>\\n   <circle cx='224.39' cy='208.97' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='34' title='34&amp;lt;br/&amp;gt;Size=2510&amp;lt;br/&amp;gt;Price=179000'/>\\n   <circle cx='269.17' cy='143.45' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='35' title='35&amp;lt;br/&amp;gt;Size=3110&amp;lt;br/&amp;gt;Price=338000'/>\\n   <circle cx='168.41' cy='229.16' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='36' title='36&amp;lt;br/&amp;gt;Size=1760&amp;lt;br/&amp;gt;Price=130000'/>\\n   <circle cx='164.68' cy='215.56' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='37' title='37&amp;lt;br/&amp;gt;Size=1710&amp;lt;br/&amp;gt;Price=163000'/>\\n   <circle cx='119.9' cy='231.22' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='38' title='38&amp;lt;br/&amp;gt;Size=1110&amp;lt;br/&amp;gt;Price=125000'/>\\n   <circle cx='138.56' cy='241.52' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='39' title='39&amp;lt;br/&amp;gt;Size=1360&amp;lt;br/&amp;gt;Price=100000'/>\\n   <circle cx='130.35' cy='241.52' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='40' title='40&amp;lt;br/&amp;gt;Size=1250&amp;lt;br/&amp;gt;Price=100000'/>\\n   <circle cx='130.35' cy='241.52' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='41' title='41&amp;lt;br/&amp;gt;Size=1250&amp;lt;br/&amp;gt;Price=100000'/>\\n   <circle cx='147.51' cy='222.36' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='42' title='42&amp;lt;br/&amp;gt;Size=1480&amp;lt;br/&amp;gt;Price=146500'/>\\n   <circle cx='150.5' cy='223.02' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='43' title='43&amp;lt;br/&amp;gt;Size=1520&amp;lt;br/&amp;gt;Price=144900'/>\\n   <circle cx='187.82' cy='207.32' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='44' title='44&amp;lt;br/&amp;gt;Size=2020&amp;lt;br/&amp;gt;Price=183000'/>\\n   <circle cx='112.43' cy='253.92' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='45' title='45&amp;lt;br/&amp;gt;Size=1010&amp;lt;br/&amp;gt;Price=69900'/>\\n   <circle cx='159.45' cy='258' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='46' title='46&amp;lt;br/&amp;gt;Size=1640&amp;lt;br/&amp;gt;Price=60000'/>\\n   <circle cx='107.21' cy='230.39' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='47' title='47&amp;lt;br/&amp;gt;Size=940&amp;lt;br/&amp;gt;Price=127000'/>\\n   <circle cx='154.98' cy='247.28' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='48' title='48&amp;lt;br/&amp;gt;Size=1580&amp;lt;br/&amp;gt;Price=86000'/>\\n   <circle cx='101.24' cy='262.12' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='49' title='49&amp;lt;br/&amp;gt;Size=860&amp;lt;br/&amp;gt;Price=50000'/>\\n   <circle cx='143.03' cy='226.27' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='50' title='50&amp;lt;br/&amp;gt;Size=1420&amp;lt;br/&amp;gt;Price=137000'/>\\n   <circle cx='131.84' cy='232.74' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='51' title='51&amp;lt;br/&amp;gt;Size=1270&amp;lt;br/&amp;gt;Price=121300'/>\\n   <circle cx='110.19' cy='249.35' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='52' title='52&amp;lt;br/&amp;gt;Size=980&amp;lt;br/&amp;gt;Price=81000'/>\\n   <circle cx='208.71' cy='205.26' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='53' title='53&amp;lt;br/&amp;gt;Size=2300&amp;lt;br/&amp;gt;Price=188000'/>\\n   <circle cx='143.78' cy='247.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='54' title='54&amp;lt;br/&amp;gt;Size=1430&amp;lt;br/&amp;gt;Price=85000'/>\\n   <circle cx='140.05' cy='226.27' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='55' title='55&amp;lt;br/&amp;gt;Size=1380&amp;lt;br/&amp;gt;Price=137000'/>\\n   <circle cx='129.6' cy='222.98' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='56' title='56&amp;lt;br/&amp;gt;Size=1240&amp;lt;br/&amp;gt;Price=145000'/>\\n   <circle cx='120.64' cy='254.29' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='57' title='57&amp;lt;br/&amp;gt;Size=1120&amp;lt;br/&amp;gt;Price=69000'/>\\n   <circle cx='120.64' cy='237.68' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='58' title='58&amp;lt;br/&amp;gt;Size=1120&amp;lt;br/&amp;gt;Price=109300'/>\\n   <circle cx='178.86' cy='228.54' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='59' title='59&amp;lt;br/&amp;gt;Size=1900&amp;lt;br/&amp;gt;Price=131500'/>\\n   <circle cx='218.42' cy='200.31' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='60' title='60&amp;lt;br/&amp;gt;Size=2430&amp;lt;br/&amp;gt;Price=200000'/>\\n   <circle cx='117.66' cy='248.97' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='61' title='61&amp;lt;br/&amp;gt;Size=1080&amp;lt;br/&amp;gt;Price=81900'/>\\n   <circle cx='137.81' cy='245.14' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='62' title='62&amp;lt;br/&amp;gt;Size=1350&amp;lt;br/&amp;gt;Price=91200'/>\\n   <circle cx='165.42' cy='231.42' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='63' title='63&amp;lt;br/&amp;gt;Size=1720&amp;lt;br/&amp;gt;Price=124500'/>\\n   <circle cx='339.33' cy='190.01' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='64' title='64&amp;lt;br/&amp;gt;Size=4050&amp;lt;br/&amp;gt;Price=225000'/>\\n   <circle cx='149' cy='226.48' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='65' title='65&amp;lt;br/&amp;gt;Size=1500&amp;lt;br/&amp;gt;Price=136500'/>\\n   <circle cx='229.69' cy='125.74' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='66' title='66&amp;lt;br/&amp;gt;Size=2581&amp;lt;br/&amp;gt;Price=381000'/>\\n   <circle cx='195.28' cy='179.71' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='67' title='67&amp;lt;br/&amp;gt;Size=2120&amp;lt;br/&amp;gt;Price=250000'/>\\n   <circle cx='241.93' cy='136.49' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='68' title='68&amp;lt;br/&amp;gt;Size=2745&amp;lt;br/&amp;gt;Price=354900'/>\\n   <circle cx='150.5' cy='225.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='69' title='69&amp;lt;br/&amp;gt;Size=1520&amp;lt;br/&amp;gt;Price=140000'/>\\n   <circle cx='132.58' cy='245.68' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='70' title='70&amp;lt;br/&amp;gt;Size=1280&amp;lt;br/&amp;gt;Price=89900'/>\\n   <circle cx='157.96' cy='226.27' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='71' title='71&amp;lt;br/&amp;gt;Size=1620&amp;lt;br/&amp;gt;Price=137000'/>\\n   <circle cx='150.5' cy='240.28' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='72' title='72&amp;lt;br/&amp;gt;Size=1520&amp;lt;br/&amp;gt;Price=103000'/>\\n   <circle cx='188.56' cy='207.32' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='73' title='73&amp;lt;br/&amp;gt;Size=2030&amp;lt;br/&amp;gt;Price=183000'/>\\n   <circle cx='140.79' cy='225.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='74' title='74&amp;lt;br/&amp;gt;Size=1390&amp;lt;br/&amp;gt;Price=140000'/>\\n   <circle cx='177.37' cy='216.8' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='75' title='75&amp;lt;br/&amp;gt;Size=1880&amp;lt;br/&amp;gt;Price=160000'/>\\n   <circle cx='252.83' cy='103.9' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='76' title='76&amp;lt;br/&amp;gt;Size=2891&amp;lt;br/&amp;gt;Price=434000'/>\\n   <circle cx='137.06' cy='229.16' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='77' title='77&amp;lt;br/&amp;gt;Size=1340&amp;lt;br/&amp;gt;Price=130000'/>\\n   <circle cx='107.21' cy='232.04' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='78' title='78&amp;lt;br/&amp;gt;Size=940&amp;lt;br/&amp;gt;Price=123000'/>\\n   <circle cx='80.34' cy='274.07' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='79' title='79&amp;lt;br/&amp;gt;Size=580&amp;lt;br/&amp;gt;Price=21000'/>\\n   <circle cx='142.29' cy='247.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='80' title='80&amp;lt;br/&amp;gt;Size=1410&amp;lt;br/&amp;gt;Price=85000'/>\\n   <circle cx='122.88' cy='253.92' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='81' title='81&amp;lt;br/&amp;gt;Size=1150&amp;lt;br/&amp;gt;Price=69900'/>\\n   <circle cx='140.05' cy='231.22' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='82' title='82&amp;lt;br/&amp;gt;Size=1380&amp;lt;br/&amp;gt;Price=125000'/>\\n   <circle cx='146.77' cy='215.72' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='83' title='83&amp;lt;br/&amp;gt;Size=1470&amp;lt;br/&amp;gt;Price=162600'/>\\n   <circle cx='155.72' cy='218.07' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='84' title='84&amp;lt;br/&amp;gt;Size=1590&amp;lt;br/&amp;gt;Price=156900'/>\\n   <circle cx='126.61' cy='239.09' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='85' title='85&amp;lt;br/&amp;gt;Size=1200&amp;lt;br/&amp;gt;Price=105900'/>\\n   <circle cx='180.35' cy='213.7' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='86' title='86&amp;lt;br/&amp;gt;Size=1920&amp;lt;br/&amp;gt;Price=167500'/>\\n   <circle cx='197.52' cy='220.17' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='87' title='87&amp;lt;br/&amp;gt;Size=2150&amp;lt;br/&amp;gt;Price=151800'/>\\n   <circle cx='201.25' cy='233.98' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='88' title='88&amp;lt;br/&amp;gt;Size=2200&amp;lt;br/&amp;gt;Price=118300'/>\\n   <circle cx='101.24' cy='243.87' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='89' title='89&amp;lt;br/&amp;gt;Size=860&amp;lt;br/&amp;gt;Price=94300'/>\\n   <circle cx='128.85' cy='244.03' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='90' title='90&amp;lt;br/&amp;gt;Size=1230&amp;lt;br/&amp;gt;Price=93900'/>\\n   <circle cx='122.13' cy='214.73' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='91' title='91&amp;lt;br/&amp;gt;Size=1140&amp;lt;br/&amp;gt;Price=165000'/>\\n   <circle cx='234.84' cy='165.29' r='1.47pt' fill='#56B1F7' stroke='#56B1F7' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='92' title='92&amp;lt;br/&amp;gt;Size=2650&amp;lt;br/&amp;gt;Price=285000'/>\\n   <circle cx='116.16' cy='264.18' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='93' title='93&amp;lt;br/&amp;gt;Size=1060&amp;lt;br/&amp;gt;Price=45000'/>\\n   <circle cx='169.16' cy='231.26' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='94' title='94&amp;lt;br/&amp;gt;Size=1770&amp;lt;br/&amp;gt;Price=124900'/>\\n   <circle cx='175.87' cy='222.15' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='95' title='95&amp;lt;br/&amp;gt;Size=1860&amp;lt;br/&amp;gt;Price=147000'/>\\n   <circle cx='116.16' cy='210.2' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='96' title='96&amp;lt;br/&amp;gt;Size=1060&amp;lt;br/&amp;gt;Price=176000'/>\\n   <circle cx='166.17' cy='201.76' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='97' title='97&amp;lt;br/&amp;gt;Size=1730&amp;lt;br/&amp;gt;Price=196500'/>\\n   <circle cx='139.3' cy='228.25' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='98' title='98&amp;lt;br/&amp;gt;Size=1370&amp;lt;br/&amp;gt;Price=132200'/>\\n   <circle cx='153.48' cy='246.3' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='99' title='99&amp;lt;br/&amp;gt;Size=1560&amp;lt;br/&amp;gt;Price=88400'/>\\n   <circle cx='137.06' cy='230.31' r='1.47pt' fill='#132B43' stroke='#132B43' stroke-width='0.71' stroke-linejoin='round' stroke-linecap='round' data-id='100' title='100&amp;lt;br/&amp;gt;Size=1340&amp;lt;br/&amp;gt;Price=127200'/>\\n   <polyline points='66.90,274.67 81.83,266.06 96.76,257.45 111.69,248.85 126.61,240.24 141.54,231.63 156.47,223.03 171.40,214.42 186.32,205.82 201.25,197.21 216.18,188.60 231.11,180.00 246.03,171.39 260.96,162.78 275.89,154.18 290.82,145.57 305.74,136.97 320.67,128.36 335.60,119.75 350.53,111.15' fill='none' stroke='#132B43' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt' title='for New=0&amp;lt;br/&amp;gt;y = 104.44*x -22227.81' data-id='1'/>\\n   <polyline points='66.90,296.82 81.83,283.11 96.76,269.40 111.69,255.69 126.61,241.98 141.54,228.27 156.47,214.57 171.40,200.86 186.32,187.15 201.25,173.44 216.18,159.73 231.11,146.02 246.03,132.31 260.96,118.61 275.89,104.90 290.82,91.19 305.74,77.48 320.67,63.77 335.60,50.06 350.53,36.36' fill='none' stroke='#56B1F7' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt' title='for New=1&amp;lt;br/&amp;gt;y = 166.35*x -100755.31' data-id='21'/>\\n  <\\/g>\\n  <g clip-path='url(#svg_d8822ff7-86b5-4381-8b07-55c802e256d5_c1)'>\\n   <text x='42.89' y='285.87' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>0<\\/text>\\n   <text x='18.41' y='203.46' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>200000<\\/text>\\n   <text x='18.41' y='121.06' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>400000<\\/text>\\n   <text x='18.41' y='38.65' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>600000<\\/text>\\n   <polyline points='49.98,282.72 52.72,282.72' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='49.98,200.31 52.72,200.31' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='49.98,117.91 52.72,117.91' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='49.98,35.50 52.72,35.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='111.69,331.24 111.69,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='186.32,331.24 186.32,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='260.96,331.24 260.96,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <polyline points='335.60,331.24 335.60,328.50' fill='none' stroke='#333333' stroke-width='1.07' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <text x='101.89' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>1000<\\/text>\\n   <text x='176.53' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>2000<\\/text>\\n   <text x='251.17' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>3000<\\/text>\\n   <text x='325.81' y='339.74' font-size='6.6pt' font-family='Arial' fill='#4D4D4D'>4000<\\/text>\\n   <text x='198.02' y='352.2' font-size='8.25pt' font-family='Arial'>Size<\\/text>\\n   <text transform='translate(13.36,179.52) rotate(-90.00)' font-size='8.25pt' font-family='Arial'>Price<\\/text>\\n   <rect x='375.67' y='110.47' width='50.85' height='113.03' fill='#FFFFFF' stroke='none'/>\\n   <image x='381.15' y='131.63' width='17.28' height='86.4' preserveAspectRatio='none' xlink:href='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAEsCAYAAAACUNnVAAAAmElEQVQ4ja2UQRLDMAgDd3hk/tnX5agcknFaCoa0vnkMCEnGsL12GQhDYDpPwoDrpBFlRDWi/9Wu6AHC9FkLYLrzlhD3oJ3mDvRHRYy7IC8A7bCK8gpWKYDzeQVArDcDyD2YAnSofZvzaOA6Q7Oahp/dmRuVTQ0xU5TGP2o8Waool1obVuv1q8qP9xPvg633XlGLDtfIhNUBcBeA5ss0BXMAAAAASUVORK5CYII=' xmlns:xlink='http://www.w3.org/1999/xlink'/>\\n   <text x='403.91' y='221.03' font-size='6.6pt' font-family='Arial'>0.00<\\/text>\\n   <text x='403.91' y='199.51' font-size='6.6pt' font-family='Arial'>0.25<\\/text>\\n   <text x='403.91' y='177.98' font-size='6.6pt' font-family='Arial'>0.50<\\/text>\\n   <text x='403.91' y='156.45' font-size='6.6pt' font-family='Arial'>0.75<\\/text>\\n   <text x='403.91' y='134.92' font-size='6.6pt' font-family='Arial'>1.00<\\/text>\\n   <text x='381.15' y='124.99' font-size='8.25pt' font-family='Arial'>New<\\/text>\\n   <line x1='381.15' y1='217.88' x2='384.6' y2='217.88' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='196.36' x2='384.6' y2='196.36' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='174.83' x2='384.6' y2='174.83' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='153.3' x2='384.6' y2='153.3' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='381.15' y1='131.77' x2='384.6' y2='131.77' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='217.88' x2='398.43' y2='217.88' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='196.36' x2='398.43' y2='196.36' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='174.83' x2='398.43' y2='174.83' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='153.3' x2='398.43' y2='153.3' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n   <line x1='394.97' y1='131.77' x2='398.43' y2='131.77' stroke='#FFFFFF' stroke-width='0.38' stroke-linejoin='round' stroke-linecap='butt'/>\\n  <\\/g>\\n <\\/g>\\n<\\/svg>\",\"js\":null,\"uid\":\"svg_d8822ff7-86b5-4381-8b07-55c802e256d5\",\"ratio\":1.2,\"settings\":{\"tooltip\":{\"css\":\".tooltip_SVGID_ { background-color:white;font-style:italic;padding:10px;border-radius:10px 20px 10px 20px; ; position:absolute;pointer-events:none;z-index:999;}\",\"placement\":\"doc\",\"offx\":10,\"offy\":0,\"use_cursor_pos\":true,\"opacity\":0.75,\"usefill\":false,\"usestroke\":false,\"delay\":{\"over\":200,\"out\":500}},\"hover\":{\"css\":\".hover_SVGID_ { r:4px;cursor:pointer;stroke:red;stroke-width:2px; }\",\"reactive\":false},\"hoverkey\":{\"css\":\".hover_key_SVGID_ { stroke:red; }\",\"reactive\":false},\"hovertheme\":{\"css\":\".hover_theme_SVGID_ { fill:green; }\",\"reactive\":false},\"hoverinv\":{\"css\":\"\"},\"zoom\":{\"min\":1,\"max\":10},\"capture\":{\"css\":\".selected_SVGID_ { fill:#FF3333;stroke:black; }\",\"type\":\"multiple\",\"only_shiny\":true,\"selected\":[]},\"capturekey\":{\"css\":\".selected_key_SVGID_ { stroke:gray; }\",\"type\":\"single\",\"only_shiny\":true,\"selected\":[]},\"capturetheme\":{\"css\":\".selected_theme_SVGID_ { stroke:gray; }\",\"type\":\"single\",\"only_shiny\":true,\"selected\":[]},\"toolbar\":{\"position\":\"topright\",\"saveaspng\":true,\"pngname\":\"diagram\"},\"sizing\":{\"rescale\":true,\"width\":1}}},\"evals\":[],\"jsHooks\":[]}\r\nPart F.\r\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\r\n\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*3000)+(-78527.50*0)+(61.92*3000*0) \r\n\r\n\r\n[1] 291092.2\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*3000)+(-78527.50*1)+(61.92*3000*1) \r\n\r\n\r\n[1] 398324.7\r\n\r\nUnder the new model:\r\nFor a 3000 square foot home that is new, the price can be estimated to be $398,324.70.\r\nFor a 3000 square foot home that is not new, the price can be estimated to be $291,092.20.\r\nPart G.\r\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\r\nFor a 1500 square foot home that is new, the price can be estimated to be $148,784.70.\r\nFor a 1500 square foot home that is not new, the price can be estimated to be $134,432.20.\r\n\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*1500)+(-78527.50*0)+(61.92*1500*0) \r\n\r\n\r\n[1] 134432.2\r\n\r\nShow code\r\n\r\n-22227.81+(104.44*1500)+(-78527.50*1)+(61.92*1500*1) \r\n\r\n\r\n[1] 148784.7\r\n\r\nIllustrating the difference and analyzing the output\r\n\r\n\r\nShow code\r\n\r\n#3,000 Square Foot House\r\n\r\nnew1 <- 398324.70\r\nold1 <- 291092.20\r\nold1/new1*100\r\n\r\n\r\n[1] 73.07912\r\n\r\nShow code\r\n\r\n#1,500 Square Foot House\r\n\r\nnew2 <- 148784.70\r\nold2 <- 134432.2\r\nold2/new2*100\r\n\r\n\r\n[1] 90.35351\r\n\r\nThe difference in value between the 3,000 square foot house and the 1,500 square foot house under this model represents a value ration of price of old to new of 73.08% vs. 90.35%. This tells me that in this model, the price of an “old” house is a smaller percentage of the price of a “new” house as the square footage increases.\r\nPart H.\r\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\r\nThe second model has a higher adjusted R squared (0.7363 v. 0.7169) than the first model, which was still an independently valid model. This gives me a hint at the statistical reliability of the model. I also think it takes into consideration the context a stronger threshold where square footage begins to diminish as a variable.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-03-dacss-603-assignment-3/dacss-603-assignment-3_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2022-04-03T17:28:54-05:00",
    "input_file": "dacss-603-assignment-3.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-dacss-603-assignment-2/",
    "title": "DACSS 603 Assignment 2",
    "description": "Assignment 2 for DACSS 603 course 'Quantitative Data Analysis': \"Regression Background, Simple Linear Regression, & Multiple Regression\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-03-11",
    "categories": [
      "statistics",
      "homework"
    ],
    "contents": "\r\nQuestion 1\r\nUnited Nations\r\nUnited Nations (Data file: UN11) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\r\n1.1.1. Identify the predictor and the response.\r\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\r\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\r\nAnswer 1\r\nFirst, I load in the appropriate data. Then I look at the head() info to preview the dataset, and use the function “?UN11” to get details on this data.\r\nThe defining measure of the variables I am examining are:\r\n\r\nppgdp, representing “per capita gross domestic product in US dollars”, and fertility, representing “number of children per woman”.\r\n\r\n1.1.1: The predictor variable is the ppgdp and the response variable is fertility.\r\nNext, I create the scatterplot and add a theoretical regression line using the smooth() function and applying the ‘lm’ method in ggplot2. Using “smooth()” in this way allows me to see patterns in the graph representing a potential linear regression line.\r\n\r\n\r\nShow code\r\n\r\n#First, I loaded the data, used the head() function to preview the dataset, and used the \"?UN11\" function to get additional details on the data represented in the dataset. \r\n\r\ndata(\"UN11\")\r\n\r\n#Then, I created a dataframe with the relevant variables and plotted them using ggplot2\r\n\r\ndf1 <- UN11 %>% \r\n  select(c(ppgdp, fertility))\r\n\r\ngg1a <- ggplot(df1, aes(x=ppgdp, y=fertility)) +\r\n  geom_point() +\r\n  geom_smooth(method=lm,se=FALSE,fullrange=TRUE,color=\"goldenrod\") +\r\n   labs(title= \"Fertility and GDP\",\r\n        x= \"GNP (Per Person in US Dollars)\", \r\n        y = \"Fertility (Births Per Woman)\") +\r\n   theme_classic()\r\n\r\ngg1a\r\n\r\n\r\n\r\n\r\n1.1.2: The scatterplot indicates that this graph does not support a straight-line mean function.\r\nFinally, I add the log() functions to analyze the linear model and again use “smooth()” to look at patterns in the graph representing a regression line on a linear model.\r\n\r\n\r\nShow code\r\n\r\ngg1b <- ggplot(df1, aes(x=log(ppgdp), y=log(fertility))) +\r\n  geom_point() +\r\n  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"goldenrod\") +\r\n   labs(title= \"Log: Fertility and GDP\",\r\n        x= \"GNP (Per Person in US Dollars)\",\r\n        y = \"Fertility (Births Per Woman)\") +\r\n   theme_classic()\r\n\r\ngg1b\r\n\r\n\r\n\r\n\r\n1.1.3: It seems the simple linear regression model is plausible. The consistent downward line would be the best fit for most of the data points.\r\nAdditionally, the summary() call of the linear model “lm()” function allows me to confirm that the p-value is statistically significant to a high level of confidence (< 0.001)\r\n\r\n\r\nShow code\r\n\r\nfit1 <- lm(fertility ~ ppgdp, data = df1)\r\nsummary(fit1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = fertility ~ ppgdp, data = df1)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\r\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.206 on 197 degrees of freedom\r\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \r\nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\r\n\r\nQuestion 2\r\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\r\nHow, if at all, does the slope of the prediction equation change?\r\n\r\nHow, if at all, does the correlation change?\r\n\r\nAnswer 2\r\nThe slope will change - if slope = “s”, the new slope becomes (s/1.33). This is because we are looking at the response variable. When converting an explanatory variable, there is an inverse relationship to the slope and we would multiply the old slope by 1.33.\r\n\r\nThe correlation will not change. This is because correlation is not affected by a change in units.\r\n\r\nQuestion 3\r\nWater Runoff in the Sierras\r\nCan Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM.\r\nDraw the scatterplot matrix for these data and summarize the information available from these plots.\r\nAnswer 3\r\nFirst, I load in the appropriate data. Then I look at the head() info to preview the dataset, and use the function “?water” to get details on this “California water” data. I can now create a scatterplot matrix for the data using the plot() function. This allows me to make an initial look into the existence of linear relationships in the data.\r\n\r\n\r\nShow code\r\n\r\n#First I load the data\r\n\r\ndata(\"water\")\r\n\r\n#Next, I create the dataframe object from the data\r\n\r\ndf3 <- water\r\n\r\n#Then, I make an initial scatterplot matrix\r\n\r\npairs(df3)\r\n\r\n\r\n\r\n\r\nUsing the plot() function allows me to identify visually the positive linear correlations between these variables. Those appear to me at first glance to be, leading with the strongest visual representation:\r\nOPSLAKE and OPRC\r\nOPSLAKE and OPBPC\r\nBSAAM and OPSLAKE\r\nBSAAM and OPRC\r\nBSAAM and OPBPC\r\nStill clear but weaker positive linear correlations:\r\nOPRC and OPBPC\r\nAPMAM and APSAB\r\nAPMAM and APSLAKE\r\nAPSAB and APSLAKE\r\nThere are no apparent positive (or negative!) linear correlations between the ‘Year’ variable and any other variable.\r\nThis is helpful in visualizing relationships, but I have better opportunities to visualize this scatterplot matrix in R. I also would like to look at the statistical correlation evaluations of these relationships.\r\nOne opportunity is a package called “GGally” that builds upon ggplot2 to look at more than just the general scatterplot matrix. The primary difference between the GGally “ggpairs()” function and the pairs function of base R is that the diagonal consists of the densities of the variables and the upper panels consist of the Pearson correlation coefficients between the variables using the argument “pearson”.\r\n\r\n\r\nShow code\r\n\r\nggpairs(df3, method = c(\"everything\", \"pearson\"))\r\n\r\n\r\n\r\n\r\nUsing this representation, I can see both the visual correlations and the statistical correlations of the pairs. This allows me to compare my predictions to the correlation scores. Highest significant correlations are:\r\nOPSLAKE and OPBPC\r\nBSAAM and OPSLAKE\r\nBSAAM and OPRC\r\nOPSLAKE and OPRC\r\nAPSLAKE and APSAB\r\nBSAAM and OPBPC\r\nOPRC and OPBPC\r\nAPSAB and APMAM\r\nAPSLAKE and APMAM\r\nThe results confirm that although my visual evaluation using the initial scatterplot matrix was not necessarily 100% accurate, it served as a reliable estimate of positive linear statistical correlation.\r\nFinally, I want to use a third option for visualizing these correlations that is even more simple and graphically pleasing, the “ggcorr()” function in GGally. This method cleanly demonstrates the strong correlations we have evaluated in a simple way.\r\n\r\n\r\nShow code\r\n\r\nggcorr(df3, method = c(\"everything\", \"pearson\")) \r\n\r\n\r\n\r\n\r\nQuestion 4\r\nProfessor Ratings\r\nIn the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Use R to reproduce the scatterplot matrix in Figure 1.13 in the ALR book (page 20). Provide a brief description of the relationships between the five ratings. (The variables don’t have to be in the same order)\r\nAnswer 4\r\nFirst, I load in the appropriate data. Then I look at the head() info to preview the dataset, and use the function “?Rateprof” to get details on this dataset. I create a dataframe of the variables of interest from the Rateprof dataset.\r\n\r\n\r\nShow code\r\n\r\n#First I load the data\r\n\r\ndata(\"Rateprof\")\r\n\r\n#Then create a data frame object of the relevant variables\r\n\r\ndf4 <- Rateprof %>% \r\n  select(c(quality, helpfulness, clarity, easiness, raterInterest))\r\n\r\n#I preview the data to understand what type of data is represented\r\n\r\nhead(df4)\r\n\r\n\r\n   quality helpfulness  clarity easiness raterInterest\r\n1 4.636364    4.636364 4.636364 4.818182      3.545455\r\n2 4.318182    4.545455 4.090909 4.363636      4.000000\r\n3 4.790698    4.720930 4.860465 4.604651      3.432432\r\n4 4.250000    4.458333 4.041667 2.791667      3.181818\r\n5 4.684211    4.684211 4.684211 4.473684      4.214286\r\n6 4.233333    4.266667 4.200000 4.533333      3.916667\r\n\r\nUsing the pairs() function, I am now able to create a scatterplot matrix that matches the one in the book (ALR, Problem 1.6).\r\n\r\n\r\nShow code\r\n\r\npairs(df4)\r\n\r\n\r\n\r\n\r\nThe relationships illustrated by the correlations represented in this scatterplot matrix indicate a strong, positive linear relationship between quality and helpfulness. There is also a strong, positive linear relationship between quality and clarity, but for one outlier. These tell me that as the rankings given to a professor on quality, helpfulness, and clarity each rise, they can expect the other variables in that group to rise as well. HOwever, I cannot guess or speculate as to the cause of these correlations.\r\nIn comparison, there is a moderate positive linear relationship between positive ratings of easiness with each variable except for raterInterest. The variable raterInterest has a horizontal correlation with all of the other 4 variables, indicating either no correlation or a very weak correlation between them.\r\nQuestion 5\r\nFor the student.survey data file in the smss package, conduct regression analyses relating (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\r\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\r\nUse graphical ways to portray the individual variables and their relationship.\r\n\r\nInterpret descriptive statistics for summarizing the individual variables and their relationship.\r\n\r\nSummarize and interpret results of inferential analyses.\r\n\r\nAnswer 5\r\nAfter installing the “smss” package and loading the relevant library, I again inspect the data and package info.\r\n\r\n\r\nShow code\r\n\r\n#First I load the data\r\n\r\ndata(\"student.survey\")\r\n\r\n#Then I create a data frame with the data set and look at the content\r\n\r\ndf5 <- student.survey\r\n\r\nhead(student.survey)\r\n\r\n\r\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi\r\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative\r\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal\r\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal\r\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate\r\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal\r\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal\r\n            re    ab    aa    ld\r\n1   most weeks FALSE FALSE FALSE\r\n2 occasionally FALSE FALSE    NA\r\n3   most weeks FALSE FALSE    NA\r\n4 occasionally FALSE FALSE FALSE\r\n5        never FALSE FALSE FALSE\r\n6 occasionally FALSE FALSE    NA\r\n\r\nFirst, I use the generalized “plot()” command to take a preliminary look at the graphical data.\r\n\r\n\r\n\r\nShow code\r\n\r\n#To plot y = political ideology and x = religiosity, the relative variable names are \"pi\" and \"re\".\r\n\r\nplot1 <- plot(pi ~ re, data = student.survey)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n#To plot y = high school GPA and x = hours of TV watching, the relative variable names are \"hi\" and \"tv\".\r\n\r\nplot2 <- plot(hi ~ tv, data = student.survey)\r\n\r\n\r\n\r\n\r\nThese initial representations really don’t tell me much about the relationships between the variables.\r\nNow I need to do some data cleaning, and convert the categorical variables for political ideology and religiosity into numeric variables. I will also rename the columns for the sake of clarity.\r\nFor political ideology (“pi”), “as.integer” represents the categorical variables with values of 1 to 7, starting with very liberal (1) and increasing in assigned value to very conservative (7). For religiousity, “as.integer” represents the categorical variables with values of 1 to 4, starting with never (attending religious services) (1) to every week (4). For religiosity (“re”) this was how often you attend religious services, “never” = 1, “occasionally” = 2, “most weeks” = 3, “every week” = 4\r\n\r\n\r\nShow code\r\n\r\ndf5 <- student.survey %>% \r\n  select(c(hi, tv, pi, re))\r\n\r\ndf5$pi <- as.integer(as.factor(df5$pi))\r\ndf5$re <- as.integer(as.factor(df5$re))\r\n\r\n\r\n\r\nThen I create a sub-frame for the particular pair of variables I want to compare. First, religiosity and political ideology; and then create a scatter plot of the correlation between religiosity and political ideology.\r\n\r\n\r\nShow code\r\n\r\ndf5a <- df5 %>%\r\n  rename(Political.Ideology = pi,\r\n         Religiosity = re) %>% \r\n  select(Political.Ideology, Religiosity)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ngg5a <- ggplot(df5a, aes(x=Religiosity, y=Political.Ideology)) +\r\n  geom_point() +\r\n  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"goldenrod\") +\r\n   labs(title= \"Religiosity & Political Ideology\",\r\n        x= \"Religiosity\", \r\n        y = \"Political Ideology\") +\r\n   theme_classic()\r\n\r\ngg5a\r\n\r\n\r\n\r\n\r\nAgain, I create a sub-frame for the specific pair of variables I want to compare. Now I look at high school GPA and hours of TV watched per week and create a scatter plot of the correlation between high school GPA and hours of TV watched per week.\r\n\r\n\r\nShow code\r\n\r\ndf5b <- student.survey %>%\r\n  rename(High.School.GPA = hi,\r\n         Hours.TV = tv) %>% \r\n  select(High.School.GPA, Hours.TV)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ngg5b <- ggplot(df5b, aes(x=Hours.TV, y=High.School.GPA)) +\r\n  geom_point() +\r\n  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"goldenrod\") +\r\n   labs(title= \"High School GPA & Average Hours of TV Watched Per Week\",\r\n        x= \"Average Number of Hours of TV Watched per Week\", \r\n        y = \"High School GPA\") +\r\n   theme_classic()\r\n\r\ngg5b\r\n\r\n\r\n\r\n\r\nI start to interpret descriptive statistics for summarizing the individual variables and their relationship using the “summary()” function.\r\n\r\n\r\n\r\nShow code\r\n\r\nsummary(df5)\r\n\r\n\r\n       hi              tv               pi              re       \r\n Min.   :2.000   Min.   : 0.000   Min.   :1.000   Min.   :1.000  \r\n 1st Qu.:3.000   1st Qu.: 3.000   1st Qu.:2.000   1st Qu.:1.750  \r\n Median :3.350   Median : 6.000   Median :2.000   Median :2.000  \r\n Mean   :3.308   Mean   : 7.267   Mean   :3.033   Mean   :2.167  \r\n 3rd Qu.:3.625   3rd Qu.:10.000   3rd Qu.:4.000   3rd Qu.:3.000  \r\n Max.   :4.000   Max.   :37.000   Max.   :7.000   Max.   :4.000  \r\n\r\nChecking the Pearson Correlation Coefficient and Linear Model Fit of Each Relationship\r\nThis basic correlation matrix gives an overview of the correlations for religiosity and political ideology, rounded to 2 decimals. This indicates a moderate positive relationship between the variables.\r\n\r\n\r\nShow code\r\n\r\nround(cor(df5a),\r\n  digits = 2 \r\n)\r\n\r\n\r\n                   Political.Ideology Religiosity\r\nPolitical.Ideology               1.00        0.58\r\nReligiosity                      0.58        1.00\r\n\r\nUsing the “lm()” formula, I can confirm that the p-value for this relationship is ~1.22.\r\n\r\n\r\nShow code\r\n\r\nfit5a <- lm(pi ~ re, data = df5)\r\n\r\nsummary(fit5a)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = pi ~ re, data = df5)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.81243 -0.87160  0.09882  1.12840  3.09882 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \r\nre            0.9704     0.1792   5.416 1.22e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.345 on 58 degrees of freedom\r\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \r\nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\r\n\r\nAgain, using a correlation matrix gives an overview of the correlations for high school GPA and hours of TV watched, rounded to 2 decimals. This indicates a weak negative correlation between the variables.\r\n\r\n\r\nShow code\r\n\r\nround(cor(df5b),\r\n  digits = 2 \r\n)\r\n\r\n\r\n                High.School.GPA Hours.TV\r\nHigh.School.GPA            1.00    -0.27\r\nHours.TV                  -0.27     1.00\r\n\r\nUsing the “lm()” formula, I can confirm that the p-value for this relationship is ~0.04\r\n\r\n\r\nShow code\r\n\r\nfit5b <- lm(tv ~ hi, data = df5)\r\n\r\nsummary(fit5b)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = tv ~ hi, data = df5)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-8.600 -3.790 -1.167  2.408 27.746 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)   \r\n(Intercept)   20.200      6.175   3.271   0.0018 **\r\nhi            -3.909      1.849  -2.114   0.0388 * \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 6.528 on 58 degrees of freedom\r\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \r\nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\r\n\r\nSummarize and interpret results of inferential analyses.\r\n\r\nThe basis analysis of these pairs of data from the “student.survey” data set tells me that if the null hypothesis are:\r\nH1: There is no correlation between religiosity and political ideology; and\r\nH2: There is no correlation between grade point average and hours of television watched,\r\nthen I can use the inferential analyses to conclude that I have sufficient evidence at the 95% confidence level to reject both H1 and H2 null hypothesis. However, if I choose to use a 99% confidence level, I would be able to to reject H1, but not to reject H2.\r\nQuestion 6\r\nFor a class of 100 students, the teacher takes the 10 students who perform poorest on the midterm exam and enrolls them in a special tutoring program. The overall class mean is 70 on both the midterm and final, but the mean for the specially tutored students increases from 50 to 60. Use the concept of regression toward the mean to explain why this is not sufficient evidence to imply that the tutoring program was successful. (Here’s a useful hint video: https://www.youtube.com/watch?v=1tSqSMOyNFE)\r\nAnswer\r\nNo, we do not have enough information to say that this result is the direct effect of the special tutoring program. The increase in scores among the tutored subset may be due to other factors, such as the students having a better day on the day of the second test, among other potential facts.\r\nMoreover, there is the statistical phenomenon at play of regression toward the mean which can help explain how extreme values correct toward the mean in repeated samples. There is a potential for the results to be statistically significant, but without additional information, the explanation that randomization and regression toward the mean remains primary.\r\nThis example could potentially be strengthened by more information, such as multiple test results over the semester rather than just two tests or a density plot of the entire class scores. It could also be strengthened by the presence of a control group.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-03-dacss-603-assignment-2/dacss-603-assignment-2_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-04-03T17:27:09-05:00",
    "input_file": "dacss-603-assignment-2.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-quantitative-data-analysis/",
    "title": "DACSS 603 Assignment 1",
    "description": "Assignment 1 for DACSS 603 course 'Quantitative Data Analysis': \"Descriptive Statistics, Probability, Statistical Inference, and Comparing Two Means\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-02-23",
    "categories": [
      "statistics",
      "homework"
    ],
    "contents": "\r\nQuestion 1\r\nSurgical Procedure - Representative Sample\r\nSurgical Procedure\r\nSample Size\r\nMean Wait Time\r\nStandard Deviation\r\nBypass\r\n539\r\n19\r\n10\r\nAngiography\r\n847\r\n18\r\n9\r\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures.\r\nIs the confidence interval narrower for angiography or bypass surgery?\r\nAnswer 1\r\nI calculated the answer by first calculating the standard error for each procedure given the mean, standard deviation, and sample size for each. I do so using 0.95 for the qnorm function so that I can determine the 5% confidence level for both the right and left side of the normal distribution, since the sample is larger than n=30. By calculating the 5% margin for each side of the distribution, this gives me the effective 90% confidence interval overall.\r\n\r\n\r\n#Calculate the actual mean wait time for the bypass:\r\n\r\nxbar1 <- 19 #sample mean\r\nsd1a <- 10 #sample standard deviation\r\nn1a <- 539 #sample size\r\n\r\nerror1a <- qnorm(0.95)*sd1a/sqrt(n1a)\r\nerror1a\r\n\r\n\r\n[1] 0.7084886\r\n\r\nlower1a <- xbar1-error1a\r\nupper1a <- xbar1+error1a\r\n\r\nlower1a\r\n\r\n\r\n[1] 18.29151\r\n\r\nupper1a\r\n\r\n\r\n[1] 19.70849\r\n\r\n\r\n\r\n#Calculate the actual mean wait time for the angiography:\r\n\r\nxbar1b <- 18 #sample mean\r\nsd1b <- 9 #sample standard deviation\r\nn1b <- 847 #sample size\r\n\r\nerror1b <- qnorm(0.95)*sd1b/sqrt(n1b)\r\nerror1b\r\n\r\n\r\n[1] 0.5086606\r\n\r\nlower1b <- xbar1b-error1b\r\nupper1b <- xbar1b+error1b\r\n\r\nlower1b\r\n\r\n\r\n[1] 17.49134\r\n\r\nupper1b\r\n\r\n\r\n[1] 18.50866\r\n\r\nNext, I created a data frame with the information.\r\n\r\n\r\nShow code\r\n\r\n#Create a data frame with the information:\r\n\r\ndf1 <- data.frame( c('Bypass', 'Angiography')\r\n                   ,c(19, 18)\r\n                   ,c(539, 847)\r\n                   ,c(10, 9)\r\n                   ,c(18.29151, 17.49134)\r\n                   ,c(19.70849, 18.50866))\r\nnames(df1) <- c('Procedure', 'Mean', 'Sample', 'SD', 'Lower', 'Upper')\r\n\r\ndf1\r\n\r\n\r\n    Procedure Mean Sample SD    Lower    Upper\r\n1      Bypass   19    539 10 18.29151 19.70849\r\n2 Angiography   18    847  9 17.49134 18.50866\r\n\r\nFinally, I created a plot to visualize the results. The visualization communicates that for each procedure, there is a range of values where we can expect 90% of the estimates to include the population mean given the sample mean and standard deviation.\r\n\r\n\r\nShow code\r\n\r\ngg1 <- ggplot(data = df1)\r\ngg1 <- gg1 + geom_point(aes(x = Procedure, y = Mean), size = 5, color = \"blue\")\r\ngg1 <- gg1 + geom_errorbar(aes(x = Procedure, y = Mean, ymin=Lower, ymax=Upper), width=.1, color = \"blue\")\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Lower, label = round(Lower, 2)), hjust = -1)\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Upper, label = round(Upper, 2)), hjust = -1)\r\ngg1 <- gg1 + geom_text(aes(x = Procedure, y = Mean, label = round(Mean, 2)), hjust = -0.5)\r\ngg1 <- gg1 + labs(x = \"Procedure\", y = \"Mean\")\r\ngg1 <- gg1 + labs(title = \"Chart Showing Mean With Upper and Lower Confidence Intervals at 90%\")\r\ngg1 <- gg1 + theme_classic()\r\n\r\ngg1\r\n\r\n\r\n\r\n\r\nFor the angiography, we can be 90% sure that the population mean for the wait time to the procedure falls between 17.49 and 18.51 days.\r\nFor the bypass, we can be 90% sure that the population mean for the wait time to the procedure falls between 18.29 and 19.71 days.\r\nThe confidence interval was narrower for the angiography surgery.\r\nQuestion 2\r\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success.\r\nFind the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\r\nConstruct and interpret a 95% confidence interval for p.\r\nAnswer 2\r\nTo construct the 95% confidence interval, I began by calculating the point sample estimate of the population proportion:\r\n\r\n\r\n#Calculate the point sample estimate of the population proportion using (p = x/n)\r\n\r\nx2 = 567 #affirmative response size\r\nn2 = 1031 #sample size - survey participants\r\n\r\np2 <- x2/n2\r\np2\r\n\r\n\r\n[1] 0.5499515\r\n\r\nThis tells me that the sample proportion of those who believe a college education is essential for success is ~45%.\r\nSince np >= 5 and n(1-p) >= 5, I know that I will calculate the confidence interval of that population proportion as follows: p +/- z * square root of (p) x (1-p)/n\r\n\r\n\r\n#Calculate the confidence interval for p:\r\n\r\nerror2 <-qnorm(0.975)*sqrt(p2*(1-p2)/n2)\r\nerror2\r\n\r\n\r\n[1] 0.03036761\r\n\r\nlower2 <- p2-error2\r\nupper2 <- p2+error2\r\n\r\nlower2\r\n\r\n\r\n[1] 0.5195839\r\n\r\nupper2\r\n\r\n\r\n[1] 0.5803191\r\n\r\nThis tells me that we can be 95% confident that the proportion of adult Americans who believe that a college education is essential for success lies between 51.96% and 58.03% of the population.\r\nAlternatively, using the R prop.test function, I can compare the calculation to my manual calculation and find it is the same for finding the point, but slightly different (at 4 decimal points) on the calculation of the confidence interval.\r\n\r\n\r\nprop.test(x2, n2)\r\n\r\n\r\n\r\n    1-sample proportions test with continuity correction\r\n\r\ndata:  x2 out of n2, null probability 0.5\r\nX-squared = 10.091, df = 1, p-value = 0.00149\r\nalternative hypothesis: true p is not equal to 0.5\r\n95 percent confidence interval:\r\n 0.5189682 0.5805580\r\nsample estimates:\r\n        p \r\n0.5499515 \r\n\r\nQuestion 3\r\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per quarter for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range.\r\nAssuming the significance level to be 5%, what should be the size of the sample?\r\nAnswer 3\r\nI will start by taking the information given and calculating the variables I need to know.\r\nIf the aid office believes the amount spent on books is between $30 and $200, I have a range of $170 to consider.\r\nI want to be 95% confident that the interval estimate contains the population mean, so my z = 1.96.\r\nI also know that the margin of error should be no more than +/- $5 on each end of the estimate, so my margin of error = 5\r\n\r\n\r\nerror3 <- (5)\r\n\r\n#I need to calculate the standard deviation at the given estimate that it is a quarter of the range:\r\n\r\nsd3 <- 170*0.25 #range * 25%\r\nsd3 #standard deviation = 42.5\r\n\r\n\r\n[1] 42.5\r\n\r\n#Now I can calculate the sample size with the formula n=(zσ/M)2.\r\n\r\nss3 <- ((1.96*sd3)/error3)^2\r\nss3 #sample size\r\n\r\n\r\n[1] 277.5556\r\n\r\nUsing these calculations, I can estimate that the financial aid office will need to use a sample size of 278 people.\r\nQuestion 4\r\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\r\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\r\nB. Report the P-value for Ha : μ < 500. Interpret.\r\nC. Report and interpret the P-value for H a: μ > 500.\r\n(Hint: The P-values for the two possible one-sided tests must sum to 1.)\r\nAnswer 4\r\nA. I will start by taking the information given and determining my hypotheses:\r\nH0: The mean weekly earnings for the population of women at the company is μ=$500#\r\nHa: The mean weekly earnings for the population of women at the company is μ≠$500\r\nI cannot assume that the population distribution is normal as I have a low sample size of 9 and it is not stated it is a random sample, but a representative sample.\r\nMy other assumptions are:\r\npopulation mean (mu4) = 500\r\nsample size (n4) = 9\r\nsample mean (xbar4) = 410\r\nsample standard deviation (sd4) = 90\r\nI also need to make a decision about using a significance level of 5%.\r\nI will use the test statistic formula to find the t-value: [t = (x̄) - (μ) / (sd/sqrt(n)]\r\n\r\n\r\na4 <- 500\r\nn4 <- 9\r\nxbar4 <- 410\r\nsd4 <- 90\r\n\r\n#Test statistic:\r\n\r\nt4 <-(xbar4-a4)/(sd4/sqrt(n4))\r\n\r\nt4\r\n\r\n\r\n[1] -3\r\n\r\nGiven that my test statistic = (-3), I can determine the p-value is .00135*2 (to get the sum of both tail probabilities) or 0.0027.\r\nSince my confidence level is 0.05 and my p-value of 0.0027 < 0.05, I can reject the null hypothesis that the mean weekly earnings for the population of women at the company is μ=$500.\r\n\r\n\r\n#Then I take the t-statistic result (-3) and the degrees of freedom by taking \"sample size - 1\" or (\"9\" - 1) = 8.\r\n\r\npval4a <- pt(-3, 8)\r\n\r\npval4a\r\n\r\n\r\n[1] 0.008535841\r\n\r\nB. For the alternative hypothesis Ha: μ < 500:\r\n\r\n\r\npval4b <- pt(-3, 8, lower.tail=FALSE)\r\n\r\npval4b\r\n\r\n\r\n[1] 0.9914642\r\n\r\nPer the hint, I can confirm that these are logical answers by adding the two probabilities together and confirming they equal 1:\r\n\r\n\r\npval4a+pval4b\r\n\r\n\r\n[1] 1\r\n\r\nQuestion 5\r\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000.\r\nJones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7.\r\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\r\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\r\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\r\nAnswer 5\r\nA. To show the t-scores, I will use the test statistic [t = (ybar) - (μ) / (se)] given the results for each:\r\n\r\n\r\n#Test statistic for Jones:\r\n\r\nybar5a <- 519.5\r\nn5 <- 1000\r\na5 <- 500\r\nse5 <- 10.0\r\n\r\nt5a <-(ybar5a-a5)/(se5)\r\n\r\nt5a\r\n\r\n\r\n[1] 1.95\r\n\r\n#Test statistic for Smith:\r\n\r\nybar5b <- 519.7\r\nn5 <- 1000\r\na5 <- 500\r\nse5 <- 10.0\r\n\r\nt5b <-(ybar5b-a5)/(se5)\r\n\r\nt5b\r\n\r\n\r\n[1] 1.97\r\n\r\nTo show the p-values, I will use the pt() function and use the t-statistic results from each of the tests and the degrees of freedom by taking “sample size - 1” or (“100” - 1) = 999. I will need to multiply each result by 2 to account for the probabilities in each tail of the normal distribution.\r\n\r\n\r\n#For Jones' results:\r\n\r\npval5a <- pt(1.95, 999, lower.tail = FALSE) * 2\r\n\r\npval5a\r\n\r\n\r\n[1] 0.05145555\r\n\r\n#For Smith's results:\r\n\r\npval5b <- pt(1.97, 999, lower.tail = FALSE) * 2\r\n\r\npval5b\r\n\r\n\r\n[1] 0.04911426\r\n\r\nB. To use α = 0.05 and look at each result and whether it is “statistically significant”, I can compare the p-values directly to the confidence level of 0.5. Jones’ results gave a p-value of 0.5145, which is just over the threshold of the confidence level given of 0.5. Smith’s results gave a p-value of 0.4911, which is just under the threshold of the confidence level given of 0.5.\r\nHypothesis tests tell us that if the p-value < α, we reject H0 and if p-value ≥ α, we do not reject H0. Given this general statistical guidance, only Smith’s results would be considered “statistically significant”.\r\nC. The results in this example could be very misleading if only whether the results were reported as simply “rejecting” or “not rejecting” H0 or being “statistically significant” or not because that is leaving out vital information on how close the results were to the confidence level used. If the actual p-values were reported instead, they would reflect how marginally “significant” the results really were. The confidence is an artificial threshold that is subjectively applied, and in this case, the results were close enough that they should not be statistically reported as significally different. This is why we need to be sure we report the full p-values and confidence intervals.\r\nQuestion 6\r\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\r\nAnswer\r\nTo answer whether there is enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents, we need to use a left-tailed t-test. We will use the sample data in the variable “gas_taxes” in the t.test function. We know that the t.test() function uses the 95% confidence interval as a default, but it also uses a two-sided test as the default, so we need to provide the alternative argument “less”, indicating a left-sided test.\r\n\r\n\r\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\r\n\r\nt.test(gas_taxes, alternative = \"less\")\r\n\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  gas_taxes\r\nt = 18.625, df = 17, p-value = 1\r\nalternative hypothesis: true mean is less than 0\r\n95 percent confidence interval:\r\n     -Inf 44.67946\r\nsample estimates:\r\nmean of x \r\n 40.86278 \r\n\r\nThis t-test gives us a confidence interval of [inf - 44.67946]. Since the confidence interval includes amounts that are all less than 45 cents, we have enough evidence to conclude that, at that confidence level, the average tax was less than 45 cents.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-03-quantitative-data-analysis/quantitative-data-analysis_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-04-03T17:41:03-05:00",
    "input_file": "quantitative-data-analysis.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-03-networks/",
    "title": "Networks Assignment 2",
    "description": "Intro Assignment for DACSS 697E course 'Social and Political Network Analysis'",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2022-02-04",
    "categories": [
      "networks",
      "homework"
    ],
    "contents": "\r\nSocial network analysis (SNA) is the process of investigating social structures through the use of networks and graph theory. It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties, edges, or links (relationships or interactions) that connect them. Examples of social structures commonly visualized through social network analysis include social media networks, memes spread, information circulation, friendship and acquaintance networks, business networks, knowledge networks, difficult working relationships, social networks, collaboration graphs, kinship, disease transmission, and sexual relationships. These networks are often visualized through sociograms in which nodes are represented as points and ties are represented as lines. These visualizations provide a means of qualitatively assessing networks by varying the visual representation of their nodes and edges to reflect attributes of interest.\r\nSocial network analysis has emerged as a key technique in modern sociology. It has also gained significant popularity in anthropology, biology, demography, communication studies, economics, geography, history, information science, organizational studies, political science, public health, social psychology, development studies, sociolinguistics, and computer science, and is now commonly available as a consumer tool.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-03T17:22:32-05:00",
    "input_file": "networks.knit.md"
  },
  {
    "path": "posts/2022-04-03-welcome-to-kristinas-blog/",
    "title": "Welcome to Kristina's Blog",
    "description": "Detailing my academic projects in the course of pursuing a M.S. in Data Science and Computation for Social Sciences at UMass Amherst",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": "https://www.kristinabecvar.com"
      }
    ],
    "date": "2021-07-12",
    "categories": [
      "personal"
    ],
    "contents": "\r\nHi! I’m Kristina Becvar, a graduate student in the Department of Social and Behavioral Sciences at University of Massachusetts Amherst. I’ve decided to start this blog to document my learning and development in data analysis.\r\nIt’s not news to anyone that coding is difficult, and it’s somewhat above and beyond my time constraints right now to learn more in order to build this blog. But I think (!) I finally got this thing going.\r\nThank you to Steve Linberg for your guidance getting the process of setting up a distill site through RStudio and for generally being a fabulous classmate!\r\nFeel free to pull up a chair, leave a comment, and join me so that we can navigate some things together.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-03T17:34:16-05:00",
    "input_file": "welcome-to-kristinas-blog.knit.md"
  }
]
