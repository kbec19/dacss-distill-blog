---
title: "Supervised Learning"
description: |
  Assignment 8 for DACSS 697D Course 'Text as Data': "Supervised Learning"
categories:
  - text as data
  - homework
  - NYT text analysis project
author:
  - name: Kristina Becvar
    url: https://www.kristinabecvar.com
date: 2022-04-16
output:
  distill::distill_article:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Set Up the Libraries and Data

```{r include=FALSE}
# load libraries
library(tidytext)
library(plyr)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.sentiment)
library(quanteda.dictionaries)
library(caret)

suppressWarnings(expr)
```
### Load Data

```{r code_folding = TRUE}
#load data
head_main <- read.csv("main_headlines.csv")
main_headlines <- as.data.frame(head_main)
#turn into data frame
head_print <- read.csv("print_headlines.csv")
print_headlines <- as.data.frame(head_print)
#inspect data
head(head_main)
head(head_print)
```

### Create Corpus

```{r echo=TRUE}
main_corpus <- corpus(main_headlines, docid_field = "doc.id", text_field = "headline.main")
print_corpus <- corpus(print_headlines, docid_field = "doc.id", text_field = "headline.print")
```

### Assign Type to Docvars

```{r echo=TRUE}
main_corpus$type <- "Main Headline"
print_corpus$type <- "Print Headline"
docvars(main_corpus, field = "type") <- main_corpus$type
docvars(print_corpus, field = "type") <- print_corpus$type
```

### Tokenization

#### Main Headlines

```{r code_folding = TRUE}
main_tokens <- tokens(main_corpus) %>%
  tokens(main_corpus, remove_punct = TRUE) %>%
  tokens(main_corpus, remove_symbols = TRUE) %>%
  tokens_remove(stopwords("english")) %>%
  tokens_remove(c("s"))

main_dfm <- dfm(main_tokens)

length(main_tokens)
print(main_tokens)
```

#### Print Headlines

```{r code_folding = TRUE}
print_tokens <- tokens(print_corpus, remove_punct = TRUE) %>%
  tokens(print_corpus, remove_symbols = TRUE) %>%
  tokens_remove(stopwords("english")) %>%
  tokens_remove(c("s"))

main_dfm <- dfm(print_tokens)

length(print_tokens)
print(print_tokens)
```

### Dictionary Analysis

#### liwcalike()

```{r code_folding = TRUE}
# use liwcalike() to estimate sentiment using NRC dictionary
main_sentiment_nrc <- liwcalike(as.character(main_corpus), data_dictionary_NRC)
names(main_sentiment_nrc)
```

### Training & Testing Data

Now I can get into a new phase of analysis for my data.

The first step is splitting the data into testing and training sets, as well as an "invisible" held-out set that I can use at the end to evaluate the conclusions I draw from training and testing my models.

#### Main Headline Set

```{r echo=TRUE}
#set seed for replication purpuses
set.seed(11)

#create id variable in corpus metadata
docvars(main_corpus, "id") <- 1:ndoc(main_corpus)

# create training set (60% of data) and initial test set
main_training_set <- ndoc(main_corpus)
main_train_index <- sample(1:main_training_set,.6 * main_training_set) 
main_test_index <- c(1:main_training_set)[-main_train_index]

# split test set in half (so 20% of data are test, 20% of data are held-out)
main_training_set <- length(main_test_index)
main_heldout_index <- sample(1:main_training_set, .5 * main_training_set)
main_test_index <- main_test_index[-main_heldout_index]

# now apply indices to create subsets and dfms
dfm_main_train <- corpus_subset(main_corpus, id %in% main_train_index) %>% tokens() %>% dfm()
dfm_main_test <- corpus_subset(main_corpus, id %in% main_test_index) %>% tokens() %>% dfm()
dfm_heldout <- corpus_subset(main_corpus, id %in% main_heldout_index) %>% tokens() %>% dfm()
```

### I'll start with a Naive Bayes model

```{r echo=TRUE}

summary(docvars(main_corpus))

polarity_NaiveBayes <- textmodel_nb(dfm_main_train, docvars(dfm_main_train, "id"), distribution = "Bernoulli")

```

